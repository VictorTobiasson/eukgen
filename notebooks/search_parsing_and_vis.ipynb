{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c66d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.output_area pre {white-space: pre;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vf/users/tobiassonva/data/eukgen\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import altair as alt\n",
    "from Bio import SeqIO\n",
    "import multiprocessing\n",
    "\n",
    "#my HHsuite module in ~/scripts\n",
    "import parseHHsuite as HH\n",
    "\n",
    "#----- NOTEBOOK CONFIG ------\n",
    "\n",
    "#disable altair max rows\n",
    "alt.data_transformers.disable_max_rows()\n",
    "#get default altair style\n",
    "%run ~/scripts/altair_style_config_default.py\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "\n",
    "#enable output scrolling rather than wrapping\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>div.output_area pre {white-space: pre;}</style>\"))\n",
    "\n",
    "\n",
    "\n",
    "root = '/home/tobiassonva/data/eukgen/'\n",
    "%cd {root}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309853a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print mmeory usage of python objects\n",
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "a = sorted([(x, sys.getsizeof(globals().get(x))/(1024**3)) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n",
    "print(*[size for size in a], sep='\\n')\n",
    "sum([size[1] for size in a])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e65456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#small helper for pkl parsing\n",
    "def load_pkl(pkl_file):\n",
    "    import parseHHsuite as HH\n",
    "    with open(pkl_file, 'rb') as infile:\n",
    "        item = pickle.load(infile)\n",
    "    return item\n",
    "\n",
    "def dump_pkl(item, pkl_file):\n",
    "    with open(pkl_file, 'wb') as outfile:\n",
    "        pickle.dump(item, outfile)\n",
    "    print(f'Pickled item as {pkl_file}')\n",
    "\n",
    "#pandas helper function to reset_index inplace\n",
    "def reindex(df, column):\n",
    "    df.sort_values(by=column, inplace=True)\n",
    "    df.set_index(keys=[column], drop=True,inplace=True)\n",
    "    \n",
    "\n",
    "    \n",
    "#take a list of strings and return counts of words separated by spaces \n",
    "#ignores anything contained in regex blacklist expression\n",
    "def calculate_label_counts(labels, blacklist='(protein)'):\n",
    "    words = [label.split() for label in labels]\n",
    "    words = [item for sublist in words for item in sublist]\n",
    "    \n",
    "    #remove common phrases from filter\n",
    "    words = pd.Series(words)[~(pd.Series(words).str.contains(blacklist, regex=True))]\n",
    "\n",
    "    word_counts = words.value_counts()\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "\n",
    "\n",
    "#basic try to ammend the cropping of profile headers employed by hhsuite, capped at 138 or 142 chars??\n",
    "#requires searchDF from parse_HHsuite or equivalent Query, Target dataframe\n",
    "#requires global reference header mapping for both query and targets containing acc and header info as index\n",
    "\n",
    "def create_hhsuite_header_mapping(searchDF, global_header_mapping):\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    #append Target and Query columns \n",
    "    entries = pd.concat([searchDF.Query, searchDF.Target]).unique()\n",
    "\n",
    "    #iterate over entries and try to find a accession\n",
    "    for hit in entries:\n",
    "        #initial attempt by direct matching\n",
    "        try:\n",
    "            hit_acc = global_header_mapping.loc[hit].acc\n",
    "        \n",
    "        #for cropped entriestry to slice acc from first space separated element\n",
    "        #then refer to the global mapping for header\n",
    "        except KeyError:\n",
    "            print('cannot find hit for', hit)\n",
    "            print('trying via acc')\n",
    "            hit_acc = hit.split()[0]\n",
    "            new_header = global_header_mapping[global_header_mapping.acc == hit_acc].index[0]\n",
    "            #print(new_header)\n",
    "            #print('new acc is', hit_acc) \n",
    "        accs.append(hit_acc)\n",
    "\n",
    "    #format and return DF\n",
    "    hhsuite_header_mapping = pd.DataFrame({'acc': accs, 'header':entries})\n",
    "\n",
    "    hhsuite_header_mapping.sort_values(by='header', inplace=True)\n",
    "    hhsuite_header_mapping.set_index(keys=['header'], drop=True,inplace=True)\n",
    "    \n",
    "    return hhsuite_header_mapping\n",
    "\n",
    "\n",
    "#alignment \"viewer style\" plot for HHSuite alignments\n",
    "#takes input as pandas Series containing the 6 series of an HHSuite alignment\n",
    "#returns the processed dataframe and the altari chart object\n",
    "#\n",
    "#Query_sequence     RRRILGPMSSMMMAMAFLSTYPPEFIKRGLEGLRPDGRRPNELRPI...\n",
    "#Query_consensus    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~R~DGR~~delRpI...\n",
    "#Matches            |+..++.-.||||.+.--   +++-+  ..+++|+|||.+||+|||...\n",
    "#Target_consensus   ~~~~~~~~~~~~~~~~~~---~~~~~--~~~~~R~dGR~~deLRpv...\n",
    "#Target_sequence    LSHWLGASGSMMMMMTMQ---MPKLI--DENMMRPDGRAPDELRPV...\n",
    "#Confidence         455667677777765432   22333  249999999999999999...\n",
    "\n",
    "def plot_alignment(alignmentDF, query_name, target_name):\n",
    "\n",
    "    alignmentDF = pd.DataFrame({i:list(alignmentDF[i]) for i in alignmentDF.index})\n",
    "    alignmentDF['seqn'] = alignmentDF.index\n",
    "    alignmentDF['opacity'] = [np.tanh(int(i)/4) if i != ' ' else 0 for i in alignmentDF.Confidence]\n",
    "    alignmentDF_melt = alignmentDF.melt(id_vars=['seqn', 'opacity'], value_vars=alignmentDF.columns[:-1], var_name='series', value_name='token')\n",
    "\n",
    "\n",
    "\n",
    "    #style configuration\n",
    "    token_color_dict = {'-': 'white','~': 'white','|': 'white', '+': 'white','.': 'white',' ': 'white',\n",
    "                    '1': 'white', '2': 'white', '3': 'white', '4': 'white', '5': 'white', '6': 'white', '7': 'white', '8': 'white', '9': 'white',\n",
    "                    'R': '#6276ba', 'K': '#7297c1', 'H': '#7297c1', 'D': '#b25652', 'E': '#b25652', 'S': '#b5b65e', 'T': '#94ae57', 'N': '#72a551', 'Q': '#72a551', 'C': '#cca389', 'G': '#c4ced4', 'P': '#95b5c7', 'A': '#bfa764', 'V': '#b5b65e', 'I': '#94ae57', 'L': '#72a551', 'M': '#cca389', 'F': '#d8c7be', 'Y': '#c4ced4', 'W': '#6276ba',\n",
    "                    'r': '#c7cee6', 'k': '#b5c9df', 'h': '#b5c9df', 'd': '#e3c2c0', 'e': '#e3c2c0', 's': '#e6e6c6', 't': '#c8d6a8', 'n': '#bed7ae', 'q': '#bed7ae', 'c': '#ead6c9', 'g': '#eef1f3', 'p': '#d1e0e8', 'a': '#e0d6b5', 'v': '#e6e6c6', 'i': '#c8d6a8', 'l': '#bed7ae', 'm': '#dfc7b6', 'f': '#eee7e3', 'y': '#e0e6e8', 'w': '#b5bfde'}\n",
    "\n",
    "    seq_len = alignmentDF_melt.seqn.max()\n",
    "    series_number = len(alignmentDF.columns)\n",
    "    scale = 12\n",
    "\n",
    "    title = ['Query:    \\t'+query_name, 'Target:   \\t'+target_name]\n",
    "\n",
    "    token_names = list(token_color_dict.keys())\n",
    "    token_colors = list(token_color_dict.values())\n",
    "    sort_order = ['Confidence', 'Query_sequence', 'Query_consensus', 'Matches', 'Target_consensus', 'Target_sequence']\n",
    "\n",
    "    #base chart \n",
    "    base = alt.Chart(alignmentDF_melt, title=title).encode(\n",
    "        alt.X('seqn:O', axis=alt.Axis(title=None, values=list(range(0,seq_len,5)), grid=False)),\n",
    "        alt.Y('series:O', sort=sort_order, axis=alt.Axis(grid=False, title=None, labelFontSize=scale)),\n",
    "        alt.Opacity('opacity', legend=None),\n",
    "    ).properties(width=seq_len*scale*1.4, height=series_number*scale*1.4)\n",
    "\n",
    "    #residue labels\n",
    "    text = base.mark_text(color ='black', align='center', fontSize=scale).encode(\n",
    "        alt.Text('token')\n",
    "    )\n",
    "\n",
    "    #colored boxes\n",
    "    box = base.mark_rect().encode(\n",
    "        alt.Color('token', scale=alt.Scale(domain=token_names, range=token_colors),\n",
    "                 legend=alt.Legend(direction='horizontal', columns=4, orient='left', title=None, labelFontSize=scale,\n",
    "                                   values=['R', 'K', 'H', 'D', 'E', 'S', 'T', 'N', 'Q', 'C', 'G', 'P', 'A', 'V', 'I', 'L', 'M', 'F', 'Y', 'W']))\n",
    "    )\n",
    "\n",
    "    chart = alt.layer(box, text).configure_title(fontSize=scale*1.5)\n",
    "\n",
    "    return alignmentDF, chart\n",
    "\n",
    "\n",
    "#calculate cumulative sum and distribution for pd.Series\n",
    "#takes pd.Series as input and returns a parsed DF and altair chart object \n",
    "def plot_cumsum_counts(series, title='Chart', x_label='value', y_label='count', \n",
    "                       x_min=0, y_min=0, x_max=None, y_max=None,\n",
    "                       x_scale_type='log', y_scale_type='log', decimals=2):\n",
    "    \n",
    "    #format DF for data handling, filter 0 values for plot \n",
    "    #round to reduce float data display jaggedness\n",
    "    series = series[series!=0].round(decimals)\n",
    "    \n",
    "    #format distribution dataframe\n",
    "    countDF = pd.DataFrame(series.value_counts())\n",
    "    countDF.columns = ['amount']\n",
    "    countDF.sort_index(inplace=True)\n",
    "    countDF['cumsum'] = countDF['amount'].cumsum()\n",
    "    countDF['frac_cumsum'] = countDF['cumsum']/countDF['cumsum'].max()\n",
    "    countDF.reset_index(inplace=True)\n",
    "    \n",
    "    #rename columns for plotting\n",
    "    countDF.columns = [x_label,y_label,'cumsum','frac_cumsum']\n",
    "\n",
    "    #format axis domains\n",
    "    x_range = [x_min, series.max()]\n",
    "    y_range = [y_min, countDF[y_label].max()]\n",
    "    \n",
    "    if x_max:\n",
    "        x_range = [x_min, x_max]\n",
    "    \n",
    "    if y_max:\n",
    "        y_range = [y_min, y_max]\n",
    "        \n",
    "    #plot cumulative distribution\n",
    "    chart_cumsum = alt.Chart(countDF, title=title).mark_line(color=colorlib['twilight_shifted_r_perm'][2],\n",
    "                                              strokeWidth=3).encode(\n",
    "        x=alt.X(x_label, title=x_label, scale=alt.Scale(type=x_scale_type)),\n",
    "        y=alt.Y('frac_cumsum', title='Cumulative Fraction', scale=alt.Scale(domain=[0,1]), axis=alt.Axis(labelAlign='left')),\n",
    "        tooltip=alt.Tooltip([x_label, y_label, 'frac_cumsum'])\n",
    "    )\n",
    "    \n",
    "    #plot value distribution\n",
    "    chart_bar = alt.Chart(countDF).mark_area(interpolate='step-after', \n",
    "                                            fillOpacity=0.2, line=True).encode(\n",
    "        x=alt.X(x_label+':Q', scale=alt.Scale(domain=x_range, type=x_scale_type)),\n",
    "        y=alt.Y(y_label, scale=alt.Scale(domain=y_range, type=y_scale_type)),\n",
    "        tooltip=alt.Tooltip([x_label, y_label, 'frac_cumsum'])\n",
    "    )\n",
    "\n",
    "    #merge and configure\n",
    "    merge = alt.layer(chart_bar, chart_cumsum).resolve_scale(y='independent').interactive()\n",
    "\n",
    "    return countDF, merge\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#parse HHSuite outfut fromm ffdata into pkl files \n",
    "def parse_and_write(file):\n",
    "    print(file)\n",
    "    new_data = HH.load_HHBlitsData(file)\n",
    "    new_data.write_pkl(file+'.pkl')\n",
    "    #new_data.write_data_tsv(file+'.tsv')\n",
    "\n",
    "\n",
    "def parse_filter_write(file):\n",
    "    thread = multiprocessing.current_process().pid\n",
    "    print(f'{thread} reading {file}\\n')\n",
    "    new_data = HH.load_HHBlitsData(file)\n",
    "    new_data.write_pkl(file+'.pkl')\n",
    "    print(f'{thread} parsing\\n')\n",
    "\n",
    "    for key, query in new_data.data.items():\n",
    "        query.add_self_hit()\n",
    "        query.filter_numeric(field='Pairwise_cov', min=20, replace=True, keep_self=True)\n",
    "        query.filter_numeric(field='Prob', min=50, replace=True, keep_self=True)\n",
    "    \n",
    "    print(f'{thread} writing\\n')\n",
    "    new_data.write_pkl(file+'.pkl_filtered')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c58105",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#parallel read and filter all data in files, save to pkl\n",
    "\n",
    "search_root='search/euk-prok/search/'\n",
    "files = [search_root+file for file in os.listdir(search_root) if file.endswith('ffdata')]\n",
    "\n",
    "with multiprocessing.Pool(processes=16) as pool:\n",
    "    pool.map(parse_filter_write, files)\n",
    "    \n",
    "\n",
    "    \n",
    "#open all parsed pkl files in folder and merge into one object\n",
    "\n",
    "all_data = HH.HHblitsData()\n",
    "search_root='search/euk-prok/pkl/'\n",
    "pkl_filtered_files = [file for file in os.listdir(search_root) if file.endswith('pkl')]\n",
    "\n",
    "for i, file in enumerate(pkl_filtered_files):\n",
    "    print(i,file)\n",
    "    new_data = HH.HHblitsData()\n",
    "    new_data.load_from_pkl(search_root+file)\n",
    "    all_data.add_entries(new_data.data)\n",
    "\n",
    "    \n",
    "#all_data.write_pkl(search_root+'merged_filtered_self-match.pkl')\n",
    "#all_data.write_data_tsv(search_root+'merged_filtered_self-match.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c52251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data from /analysis/core_data\n",
    "\n",
    "#clustering data\n",
    "\n",
    "print('Loading clustering data')\n",
    "euk_clust = load_pkl(root+'analysis/core_data/euk72_filtered-prof-search-clust.pkl')['members']\n",
    "prok_clust = load_pkl(root+'analysis/core_data/prok2111_filtered-prof-search-clust.pkl')['members']\n",
    "\n",
    "reindex(euk_clust, 'cluster_acc')\n",
    "reindex(prok_clust, 'cluster_acc')\n",
    "\n",
    "# header mapping\n",
    "\n",
    "print('Loading header mapping')\n",
    "euk_header = load_pkl(root+'analysis/core_data/euk72_header_mapping.pkl')\n",
    "prok_header = load_pkl(root+'analysis/core_data/prok2111_header_mapping.pkl')\n",
    "\n",
    "reindex(euk_header, 'header')\n",
    "reindex(prok_header, 'header')\n",
    "\n",
    "#hhsuite profile header mapping as hhsuite crops header info\n",
    "hhsuite_header = load_pkl(root+'analysis/core_data/hhsuite_header_mapping.pkl')\n",
    "\n",
    "#cluster taxonomic filter info\n",
    "print('Loading taxonomy info')\n",
    "euk_tax = load_pkl('euk72/euk72_protein_taxonomy.pkl')\n",
    "\n",
    "#full prok tax\n",
    "#prok_tax = load_pkl('prok2111/prok2111_protein_taxonomy.pkl')\n",
    "\n",
    "#lighter parsed version\n",
    "prok_tax = load_pkl('analysis/core_data/prok2111_protein_taxonomy_trimmed.pkl')\n",
    "\n",
    "#search data\n",
    "print('Loading search data')\n",
    "#full searchDF\n",
    "#searchDF = load_pkl(root+'analysis/core_data/merged_filtered_cov20_self-match_tsv.pkl')\n",
    "\n",
    "#parsed acc viersion without alignment, self hits and indexed in query\n",
    "searchDF = load_pkl('analysis/core_data/merged_filtered_cov20_self-match_tsv_edited_no_aln.pkl')\n",
    "\n",
    "search_queries = load_pkl(root+'analysis/core_data/merged_filtered_cov20_self-match_tsv.query.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d29079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example quesries for testing\n",
    "euk_queries_test1 = ['OLP83888.1', 'SPQ97222.1', 'XP_009012109.1', 'XP_001461706.1',\n",
    "       'OLP92683.1', 'XP_002965802.1', 'XP_024530808.1', 'OLP86390.1',\n",
    "       'KAA0160735.1', 'XP_002681799.1', 'XP_002113075.1', 'OLQ13277.1',\n",
    "       'XP_001469768.1', 'XP_008902708.1', 'XP_002682078.1', 'XP_024309865.1',\n",
    "       'OLP87304.1', 'XP_004365904.1', 'XP_012899378.1', 'XP_032224311.1',\n",
    "       'XP_032223284.1', 'XP_001707828.1', 'XP_005536084.1', 'XP_018187362.1',\n",
    "       'XP_645838.1', 'PXF40497.1', 'XP_005847475.1', 'KAA0160637.1',\n",
    "       'XP_013754706.1', 'XP_008905306.1']\n",
    "\n",
    "euk_queries_test2 = ['CBN77353.1', 'CEL94470.1', 'CEL98020.1', 'CEM00912.1',\n",
    "       'CEM13793.1', 'CEO94447.1', 'CEP02189.1', 'CEP02404.1',\n",
    "       'EPZ31333.1', 'GBG32138.1', 'GBG34166.1', 'GBG34636.1',\n",
    "       'GBG88810.1', 'KAA0151157.1', 'KAA0167757.1', 'KAA6364588.1',\n",
    "       'KAA6383781.1', 'NP_001022034.1', 'NP_001105121.2',\n",
    "       'NP_001170744.1', 'NP_001189295.1', 'NP_001242666.1',\n",
    "       'NP_001259573.1', 'NP_001261837.1', 'NP_001294564.1',\n",
    "       'NP_001307934.1', 'NP_001328712.1', 'NP_012528.1', 'NP_050092.1',\n",
    "       'NP_051148.1', 'NP_189541.1', 'NP_197350.1', 'NP_498455.2',\n",
    "       'NP_505960.3', 'NP_588329.1', 'NP_595422.1', 'NP_609709.1',\n",
    "       'NP_611238.2', 'NP_649295.1', 'OAD00700.1', 'OAD03858.1',\n",
    "       'OAD05886.1', 'OAE33051.1', 'OLP78629.1', 'OLQ06972.1',\n",
    "       'OLQ08228.1', 'OLQ08510.1', 'OLQ11720.1', 'OLQ12045.1',\n",
    "       'OLQ14344.1']\n",
    "\n",
    "euk_queries_test3 = ['XP_008911403.1', 'XP_011408184.1', 'XP_002681038.1', 'XP_002673113.1',\n",
    "                     'OAD09041.1', 'XP_001634466.1', 'XP_005765180.1', 'XP_011407364.1', \n",
    "                     'XP_005789988.1', 'KAA6344160.1', 'KAA6409619.1', 'XP_002287408.1',\n",
    "                     'OAE21175.1', 'RKP17192.1', 'XP_013760427.1', 'KAA0163767.1',\n",
    "                     'XP_002119908.1', 'XP_009692086.1']\n",
    "\n",
    "#queries in superkingdoms with between 20, 50 mmebers hitting less than 20 prokaryotic profiles\n",
    "#and the representative seuqence is from a fish, insect or mammal\n",
    "euk_queries_test4 = ['NP_001002332.1', 'NP_001240313.1',\n",
    "       'NP_001259573.1', 'NP_001260847.1', \n",
    "       'NP_001278869.1', 'NP_001307724.1', 'NP_001307934.1', 'NP_001334755.1',\n",
    "       'NP_001356620.1', 'NP_115888.1', \n",
    "       'NP_610753.1', 'NP_611238.2', 'NP_956312.1',\n",
    "       'NP_998197.1', 'NP_998403.1', 'XP_005256905.1',  'XP_017206845.1', 'XP_021326060.1', 'XP_021336265.1']\n",
    "\n",
    "#queries in superkingdoms with between 20, 50 mmebers hitting less than 20 prokaryotic profiles\n",
    "#and there are between 200 and 2000 prokaryotic hits \n",
    "euk_queries_test5 = ['AGK83073.1', 'CBN73833.1', 'CBN79086.1', 'CEM35385.1', 'CEO94447.1', 'CEP00213.1', 'CEP03651.1', 'EPZ30938.1', 'EPZ31301.1', 'GBG60132.1', \n",
    "'GBG70565.1', 'GBG80562.1', 'GBG83744.1', 'KAA0165271.1', 'KAA0172078.1', 'KAA6408708.1', 'NP_001002332.1', 'NP_001259573.1', 'NP_001294564.1',\n",
    " 'NP_001307724.1', 'NP_001328712.1', 'NP_001334755.1', 'NP_001356620.1', 'NP_011081.1', 'NP_050092.1', 'NP_594946.1', 'NP_610753.1', 'NP_848958.1',\n",
    "  'NP_849074.1', 'NP_956312.1', 'NP_998197.1', 'OAD04802.1', 'OAD06369.1', 'OAE33370.1', 'OLP84660.1', 'OLQ06972.1', 'OLQ08228.1', 'OLQ14344.1',\n",
    "   'OSX69435.1', 'OSX71470.1', 'OSX72678.1', 'OSX75094.1', 'OSX77054.1', 'PTQ50428.1', 'PXF41822.1', 'PXF45288.1', 'RKP17849.1', 'RKP18091.1', \n",
    "   'RKP20265.1', 'RWR93989.1', 'RWR97906.1', 'RWR98344.1', 'SLM34047.1', 'SLM40311.1', 'SLM40671.1', 'SPQ96285.1', 'SPQ98172.1']\n",
    "\n",
    "\n",
    "print('Done')\n",
    "\n",
    "#calcualtion of profiles to be included in MSA evaluation\n",
    "euk_lca = load_pkl('analysis/core_data/euk72_filtered-prof-search-clust.lca.pkl')\n",
    "euk_lca_superkingdom = euk_lca[euk_lca.lca.isin(['superkingdom'])].index.unique().values\n",
    "\n",
    "searchDF_filtered = searchDF[(searchDF.index.isin(euk_lca_superkingdom)) &\n",
    "                            (searchDF.Pairwise_cov > 0.5) &\n",
    "                            (searchDF.Prob > 50)]\n",
    "\n",
    "\n",
    "queries_filtered = searchDF_filtered.index.unique().values\n",
    "\n",
    "\n",
    "#extended set for full miscrocosm evalution testing\n",
    "wider_set = searchDF_filtered[(euk_clust.index.value_counts().between(20,1000)) & \n",
    "                  (searchDF.index.value_counts().between(2,500))]\n",
    "wider_set_queries = wider_set.sample(1000).index.unique()[0:200].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b64214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_microcosm(queries, query_hits, query_clusters, target_clusters, root):    \n",
    "    root_query = root+query+'/'\n",
    "    os.mkdir(root_query)\n",
    "\n",
    "    print(f'Q:{query}')\n",
    "    query_members = query_clusters.loc[query, 'acc']\n",
    "    hits = query_hits.loc[query]\n",
    "    members = target_clusters.loc[hits]\n",
    "\n",
    "    #write quesry accessions to .acc\n",
    "    with open(root_query+f'{query}.acc', 'w') as outfile:\n",
    "        outfile.write(pd.DataFrame(query_members).to_csv(sep='\\t', header=None, index=None))\n",
    "\n",
    "    with open(root_query+f'{query}.targets', 'w') as outfile:\n",
    "        outfile.write(pd.DataFrame(members).to_csv(sep='\\t', header=None))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998b093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.system('rm -r microcosm4/*')\n",
    "for query in wider_set_queries:\n",
    "    structure_microcosm(query, searchDF_filtered.Target, euk_clust, prok_clust, 'microcosm2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cefeb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'CBJ26283.1'\n",
    "structure_microcosm(query, searchDF_filtered.Target, euk_clust, prok_clust, 'microcosm2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42d7c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = searchDF_filtered[(euk_clust.index.value_counts().between(20,100)) & \n",
    "                  (searchDF.index.value_counts().between(2,30))]\n",
    "\n",
    "wider_set = searchDF_filtered[(euk_clust.index.value_counts().between(20,1000)) & \n",
    "                  (searchDF.index.value_counts().between(2,500))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1381a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wider_set.sample(1000).index.unique()[0:200].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74746e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts = 0\n",
    "\n",
    "for n, i in enumerate(searchDF_filtered.index.unique()):\n",
    "    hits = searchDF_filtered.loc[[i],'Target'].values\n",
    "    counts += prok_clust.loc[hits].shape[0]\n",
    "    print(n, round(counts/(n+1),1), counts, i, sep='\\t')\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b0fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'OAE21175.1'\n",
    "query_clusters = euk_clust\n",
    "query_hits = searchDF_filtered.Target\n",
    "target_clusters = prok_clust\n",
    "\n",
    "query_members = query_clusters.loc[query, 'acc']\n",
    "hits = query_hits.loc[query]\n",
    "members = target_clusters.loc[hits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312c9d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_clusters.loc[query, 'acc'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea78f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef1f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_file = 'microcosm2/KAA0163767.1/KAA0163767.1.acc'\n",
    "with open(cluster_file, 'r') as infile:\n",
    "    clusters = {}\n",
    "\n",
    "    for l in infile.readlines():\n",
    "        cluster_acc, acc = l.strip().split('\\t')\n",
    "\n",
    "        if cluster_acc not in clusters.keys():\n",
    "            clusters[cluster_acc] = [acc]\n",
    "\n",
    "        else:\n",
    "            clusters[cluster_acc].append(acc)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff0174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a series of query accessions, retreive all proteins from prokaryotic hits and return their phylogenetic distribution count\n",
    "#designed for multiprocess Pool and checkpointing to .pkl files\n",
    "#not very portable \n",
    "\n",
    "def get_hit_tax_dist(queries):\n",
    "    stime = time.time()\n",
    "    querynr = len(queries)\n",
    "    \n",
    "    thread = multiprocessing.current_process().pid\n",
    "    print(f'{thread}: started\\n')\n",
    "    \n",
    "    print(f'{thread}: processing {querynr} queries\\n')\n",
    "    \n",
    "    #load data from files \n",
    "    print(f'{thread}: loading search\\n')\n",
    "    searchDF = load_pkl(root+'analysis/core_data/merged_filtered_cov20_self-match_tsv_edited_no_aln.pkl')\n",
    "    \n",
    "    print(f'{thread}: loading clust\\n')\n",
    "    prok_clust = load_pkl(root+'analysis/core_data/prok2111_filtered-prof-search-clust.pkl')['members']\n",
    "    reindex(prok_clust, 'cluster_acc')\n",
    "\n",
    "    print(f'{thread}: loading tax\\n')\n",
    "    prok_tax = load_pkl('analysis/core_data/prok2111_protein_taxonomy_trimmed.pkl')\n",
    "\n",
    "    #process only the given slice\n",
    "    searchDF = searchDF[(searchDF.Query.isin(queries)) &\n",
    "                         (searchDF.Query != searchDF.Target) &\n",
    "                         (searchDF.Pairwise_cov > 0.5)]\n",
    "\n",
    "    #set index for faster iterating over queries\n",
    "    searchDF.sort_values(by='Query', inplace=True)\n",
    "    searchDF.set_index(keys=['Query'], drop=True, inplace=True)\n",
    "\n",
    "    #iterate and pool taxa distributions\n",
    "    taxa = {}\n",
    "    n = 0\n",
    "    printn = 50\n",
    "    checkn = 1000\n",
    "\n",
    "    for query in queries:\n",
    "        \n",
    "        if n%printn == 0:\n",
    "            print(f'{thread}: calculating {query} \\t{n}|{querynr} \\tT+{round(time.time()-stime)} seconds\\n')\n",
    "        \n",
    "        #find all target profile hits\n",
    "        profiles = pd.Series(searchDF.loc[query, 'Target'])\n",
    "\n",
    "        #find all proteins in target profile hits\n",
    "        proteins = pd.Series(prok_clust.loc[profiles,'acc'])\n",
    "\n",
    "        #find all taxonomic information from proteins in taget profile hits\n",
    "        query_taxa = prok_tax.loc[proteins, 'class'].value_counts()\n",
    "\n",
    "        #add to dict\n",
    "        taxa[query] = query_taxa\n",
    "        \n",
    "        #itermediate save\n",
    "        if n != 0 and n%checkn == 0:\n",
    "            print(f'{thread}: saved checkpoint {n/checkn}')\n",
    "            dump_pkl(taxa, f'analysis/core_data/tax/{thread}_checkpoint_{int(n/checkn)}_tax.pkl')\n",
    "            taxa = {}\n",
    "            \n",
    "        n+=1\n",
    "        \n",
    "    dump_pkl(taxa, f'analysis/core_data/tax/{thread}_tax.pkl')\n",
    "    return taxa\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#launch parallel execution\n",
    "queries = searchDF[(searchDF.Query != searchDF.Target)\n",
    "                    & (searchDF.Pairwise_cov > 0.5)].Query.unique()\n",
    "\n",
    "splits = np.array_split(queries, 16)\n",
    "with multiprocessing.Pool(processes=16) as pool:\n",
    "    pool.map(get_hit_tax_dist, splits)\n",
    "    \n",
    "    \n",
    "#load data from savepoints into one dictionary\n",
    "tax_data = {}\n",
    "for file in os.listdir('analysis/core_data/tax/'):\n",
    "    print('analysis/core_data/tax/'+file)\n",
    "    data = load_pkl('analysis/core_data/tax/'+file)\n",
    "    tax_data = tax_data.copy()\n",
    "    tax_data.update(data)\n",
    "    \n",
    "#merge dict series into one dataframe\n",
    "query_tax = pd.DataFrame()\n",
    "for query, data in tax_data.items():\n",
    "    print(query)\n",
    "\n",
    "    data.name = query\n",
    "    \n",
    "    temp_tax = pd.DataFrame(data).transpose()\n",
    "    query_tax = pd.concat([query_tax, temp_tax])\n",
    "\n",
    "#save processed dataframe\n",
    "dump_pkl(query_tax, 'analysis/core_data/hit_distribution_cov50.pkl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#calculate query statistics tables\n",
    "#load raw hit counts\n",
    "query_tax = load_pkl('analysis/core_data/hit_distribution_cov50.pkl')\n",
    "\n",
    "#normalize to relative total hits\n",
    "query_tax_rel = query_tax.div(query_tax.sum(axis=1), axis=0)\n",
    "#calculate percentile ranks for relative observations skipping 0 observations\n",
    "query_tax_rel_percentile = query_tax_rel.apply(lambda df: df[df!=0].rank(method='max', pct=True), axis=0).fillna(0)\n",
    "\n",
    "#multiply by individual hits to get weights\n",
    "query_tax_weight = pd.DataFrame(query_tax.values*query_tax_rel.values, columns=query_tax.columns, index=query_tax.index)\n",
    "#calculate percentile rank of observation of weight\n",
    "query_tax_weight_percentile = query_tax_weight.apply(lambda df: df.rank(method='max', pct=True), axis=0)\n",
    "\n",
    "\n",
    "#write proteins from query clusters which have top 10% relative hits in \n",
    "#respective taxon to file\n",
    "\n",
    "prominent_taxon_proteins = {}\n",
    "for taxon in query_tax.columns:\n",
    "    prominent_queries = query_tax_rel_percentile[(query_tax_rel_percentile[taxon].between(0.85, 0.90))].index\n",
    "    prominent_proteins = euk_clust.loc[prominent_queries, 'acc'].values\n",
    "\n",
    "    homo_tax = euk_tax[euk_tax['class'] == 'Mammalia']\n",
    "    prominent_homo_proteins = homo_tax[homo_tax.index.isin(prominent_proteins)].index\n",
    "    outdata = euk_header[euk_header.acc.isin(prominent_homo_proteins)]\n",
    "    print(taxon, outdata.shape[0])\n",
    "    outdata.reset_index(inplace=True)\n",
    "    \n",
    "    outdata['header'].to_csv(f'analysis/core_data/significant/{taxon.replace(\" \", \"_\")}.tsv', sep='\\t', index=None, header=None)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25fa6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#describe search results\n",
    "\n",
    "print('Load data')\n",
    "#load parse_HHSuite data from pkl \n",
    "search_data = HH.HHblitsData()\n",
    "search_data.load_from_pkl(root+'search/euk-prok/merged_filtered_cov20_self-match.pkl')\n",
    "\n",
    "#search block csv\n",
    "search_csv = pd.read_csv(root+'search/euk-prok/merged_filtered_cov20_self-match.csv', sep='\\t')\n",
    "#query block csv\n",
    "search_queries_csv = pd.read_csv(root+'search/euk-prok/merged_filtered_cov20_self-match.queries.csv', sep='\\t', index_col='Query')\n",
    "\n",
    "#remove self hits\n",
    "search_nonself_csv = search_csv[(search_csv.Query != search_csv.Target)]\n",
    "\n",
    "#total hits\n",
    "print(f'Total hits = {search_nonself_csv .shape[0]}\\n')\n",
    "\n",
    "#identities\n",
    "probs = search_nonself_csv.Prob.describe()\n",
    "print('Probability breakdown\\n', probs,'\\n')\n",
    "\n",
    "#probabilities\n",
    "ident = search_nonself_csv.Identities.describe()\n",
    "print('Identity breakdown\\n', ident,'\\n')\n",
    "\n",
    "\n",
    "#calculate number of hits per query and number of queries per hit\n",
    "print('calculating hit statistics\\n')\n",
    "hits = []\n",
    "for query in search_data.query_names[0:100000]:\n",
    "    query_data = pd.DataFrame(search_data.data[query].hit_dict)\n",
    "    hits.append(query_data[query_data.Target != query].shape[0])\n",
    "    \n",
    "inverse_hits = search_nonself_csv.Target.value_counts()\n",
    "\n",
    "print('Most frequent inverse hits\\n')\n",
    "print(inverse_hits[0:20], '\\n')\n",
    "\n",
    "print('most common words among inverse hits\\n')\n",
    "print(calculate_label_counts(inverse_hits[inverse_hits>100].index)[0:20], '\\n')\n",
    "\n",
    "#plot hit distributions\n",
    "plot_cumsum_counts(pd.Series(inverse_hits.values)) &  plot_cumsum_counts(pd.Series(hits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8465d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot distribution of probabilities against identities\n",
    "\n",
    "subset = search_data[search_data.Query != search_data.Target].sample(100000)\n",
    "\n",
    "chart = alt.Chart(subset).mark_rect().encode(\n",
    "    x=alt.X('Prob:Q', bin=alt.Bin(extent=[0,100], step=1), axis=alt.Axis(grid=False)),\n",
    "    y=alt.Y('Identities:Q', bin=alt.Bin(extent=[0,100], step=1), axis=alt.Axis(grid=False)),\n",
    "    color=alt.Color('count()', scale=alt.Scale(scheme='bluepurple', domain=[-1,200]))\n",
    "\n",
    ").properties(width=800, height=450)\n",
    "\n",
    "\n",
    "#precomputed histogram for faster render\n",
    "#doesn't work properly\n",
    "\n",
    "xvalue='Prob'\n",
    "yvalue='Similarity'\n",
    "\n",
    "xbins = 100\n",
    "ybins = 100\n",
    "xrange = [subset[xvalue].min(), subset[xvalue].max()]\n",
    "yrange = [subset[yvalue].min(), subset[yvalue].max()]\n",
    "\n",
    "hist_data = np.histogram2d(subset[xvalue], subset[yvalue], \n",
    "                           bins=(np.linspace(xrange[0], xrange[1], xbins).round(2), \n",
    "                                 np.linspace(yrange[0], yrange[1], ybins).round(2)), density=True)\n",
    "x = []\n",
    "y = []\n",
    "v = []\n",
    "for i, m in enumerate(hist_data[1][:-1]):\n",
    "    for j, n in enumerate(hist_data[2][:-1]):\n",
    "        x.append(m)\n",
    "        y.append(n)\n",
    "        v.append(hist_data[0][i][j])\n",
    "        \n",
    "hist_data_plot = pd.DataFrame({'xbins': x, 'ybins': y, 'density': v})\n",
    "\n",
    "\n",
    "alt.Chart(hist_data_plot).mark_rect().encode(\n",
    "    x=alt.X('xbins:O', axis=alt.Axis(grid=False)),\n",
    "    y=alt.Y('ybins:O', axis=alt.Axis(grid=False)),\n",
    "    color = alt.Color('density',  scale=alt.Scale(scheme='bluepurple', domain=[min(v)*5, max(v)/10]))\n",
    ").properties(width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78364703",
   "metadata": {},
   "source": [
    "------ REFORMATTING FOR CLUSTERING --------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e890c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dirty check for label consistency of queries and blasttab formatting for mmseqs\n",
    "\n",
    "root = 'clust/euk72-profiles/profile-profile-hhsearch/'\n",
    "#header = ['Query','Target','Prob','E-value','P-value','Score','SS','Cols','Identities','Similarity','Sum_probs','Query-HMM-start','Query-HMM-end','Template-HMM-start','Template-HMM-end','Template_columns','Template_Neff']\n",
    "data = pd.read_csv(root+'merged_data_filtered_c80.tsv', sep = '\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93201e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = '/data/tobiassonva/data/eukgen/euk72/euk72.lookup'\n",
    "lookup = pd.read_csv(lookup, sep='\\t', header=None, index_col=1, names=['name', 'index', 'none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9616c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the labels to get accessions\n",
    "parse_query = [entry.split()[0] for entry in data.Query]\n",
    "parse_target = [entry.split()[0] for entry in data.Target]\n",
    "\n",
    "\n",
    "#find label discrepancies\n",
    "queries = parse_query\n",
    "entries = lookup.index\n",
    "\n",
    "clusters = '/data/tobiassonva/data/eukgen/clust/euk72-profiles/euk72_filtered-casc-clust-6-merged.tsv'\n",
    "clusters = pd.read_csv(clusters, sep ='\\t', header=None, names=['clust', 'mem'])['clust']\n",
    "\n",
    "a = set(queries).difference(set(clusters))\n",
    "b = set(clusters).difference(set(queries))\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inconsistent labels from mmseqs internal parsing\n",
    "query = pd.Series(parse_query)\n",
    "\n",
    "query.replace('pir||A44923', 'A44923',inplace=True)\n",
    "query.replace('prf||1111187A', '1111187A', inplace=True)\n",
    "query.replace('prf||1111187C', '1111187C', inplace=True)\n",
    "\n",
    "target = pd.Series(parse_target)\n",
    "\n",
    "target.replace('pir||A44923', 'A44923',inplace=True)\n",
    "target.replace('prf||1111187A', '1111187A', inplace=True)\n",
    "target.replace('prf||1111187C', '1111187C', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad2e35e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_index = [lookup.loc[entry, 'name'] for entry in query]\n",
    "target_index = [lookup.loc[entry, 'name'] for entry in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa99569",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = pd.Series(parse_query)\n",
    "query_index = [lookup.loc[entry, 'name'] for entry in query]\n",
    "print('target')\n",
    "target = pd.Series(parse_target)\n",
    "target_index = [lookup.loc[entry, 'name'] for entry in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162e0922",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Query = query_index\n",
    "data.Target = target_index\n",
    "data_queries = data.Query.unique()\n",
    "data_filtered = data[data.Target.isin(data_queries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b8fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#format blasttab format mode 8 for mmseqs tsv2result\n",
    "blasttab_m8 = pd.DataFrame({'Query': data_filtered.Query, 'Target':data_filtered.Target})\n",
    "\n",
    "add_columns = ['Identities', 'Template_columns',  'Query-HMM-start','Query-HMM-end','Template-HMM-start','Template-HMM-end','E-value', 'Score']\n",
    "order = ['Query', 'Target', 'Identities', 'Template_columns', 'Mismatch', 'Gap_open', 'Query-HMM-start','Query-HMM-end','Template-HMM-start','Template-HMM-end','E-value', 'Score']\n",
    "\n",
    "\n",
    "blasttab_m8[add_columns] = data_filtered[add_columns]\n",
    "blasttab_m8['Mismatch'] = [0 for _ in blasttab_m8.index]\n",
    "blasttab_m8['Gap_open'] = [0 for _ in blasttab_m8.index]\n",
    "\n",
    "blasttab_m8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58de4904",
   "metadata": {},
   "outputs": [],
   "source": [
    "blasttab_m8.to_csv(root+'merged_data_filtered_c80.tsv.blasttab', sep = '\\t', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcb28335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#format from self_search\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "search_file = '/home/tobiassonva/data/eukgen/processing/euk72_ep3/self_search/cluster/self_search.acc'\n",
    "hh_search = pd.read_csv(search_file, sep='\\t', header=None, names=['cluster_acc','acc'])\n",
    "hh_search = hh_search.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a222af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_file_euk = '/home/tobiassonva/data/eukgen/processing/euk72_ep3/euk72_ep.repseq.lookup'\n",
    "lookup_file_prok = '/home/tobiassonva/data/eukgen/processing/prok2111_as/prok2111_as.repseq.lookup'\n",
    "lookup = pd.read_csv(lookup_file_euk, sep='\\t', header=None, index_col=1)\n",
    "# lookup = pd.read_csv(lookup_file_prok, sep='\\t', header=None, index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b6d0a61",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lookup\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalue_counts()[\u001b[43mlookup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.9/lib/python3.9/site-packages/pandas/core/base.py:1015\u001b[0m, in \u001b[0;36mIndexOpsMixin.value_counts\u001b[0;34m(self, normalize, sort, ascending, bins, dropna)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue_counts\u001b[39m(\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    935\u001b[0m     dropna: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    936\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m    937\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;124;03m    Return a Series containing counts of unique values.\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;124;03m    Name: count, dtype: int64\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_counts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mascending\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.9/lib/python3.9/site-packages/pandas/core/algorithms.py:899\u001b[0m, in \u001b[0;36mvalue_counts\u001b[0;34m(values, sort, ascending, normalize, bins, dropna)\u001b[0m\n\u001b[1;32m    896\u001b[0m         result \u001b[38;5;241m=\u001b[39m Series(counts, index\u001b[38;5;241m=\u001b[39midx, name\u001b[38;5;241m=\u001b[39mname, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sort:\n\u001b[0;32m--> 899\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mascending\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[1;32m    902\u001b[0m     result \u001b[38;5;241m=\u001b[39m result \u001b[38;5;241m/\u001b[39m counts\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.9/lib/python3.9/site-packages/pandas/core/series.py:3634\u001b[0m, in \u001b[0;36mSeries.sort_values\u001b[0;34m(self, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   3632\u001b[0m \u001b[38;5;66;03m# GH 35922. Make sorting stable by leveraging nargsort\u001b[39;00m\n\u001b[1;32m   3633\u001b[0m values_to_sort \u001b[38;5;241m=\u001b[39m ensure_key_mapped(\u001b[38;5;28mself\u001b[39m, key)\u001b[38;5;241m.\u001b[39m_values \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 3634\u001b[0m sorted_index \u001b[38;5;241m=\u001b[39m \u001b[43mnargsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues_to_sort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mascending\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_position\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_range_indexer(sorted_index, \u001b[38;5;28mlen\u001b[39m(sorted_index)):\n\u001b[1;32m   3637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inplace:\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.9/lib/python3.9/site-packages/pandas/core/sorting.py:429\u001b[0m, in \u001b[0;36mnargsort\u001b[0;34m(items, kind, ascending, na_position, key, mask)\u001b[0m\n\u001b[1;32m    427\u001b[0m     non_nans \u001b[38;5;241m=\u001b[39m non_nans[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    428\u001b[0m     non_nan_idx \u001b[38;5;241m=\u001b[39m non_nan_idx[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 429\u001b[0m indexer \u001b[38;5;241m=\u001b[39m non_nan_idx[\u001b[43mnon_nans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ascending:\n\u001b[1;32m    431\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lookup.index.value_counts()[lookup.index.value_counts() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "aff5b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip header to match accession\n",
    "# hh_search.Query = [n.split(' ')[0] for n in hh_search.Query]\n",
    "# hh_search.Target = [n.split(' ')[0] for n in hh_search.Target]\n",
    "\n",
    "#replace poorly parsed accessions \n",
    "#hh_search.replace({'pir||A44923': 'A44923', 'prf||1111187C': '1111187C'}, inplace=True)\n",
    "\n",
    "#drop all extra rows\n",
    "#hh_search = hh_search.iloc[:, :2]\n",
    "\n",
    "hh_search.to_csv('/home/tobiassonva/data/eukgen/processing/euk72_ep2/self_search/euk72_ep.hits.tsv', sep='\\t', header=None, index=None)\n",
    "pd.Series(hh_search.Target.unique()).to_csv('/home/tobiassonva/data/eukgen/processing/euk72_ep2/self_search/euk72_ep.unique_accs.tsv', sep='\\t', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8d1aca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NP_904080.1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NP_178089.1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NP_001322245.1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XP_006677078.1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NP_506559.1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EP01083P185297</th>\n",
       "      <td>30375741</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EP01083P218065</th>\n",
       "      <td>30375742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EP01083P250833</th>\n",
       "      <td>30375743</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EP01083P283601</th>\n",
       "      <td>30375744</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EP01083P316369</th>\n",
       "      <td>30375745</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30375746 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0  2\n",
       "1                          \n",
       "NP_904080.1            0  0\n",
       "NP_178089.1            1  0\n",
       "NP_001322245.1         2  0\n",
       "XP_006677078.1         3  0\n",
       "NP_506559.1            4  0\n",
       "...                  ... ..\n",
       "EP01083P185297  30375741  0\n",
       "EP01083P218065  30375742  0\n",
       "EP01083P250833  30375743  0\n",
       "EP01083P283601  30375744  0\n",
       "EP01083P316369  30375745  0\n",
       "\n",
       "[30375746 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83ba1122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.98 s, sys: 7.13 ms, total: 4.99 s\n",
      "Wall time: 5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#translate to mmseqs index\n",
    "\n",
    "hh_search_i = pd.DataFrame()\n",
    "\n",
    "hh_search_i['cluster_acc_i'] = lookup.loc[hh_search['cluster_acc']][0].values\n",
    "hh_search_i['acc_i'] = lookup.loc[hh_search['acc']][0].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67c2f40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1542971,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh_search_i.cluster_acc_i.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c7cbe06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1542971,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh_search_i.acc_i.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8646f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(hh_search_i.cluster_acc_i.unique()).to_csv(search_file+'.i.unique',sep='\\t', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97bad07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_search_i.to_csv(search_file+'.i',sep='\\t', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2215417a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1541367,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh_search_i.cluster_acc_i.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "970cbfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(hh_search_i.cluster_acc_i.unique()).to_csv(search_file+'.query_unique', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39307e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a8dca2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_search_index = pd.DataFrame(columns=['Query', 'Target'])\n",
    "hh_search_index.Query = lookup.loc[hh_search.Query][0].values\n",
    "hh_search_index.Target = lookup.loc[hh_search.Target][0].values\n",
    "\n",
    "hh_search_index.to_csv('/home/tobiassonva/data/eukgen/processing/euk72_ep2/self_search/euk72_ep.hits_i.tsv', sep='\\t', header=None, index=None)\n",
    "pd.Series(hh_search_index.Target.unique()).to_csv('/home/tobiassonva/data/eukgen/processing/euk72_ep2/self_search/euk72_ep.unique_accs_i.tsv', sep='\\t', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9b8488b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1\n",
       "NP_904080.1       1\n",
       "XP_024309639.1    1\n",
       "XP_001448410.1    1\n",
       "XP_026411480.1    1\n",
       "XP_026458534.1    1\n",
       "                 ..\n",
       "XP_008648114.1    1\n",
       "XP_019076432.1    1\n",
       "XP_002274411.1    1\n",
       "XP_001322998.1    1\n",
       "XP_005770914.1    1\n",
       "Name: count, Length: 1814550, dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup.index.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "809adb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Query, Target]\n",
       "Index: []"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh_search_index[hh_search_index.Query =='11041657']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba2914bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('/data/tobiassonva/data/eukgen/processing/euk72_ep3/self_search/tmp_head', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67da56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "733834c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     8.950320e+05\n",
       "mean      4.641312e-03\n",
       "std       4.574427e-02\n",
       "min       0.000000e+00\n",
       "25%      4.000000e-169\n",
       "50%       2.000000e-85\n",
       "75%       2.600000e-25\n",
       "max       1.100000e+00\n",
       "Name: E-value, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[(a.Pairwise_cov > 0.5) & (a.Prob > 80)]['E-value'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "762bff97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(553445,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc305b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_search[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a46ef97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Query     2938585\n",
       "Target    2938585\n",
       "Name: 181943, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh_search_index.loc[181943,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0e63b88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181934</th>\n",
       "      <td>18933872</td>\n",
       "      <td>18933872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Query    Target\n",
       "181934  18933872  18933872"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh_search_index[hh_search.Query == '1111187C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a58d72a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (10039) does not match length of index (10030)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.9/lib/python3.9/site-packages/pandas/core/frame.py:3950\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3947\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3948\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3949\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 3950\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.9/lib/python3.9/site-packages/pandas/core/frame.py:4143\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4134\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4135\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4136\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4141\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4142\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4143\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4146\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4147\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4148\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   4149\u001b[0m     ):\n\u001b[1;32m   4150\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4151\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.9/lib/python3.9/site-packages/pandas/core/frame.py:4870\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   4869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 4870\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4871\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.9/lib/python3.9/site-packages/pandas/core/common.py:576\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (10039) does not match length of index (10030)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "hh_search_index = pd.DataFrame(columns=['Query', 'Target'])\n",
    "\n",
    "\n",
    "hh_search_index['Query'] = lookup.loc[hh_search.Query.values[0:10000]][0].values\n",
    "hh_search_index['Target'] = lookup.loc[hh_search.Target.values[0:10000]][0].values\n",
    "\n",
    "hh_search_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e22893a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XP_005535545.1</td>\n",
       "      <td>XP_005535545.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CBJ26689.1</td>\n",
       "      <td>CBJ26689.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CBJ26982.1</td>\n",
       "      <td>CBJ26982.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CBJ26982.1</td>\n",
       "      <td>CBJ26982.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CBJ26982.1</td>\n",
       "      <td>CBJ26982.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244959</th>\n",
       "      <td>CEM09677.1</td>\n",
       "      <td>CEM09677.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244960</th>\n",
       "      <td>CEM07901.1</td>\n",
       "      <td>CEM07901.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244961</th>\n",
       "      <td>CEM11326.1</td>\n",
       "      <td>CEM11326.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244962</th>\n",
       "      <td>CEM14633.1</td>\n",
       "      <td>CEM14633.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244963</th>\n",
       "      <td>CEM12967.1</td>\n",
       "      <td>CEM12967.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244964 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Query          Target\n",
       "0       XP_005535545.1  XP_005535545.1\n",
       "1           CBJ26689.1      CBJ26689.1\n",
       "2           CBJ26982.1      CBJ26982.1\n",
       "3           CBJ26982.1      CBJ26982.1\n",
       "4           CBJ26982.1      CBJ26982.1\n",
       "...                ...             ...\n",
       "244959      CEM09677.1      CEM09677.1\n",
       "244960      CEM07901.1      CEM07901.1\n",
       "244961      CEM11326.1      CEM11326.1\n",
       "244962      CEM14633.1      CEM14633.1\n",
       "244963      CEM12967.1      CEM12967.1\n",
       "\n",
       "[244964 rows x 2 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b87afe1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acc\n",
       "GBG58979.1            GBG58979.1\n",
       "NP_509080.2          NP_509080.2\n",
       "XP_001311970.1    XP_001311970.1\n",
       "XP_002838044.1    XP_002838044.1\n",
       "CEL96693.1            CEL96693.1\n",
       "                       ...      \n",
       "XP_009053570.1    XP_009053570.1\n",
       "OLP74334.1            OLP74334.1\n",
       "OAE22863.1            OAE22863.1\n",
       "XP_004346567.1    XP_004346567.1\n",
       "PTQ45874.1            PTQ45874.1\n",
       "Name: cluster_acc, Length: 192002, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "casc_clusters = casc_clust.set_index('acc').loc[hh_clust.acc].cluster_acc\n",
    "casc_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ef74c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster_acc</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GBG58979.1</th>\n",
       "      <td>GBG58979.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NP_509080.2</th>\n",
       "      <td>NP_509080.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XP_001311970.1</th>\n",
       "      <td>XP_001311970.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XP_002838044.1</th>\n",
       "      <td>XP_002838044.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CEL96693.1</th>\n",
       "      <td>CEL96693.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OLP74334.1</th>\n",
       "      <td>OLP74334.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OAE22863.1</th>\n",
       "      <td>OAE22863.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XP_004346567.1</th>\n",
       "      <td>XP_004346567.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTQ45874.1</th>\n",
       "      <td>PTQ45874.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTQ45874.1</th>\n",
       "      <td>OAE21737.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1092087 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           acc\n",
       "cluster_acc                   \n",
       "GBG58979.1          GBG58979.1\n",
       "NP_509080.2        NP_509080.2\n",
       "XP_001311970.1  XP_001311970.1\n",
       "XP_002838044.1  XP_002838044.1\n",
       "CEL96693.1          CEL96693.1\n",
       "...                        ...\n",
       "OLP74334.1          OLP74334.1\n",
       "OAE22863.1          OAE22863.1\n",
       "XP_004346567.1  XP_004346567.1\n",
       "PTQ45874.1          PTQ45874.1\n",
       "PTQ45874.1          OAE21737.1\n",
       "\n",
       "[1092087 rows x 1 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "casc_clust.set_index('cluster_acc').loc[casc_clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88f166f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GBG58979.1', 'NP_509080.2', 'XP_001311970.1', 'XP_002838044.1',\n",
       "       'CEL96693.1', 'EP00615P012161', 'EP00615P004769', 'EP00615P003329',\n",
       "       'EP00615P017473', 'EP00615P007201',\n",
       "       ...\n",
       "       'XP_008897273.1', 'XP_020406395.1', 'XP_001745180.1', 'NP_012105.1',\n",
       "       'GBG59653.1', 'XP_009053570.1', 'OLP74334.1', 'OAE22863.1',\n",
       "       'XP_004346567.1', 'PTQ45874.1'],\n",
       "      dtype='object', name='acc', length=192002)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "casc_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4e89c33",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['GBG58979.1', 'NP_509080.2', 'XP_001311970.1', 'XP_002838044.1',\\n       'CEL96693.1', 'EP00615P012161', 'EP00615P004769', 'EP00615P003329',\\n       'EP00615P017473', 'EP00615P007201',\\n       ...\\n       'XP_008897273.1', 'XP_020406395.1', 'XP_001745180.1', 'NP_012105.1',\\n       'GBG59653.1', 'XP_009053570.1', 'OLP74334.1', 'OAE22863.1',\\n       'XP_004346567.1', 'PTQ45874.1'],\\n      dtype='object', length=192002)] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcasc_clust\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcasc_clusters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.9/lib/python3.9/site-packages/pandas/core/indexing.py:1103\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1100\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1102\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.9/lib/python3.9/site-packages/pandas/core/indexing.py:1332\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.9/lib/python3.9/site-packages/pandas/core/indexing.py:1272\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1272\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1274\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.9/lib/python3.9/site-packages/pandas/core/indexing.py:1462\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1459\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1460\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1462\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5877\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5879\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5881\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Anaconda/envs/py3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5937\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 5938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5940\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   5941\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['GBG58979.1', 'NP_509080.2', 'XP_001311970.1', 'XP_002838044.1',\\n       'CEL96693.1', 'EP00615P012161', 'EP00615P004769', 'EP00615P003329',\\n       'EP00615P017473', 'EP00615P007201',\\n       ...\\n       'XP_008897273.1', 'XP_020406395.1', 'XP_001745180.1', 'NP_012105.1',\\n       'GBG59653.1', 'XP_009053570.1', 'OLP74334.1', 'OAE22863.1',\\n       'XP_004346567.1', 'PTQ45874.1'],\\n      dtype='object', length=192002)] are in the [index]\""
     ]
    }
   ],
   "source": [
    "casc_clust.loc[casc_clusters.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf438f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/tobiassonva/data/eukgen/processing/euk72_ep/'\n",
    "with open(root+'euk72_ep.test_clust', 'r') as infile:\n",
    "    clusters = [line for line in infile.read().split('\\n')]\n",
    "    \n",
    "cluster_seqs_lookup = pd.read_csv(root+'euk72_ep.test_seqs.lookup', sep='\\t', header=None, names = ['a', 'b', 'c'])\n",
    "repseq_seqs_lookup = pd.read_csv(root+'euk72_ep.repseq.lookup', sep='\\t', header=None, names = ['a', 'b', 'c'])\n",
    "\n",
    "cluster_seqs_lookup.set_index('a', inplace=True)\n",
    "repseq_seqs_lookup_reverse = repseq_seqs_lookup.set_index('a')\n",
    "repseq_seqs_lookup.set_index('b', inplace=True)\n",
    "\n",
    "cluster_seqs_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "repseq_seqs_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a073ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "repseq_seqs_lookup_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0100a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idlines = [int(line.strip('\\x00')) for line in clusters if line.strip('\\x00') != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_seqs_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0380a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cluster_seqs_lookup.loc[idlines].b\n",
    "repseq_accs = repseq_seqs_lookup.loc[cluster_seqs_lookup.loc[idlines].b]\n",
    "repseq_ids = repseq_accs[~repseq_accs.index.duplicated(keep='first')].a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe64403",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [False if i[0] == \"\\x00\" else True for i in clusters]\n",
    "with open(root+'euk72_ep.test_clust_edit', 'w') as out:\n",
    "    for i, n in enumerate(repseq_ids):\n",
    "        print(i,n)\n",
    "        if indexes[i]:\n",
    "            out.write(str(n)+'\\n')\n",
    "        else:\n",
    "            out.write('\\x00'+str(n)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f733ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f0c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python/3.9",
   "language": "python",
   "name": "py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
