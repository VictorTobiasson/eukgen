{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a2d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiprocessing\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "#numerics and vis\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "#disable altair max rows\n",
    "alt.data_transformers.disable_max_rows()\n",
    "#get default altair style\n",
    "%run ~/scripts/altair_style_config_default.py\n",
    "\n",
    "#trees\n",
    "import ete3\n",
    "from ete3 import Tree, TreeStyle, TextFace\n",
    "\n",
    "\n",
    "root = '/home/tobiassonva/data/eukgen/'\n",
    "%cd {root}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bef718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#small helper for pkl parsing\n",
    "def load_pkl(pkl_file):\n",
    "    import pickle\n",
    "    with open(pkl_file, 'rb') as infile:\n",
    "        item = pickle.load(infile)\n",
    "    return item\n",
    "\n",
    "euk_queries_test2 = ['CBN77353.1', 'CEL94470.1', 'CEL98020.1', 'CEM00912.1',\n",
    "       'CEM13793.1', 'CEO94447.1', 'CEP02189.1', 'CEP02404.1',\n",
    "       'EPZ31333.1', 'GBG32138.1', 'GBG34166.1', 'GBG34636.1',\n",
    "       'GBG88810.1', 'KAA0151157.1', 'KAA0167757.1', 'KAA6364588.1',\n",
    "       'KAA6383781.1', 'NP_001022034.1', 'NP_001105121.2',\n",
    "       'NP_001170744.1', 'NP_001189295.1', 'NP_001242666.1',\n",
    "       'NP_001259573.1', 'NP_001261837.1', 'NP_001294564.1',\n",
    "       'NP_001307934.1', 'NP_001328712.1', 'NP_012528.1', 'NP_050092.1',\n",
    "       'NP_051148.1', 'NP_189541.1', 'NP_197350.1', 'NP_498455.2',\n",
    "       'NP_505960.3', 'NP_588329.1', 'NP_595422.1', 'NP_609709.1',\n",
    "       'NP_611238.2', 'NP_649295.1', 'OAD00700.1', 'OAD03858.1',\n",
    "       'OAD05886.1', 'OAE33051.1', 'OLP78629.1', 'OLQ06972.1',\n",
    "       'OLQ08228.1', 'OLQ08510.1', 'OLQ11720.1', 'OLQ12045.1',\n",
    "       'OLQ14344.1']\n",
    "\n",
    "euk_queries_test3 = ['XP_008911403.1', 'XP_011408184.1', 'XP_002681038.1', 'XP_002673113.1',\n",
    "                     'OAD09041.1', 'XP_001634466.1', 'XP_005765180.1', 'XP_011407364.1', \n",
    "                     'XP_005789988.1', 'KAA6344160.1', 'KAA6409619.1', 'XP_002287408.1',\n",
    "                     'OAE21175.1', 'RKP17192.1', 'XP_013760427.1', 'KAA0163767.1',\n",
    "                     'XP_002119908.1', 'XP_009692086.1']\n",
    "\n",
    "euk_queries_test4 = ['NP_001002332.1', 'NP_001240313.1',\n",
    "       'NP_001259573.1', 'NP_001260847.1', \n",
    "       'NP_001278869.1', 'NP_001307724.1', 'NP_001307934.1', 'NP_001334755.1',\n",
    "       'NP_001356620.1', 'NP_115888.1', \n",
    "       'NP_610753.1', 'NP_611238.2', 'NP_956312.1',\n",
    "       'NP_998197.1', 'NP_998403.1', 'XP_005256905.1',  'XP_017206845.1', 'XP_021326060.1', 'XP_021336265.1']\n",
    "\n",
    "euk_queries_test5 = ['AGK83073.1', 'CBN73833.1', 'CBN79086.1', 'CEM35385.1', 'CEO94447.1', 'CEP00213.1', 'CEP03651.1', 'EPZ30938.1', 'EPZ31301.1', 'GBG60132.1', \n",
    "'GBG70565.1', 'GBG80562.1', 'GBG83744.1', 'KAA0165271.1', 'KAA0172078.1', 'KAA6408708.1', 'NP_001002332.1', 'NP_001259573.1', 'NP_001294564.1',\n",
    " 'NP_001307724.1', 'NP_001328712.1', 'NP_001334755.1', 'NP_001356620.1', 'NP_011081.1', 'NP_050092.1', 'NP_594946.1', 'NP_610753.1', 'NP_848958.1',\n",
    "  'NP_849074.1', 'NP_956312.1', 'NP_998197.1', 'OAD04802.1', 'OAD06369.1', 'OAE33370.1', 'OLP84660.1', 'OLQ06972.1', 'OLQ08228.1', 'OLQ14344.1',\n",
    "   'OSX69435.1', 'OSX71470.1', 'OSX72678.1', 'OSX75094.1', 'OSX77054.1', 'PTQ50428.1', 'PXF41822.1', 'PXF45288.1', 'RKP17849.1', 'RKP18091.1', \n",
    "   'RKP20265.1', 'RWR93989.1', 'RWR97906.1', 'RWR98344.1', 'SLM34047.1', 'SLM40311.1', 'SLM40671.1', 'SPQ96285.1', 'SPQ98172.1']\n",
    "\n",
    "\n",
    "#euk_clust = load_pkl(root+'analysis/core_data/euk72_filtered-prof-search-clust.pkl')['members']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f11dc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate cumulative sum and distribution for pd.Series\n",
    "#takes pd.Series as input and returns a parsed DF and altair chart object \n",
    "def plot_cumsum_counts(series, title='Chart', x_label='value', y_label='count', \n",
    "                       x_min=0, y_min=0, x_max=None, y_max=None,\n",
    "                       x_scale_type='log', y_scale_type='log', decimals=2):\n",
    "    \n",
    "    #format DF for data handling, filter 0 values for plot \n",
    "    #round to reduce float data display jaggedness\n",
    "    series = series[series!=0].round(decimals)\n",
    "    \n",
    "    #format distribution dataframe\n",
    "    countDF = pd.DataFrame(series.value_counts())\n",
    "    countDF.columns = ['amount']\n",
    "    countDF.sort_index(inplace=True)\n",
    "    countDF['cumsum'] = countDF['amount'].cumsum()\n",
    "    countDF['frac_cumsum'] = countDF['cumsum']/countDF['cumsum'].max()\n",
    "    countDF.reset_index(inplace=True)\n",
    "    \n",
    "    #rename columns for plotting\n",
    "    countDF.columns = [x_label,y_label,'cumsum','frac_cumsum']\n",
    "\n",
    "    #format axis domains\n",
    "    x_range = [x_min, series.max()]\n",
    "    y_range = [y_min, countDF[y_label].max()]\n",
    "    \n",
    "    if x_max:\n",
    "        x_range = [x_min, x_max]\n",
    "    \n",
    "    if y_max:\n",
    "        y_range = [y_min, y_max]\n",
    "        \n",
    "    #plot cumulative distribution\n",
    "    chart_cumsum = alt.Chart(countDF, title=title).mark_line(color=colorlib['twilight_shifted_r_perm'][2],\n",
    "                                              strokeWidth=3).encode(\n",
    "        x=alt.X(x_label, title=x_label, scale=alt.Scale(type=x_scale_type)),\n",
    "        y=alt.Y('frac_cumsum', title='Cumulative Fraction', scale=alt.Scale(domain=[0,1]), axis=alt.Axis(labelAlign='left')),\n",
    "        tooltip=alt.Tooltip([x_label, y_label, 'frac_cumsum'])\n",
    "    )\n",
    "    \n",
    "    #plot value distribution\n",
    "    chart_bar = alt.Chart(countDF).mark_area(interpolate='step-after', \n",
    "                                            fillOpacity=0.2, line=True).encode(\n",
    "        x=alt.X(x_label+':Q', scale=alt.Scale(domain=x_range, type=x_scale_type)),\n",
    "        y=alt.Y(y_label, scale=alt.Scale(domain=y_range, type=y_scale_type)),\n",
    "        tooltip=alt.Tooltip([x_label, y_label, 'frac_cumsum'])\n",
    "    )\n",
    "\n",
    "    #merge and configure\n",
    "    merge = alt.layer(chart_bar, chart_cumsum).resolve_scale(y='independent').interactive()\n",
    "\n",
    "    return countDF, merge\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0929f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper IO functions for basic fasta reading into {id:seq} dict\n",
    "#id taken as string up until first space character\n",
    "def fasta_to_dict(fastastring=None, file=None):\n",
    "    \n",
    "    if file != None:\n",
    "        with open(file, 'r') as fastafile:\n",
    "            fastastring = fastafile.read()\n",
    "    \n",
    "    entries = [entry.strip() for entry in ''.join(fastastring).split('>') if entry != '']\n",
    "    fastas = {entry.split('\\n')[0].split(' ')[0]: ''.join(entry.split('\\n')[1:]) for entry in entries}\n",
    "    \n",
    "    #replace unknown characters with A\n",
    "    blacklist = set('BJUXZ')\n",
    "    for key, seq in fastas.items():\n",
    "        \n",
    "        for char in set(seq):\n",
    "            if char in blacklist:\n",
    "                seq = seq.replace(char, 'A')\n",
    "                print(f'WARNING: {key} Replaced illegal {char} with A')\n",
    "                \n",
    "        fastas[key] = seq\n",
    "    return fastas\n",
    "\n",
    "#returns simple single line fasta from {id:seq} dict\n",
    "def dict_to_fasta(seq_dict, write_file=False):\n",
    "    \n",
    "    fasta_str =  '\\n'.join(f'>{key}\\n{value}' for key, value in seq_dict.items())\n",
    "    \n",
    "    if write_file != False:\n",
    "        with open(write_file, 'w') as outfile:\n",
    "            outfile.write(fasta_str)\n",
    "        print(f'Wrote {write_file}')\n",
    "            \n",
    "    return fasta_str\n",
    "\n",
    "#helper command for submitting subprocesses to the current shell via subprocesscan accept piped data\n",
    "#equivalent to \"$ command < stdin > stdout\" if given a stdin string\n",
    "#command string should be single space delimited unix command ex \"clustalo -i test -o test.clu\"\n",
    "def run_subprocess_with_stdin(command_str, stdin_str=''):\n",
    "    command = command_str.split(' ')\n",
    "    command_process = subprocess.Popen(command, stdin=subprocess.PIPE, \n",
    "                                       stdout=subprocess.PIPE, \n",
    "                                       stderr=subprocess.PIPE)\n",
    "    \n",
    "    #input and output is taken as bytes so encode and decode inputs\n",
    "    stdout, stderr = command_process.communicate(input=stdin_str.encode('utf-8'))\n",
    "    \n",
    "    #hhconsensus occationally produces additional bytes at the end of the consensus sequence\n",
    "    #errors=ignore should not write these but ocasionally do?\n",
    "    #these appeat not to affect downstream processing but are wierd\n",
    "    return stdout.decode('utf-8',errors=\"ignore\"), stderr.decode('utf-8',errors=\"ignore\")\n",
    "\n",
    "\n",
    "#iterate over seq and create a boolean list of insertions as False\n",
    "#apply_matchlist then replaces false with '-' and True from sample string at index\n",
    "def calculate_matchlist(seq, ref_seq):\n",
    "    ref_i = 0\n",
    "    max_i = len(ref_seq)-1\n",
    "    matchlist = []\n",
    "\n",
    "    for i, c in enumerate(seq):\n",
    "        ref_i = min(ref_i, max_i)\n",
    "\n",
    "        if c == ref_seq[ref_i] and ref_seq[ref_i] != '-':\n",
    "            matchlist.append(True)\n",
    "            ref_i += 1\n",
    "\n",
    "        else:\n",
    "            matchlist.append(False)\n",
    "\n",
    "    return matchlist\n",
    "\n",
    "#take list of gaps as [True, False, ...] where False indicate gaps\n",
    "#replace matches (True) with seq from sequence\n",
    "def apply_matchlist(seq, matchlist):\n",
    "    \n",
    "    ref_i = 0\n",
    "    new_seq_list = matchlist.copy()\n",
    "\n",
    "    for i, b in enumerate(new_seq_list):\n",
    "        if b:\n",
    "            new_seq_list[i] = seq[ref_i]\n",
    "            ref_i += 1\n",
    "        else:\n",
    "            new_seq_list[i] = '-'\n",
    "\n",
    "    return ''.join(new_seq_list)\n",
    "\n",
    "#define the entropy for a string given amino acids, protein=True or, DNA protein=False\n",
    "def column_entropy(string, protein=True, gaptoken='-'):\n",
    "    \n",
    "    size = len(string)\n",
    "    counts = [string.count(i) for i in set(string).difference({gaptoken})]\n",
    "    entropy = -sum([i/size*math.log2(i/size) for i in counts])\n",
    "    \n",
    "    if protein:\n",
    "        entropy_uniform = math.log2(20)\n",
    "    else:\n",
    "        entropy_uniform = 2\n",
    "        \n",
    "    gap_entropy = entropy_uniform*(string.count(gaptoken)/size)\n",
    "    information = entropy_uniform - entropy - gap_entropy\n",
    "    \n",
    "    return max(information,0)\n",
    "\n",
    "\n",
    "\n",
    "#columnwise cut based on criteria\n",
    "def filter_by_entropy(seq_dict, entropy_min, filter_accs=[], gaptoken='-'):\n",
    "        \n",
    "    #transpose seqs into columns\n",
    "    cols = [''.join(seq) for seq in list(zip(*seq_dict.values()))]\n",
    "    \n",
    "    #filter only by columns present in filter_accs keys\n",
    "    if filter_accs:\n",
    "        filter_dict = {key:value for key, value in seq_dict.items() if key in filter_accs}\n",
    "        filter_cols = [''.join(seq) for seq in list(zip(*filter_dict.values()))]\n",
    "    \n",
    "    else:\n",
    "        filter_cols = cols\n",
    "    \n",
    "    #include based on entropy threshold\n",
    "    filter_cols = [col for i, col in enumerate(cols) if column_entropy(filter_cols[i]) > entropy_min]\n",
    "\n",
    "    #transpose back to alignment\n",
    "    filter_aln = [''.join(col) for col in list(zip(*filter_cols))]\n",
    "    \n",
    "    #return with original keys\n",
    "    return {key: value for key, value in zip(seq_dict.keys(), filter_aln)}\n",
    "\n",
    "\n",
    "\n",
    "#input helper to read tsv from file, merge singletons\n",
    "def read_cluster_tsv(cluster_file, split_large=False, max_size=500, batch_single=False, single_cutoff=1):   \n",
    "\n",
    "    #read TSV and group clusters based on first tsv column\n",
    "    with open(cluster_file, 'r') as infile:\n",
    "        clusters = {}\n",
    "\n",
    "        for l in infile.readlines():\n",
    "            cluster_acc, acc = l.strip().split('\\t')\n",
    "\n",
    "            if cluster_acc not in clusters.keys():\n",
    "                clusters[cluster_acc] = [acc]\n",
    "\n",
    "            else:\n",
    "                clusters[cluster_acc].append(acc)\n",
    "    \n",
    "    #merge all clusters smaller than cutoff into one\n",
    "    if batch_single:\n",
    "        filter_dict = {}\n",
    "        singles = []\n",
    "        for key, accs in clusters.items():  \n",
    "            #gather singletons\n",
    "            if len(accs) <= single_cutoff:\n",
    "                singles.extend(accs)\n",
    "            \n",
    "            #keep larger clusters\n",
    "            else:\n",
    "                filter_dict[key] = accs\n",
    "                \n",
    "        if singles:\n",
    "            filter_dict[singles[0]] = singles\n",
    "        \n",
    "        clusters = filter_dict\n",
    "    \n",
    "    #split clusters larger than x into smaller pieces\n",
    "    if split_large:\n",
    "        filter_dict = {}\n",
    "        for key, accs in clusters.items():  \n",
    "            if len(accs) > max_size:\n",
    "                #partition large clusters into batches of max_size\n",
    "                for split in range(0, len(accs), max_size):\n",
    "                    batch = accs[split:split + max_size]\n",
    "                    filter_dict[batch[0]] = batch\n",
    "\n",
    "            #keep smaller clusters\n",
    "            else:\n",
    "                filter_dict[key] = accs\n",
    "                \n",
    "        clusters = filter_dict         \n",
    "        \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0e7010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrapper function for consensus alignment to track thread numbering etc.\n",
    "def pool_align_with_consensus(run_seq_dict, return_consensus=True):\n",
    "    \n",
    "    thread = multiprocessing.current_process().pid\n",
    "    ref_seq = list(run_seq_dict.keys())[0]\n",
    "    #print(f'{thread}: ref_seq = {ref_seq} started with {len(run_seq_dict)} sequences\\n', end='')\n",
    "    \n",
    "    #filter seqs from \n",
    "    run_seq_dict_consensus = align_with_consensus(run_seq_dict, return_consensus)\n",
    "    \n",
    "    #print(f'{thread} finished\\n', end='')\n",
    "    \n",
    "    #return dict with key = first sequence id from run_seq_dict\n",
    "    return {ref_seq: run_seq_dict_consensus}\n",
    "\n",
    "\n",
    "#wrapper for clustalo alignment from stdin piped to hhconsensus capturing stdout\n",
    "#avoids file generation\n",
    "def align_with_consensus(seq_dict, return_consensus=True):\n",
    "    \n",
    "    #format fasta-like string from seqs_dict\n",
    "    aligner_in = '\\n'.join(f'>{key}\\n{value}' for key, value in seq_dict.items())\n",
    "    \n",
    "    aligner_command = 'clustalo -i - --threads 1'\n",
    "    #aligner_command = 'mafft --quiet --auto /dev/stdin'\n",
    "    hhconsensus_command = 'hhconsensus -i stdin -o stdout'\n",
    "\n",
    "    print(f'Aligning {len(seq_dict.keys())} seqs\\n', end='')\n",
    "\n",
    "    aligner_stdout, aligner_stderr = run_subprocess_with_stdin(aligner_command, aligner_in)\n",
    "    \n",
    "    if return_consensus:\n",
    "        hhcons_stdout, hhcons_stderr = run_subprocess_with_stdin(hhconsensus_command, aligner_stdout)\n",
    "        hhcons_fasta = '\\n'.join(['>consensus']+hhcons_stdout.split('\\n')[2:])\n",
    "        seq_dict = fasta_to_dict(hhcons_fasta)\n",
    "    \n",
    "    else:\n",
    "        seq_dict = fasta_to_dict(aligner_stdout)\n",
    "    \n",
    "    return seq_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def hierarchical_realignment(seq_fasta, cluster_file, filter_entropy=False, parallel_n=None):\n",
    "    \n",
    "    thread = multiprocessing.current_process().pid\n",
    "    threadID_string = f'{thread} | {seq_fasta.split(\"/\")[-1]} alignment:'\n",
    "    \n",
    "    aligner_threads = 1\n",
    "    if parallel_n != None:\n",
    "        aligner_threads = parallel_n\n",
    "        \n",
    "    with open(seq_fasta, 'r') as infile:\n",
    "        seq_dict = fasta_to_dict(''.join(infile.readlines()))\n",
    "\n",
    "    #split larger clusters to avoid large realignments\n",
    "    max_size = 200\n",
    "    singleton_threshold = 10\n",
    "\n",
    "    clusters = read_cluster_tsv(cluster_file, split_large=True, max_size=max_size,\n",
    "                                batch_single=True, single_cutoff=singleton_threshold)\n",
    "\n",
    "    #in some edge cases partitioning and merginf clusters froms a single sequence non-singleton cluster\n",
    "    #add all clusters larger than one to realignment list\n",
    "    seq_dicts = [{key:seq_dict[key] for key in seqs} for seqs in clusters.values() if len(seqs) > 1]\n",
    "\n",
    "    \n",
    "    #add those lonley edge sequences to a separate dict for merger later\n",
    "    singles_dict = {seq[0]:seq_dict[seq[0]] for seq in clusters.values() if len(seq) == 1}\n",
    "    \n",
    "    print(f'{threadID_string} realigning a total of {len(seq_dicts)} clusters')\n",
    "    \n",
    "    \n",
    "    #return seq_dicts, seq_dicts2, singles_dict\n",
    "    \n",
    "    if parallel_n != None:\n",
    "        #multiprocessing\n",
    "        print(f'{threadID_string} Cluster alignment')\n",
    "        with multiprocessing.Pool(processes=parallel_n) as pool:\n",
    "            cluster_alignments_stream = pool.map(pool_align_with_consensus, seq_dicts)\n",
    "    \n",
    "    else:\n",
    "        #serial implementation\n",
    "        cluster_alignments_stream = []\n",
    "        for seq_dict in seq_dicts:\n",
    "            cluster_alignments_stream.append(pool_align_with_consensus(seq_dict))\n",
    "    \n",
    "    #merge and order results from stream \n",
    "    cluster_alignments = {key: value for result in cluster_alignments_stream for key, value in result.items()}\n",
    "    \n",
    "    #merge consensus sequences with singletons and realign\n",
    "    consensus_dict = {key: value['consensus'] for key, value in cluster_alignments.items()}\n",
    "    print(f'{threadID_string} Running consensus alignment of length {len(consensus_dict)}')\n",
    "    \n",
    "    outfile = dict_to_fasta(consensus_dict, write_file=f'{seq_fasta}.cons.fasta')\n",
    "    \n",
    "    #add edge case singles as consensus sequences\n",
    "    consensus_dict.update(singles_dict)\n",
    "\n",
    "    aligner_command = f'clustalo -i - --threads {aligner_threads}'\n",
    "    aligner_in = dict_to_fasta(consensus_dict)\n",
    "\n",
    "    aligner_stdout, aligner_stderr = run_subprocess_with_stdin(aligner_command, aligner_in)\n",
    "    singles_consensus_alignment_dict = fasta_to_dict(aligner_stdout)\n",
    "    \n",
    "    outfile = dict_to_fasta(singles_consensus_alignment_dict, write_file=f'{seq_fasta}.cons.clu')\n",
    "    \n",
    "    print(f'{threadID_string} Recombining cluster and consensus alignments')\n",
    "    #replace consensus sequences with original aligned sequences\n",
    "    for cluster_acc, cluster_alignment in cluster_alignments.items():\n",
    "\n",
    "        #set the reference and realigned consensus sequences\n",
    "        realigned_consensus_seq = singles_consensus_alignment_dict[cluster_acc]\n",
    "        consensus_seq = cluster_alignment['consensus']\n",
    "\n",
    "        #calculate the matches\n",
    "        matchlist = calculate_matchlist(realigned_consensus_seq, consensus_seq)\n",
    "\n",
    "        #for each cluster member apply the transformation to yield \"realigned\" version\n",
    "        #and add the realigned verison to the singles dictionary\n",
    "        for acc, alignment in cluster_alignment.items():\n",
    "            if acc != 'consensus':\n",
    "                new_alignment = apply_matchlist(alignment, matchlist)\n",
    "\n",
    "                #consensus sequences are replaced by their realigned versions of the same name\n",
    "                singles_consensus_alignment_dict[acc] = new_alignment\n",
    "    \n",
    "    #delete columns with entropy lower than the filter level\n",
    "    if filter_entropy != None:\n",
    "        print(f'{threadID_string} Filtering alignment by bitscore > {filter_entropy}')\n",
    "        singles_consensus_alignment_dict = filter_by_entropy(singles_consensus_alignment_dict, filter_entropy)\n",
    "        \n",
    "    #print(f'{threadID_string} finished initial profile creation, wrote {seq_fasta}.haln.clu file')\n",
    "    with open(seq_fasta+'.haln', 'w') as out:\n",
    "        out.write(dict_to_fasta(singles_consensus_alignment_dict))\n",
    "        \n",
    "    #realign all sequences to merged alignment profile\n",
    "    #print(f'{threadID_string} Serially realigning all sequences against profile')    \n",
    "    \n",
    "    #using mafft --add --keeplength to seriall trealign all sequences individually to combined consensus \n",
    "    #aligner_command = f'mafft --anysymbol --thread {aligner_threads} --keeplength --quiet --add {seq_fasta} {seq_fasta}.haln > {seq_fasta}.haln.mafft-add'\n",
    "    #os.system(aligner_command)\n",
    "    #with open(f'{seq_fasta}.haln.mafft-add', 'r') as infile:\n",
    "    #    aligner_stdout = ''.join(infile.readlines())\n",
    "    #returned file contains duplicate sequences from profile, take last half\n",
    "    #aligner_stdout = '>'.join(aligner_stdout.split('>')[len(singles_consensus_alignment_dict)-1:])   \n",
    "    \n",
    "    #singles_consensus_alignment_dict = fasta_to_dict(aligner_stdout)\n",
    "    \n",
    "    #delete columns with entropy lower than the filter level\n",
    "    #if filter_entropy != None:\n",
    "    #    print(f'{threadID_string} Filtering final alignment by bitscore > {filter_entropy}')\n",
    "    #    singles_consensus_alignment_dict = filter_by_entropy(singles_consensus_alignment_dict, filter_entropy)\n",
    "\n",
    "        \n",
    "    #write final output alignment\n",
    "    #print(f'{threadID_string} Finished final profile creation, wrote {seq_fasta}.haln file')\n",
    "    #with open(seq_fasta+'.haln', 'w') as out:\n",
    "    #    out.write(dict_to_fasta(singles_consensus_alignment_dict))\n",
    "        \n",
    "    \n",
    "    print(f'{threadID_string} DONE!')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb77cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrapper function for target reclustering which required a temporary accession file to be written\n",
    "def mmseqs_run_target(target, members, root, seq_DB, threads=1):\n",
    "    \n",
    "    #boilerplate for informative print()\n",
    "    thread = multiprocessing.current_process().pid\n",
    "    threadID_string = f'{thread} | {target}:'\n",
    "    print(f'{threadID_string} Preparing mmseqs data for target {target}\\n', end='')\n",
    "    \n",
    "    basename = f'{root+target}'\n",
    "    \n",
    "    #write all accessstions to temporary file\n",
    "    with open(basename, 'w') as outfile:\n",
    "        outfile.writelines([acc+'\\n' for acc in members])\n",
    "    \n",
    "    #create a temporary seqDB, extract fastas, cluster and calculate cluster.tsv file\n",
    "    os.system(f'mmseqs createsubdb -v 0 --id-mode 1 {basename} {seq_DB} {basename}.DB')\n",
    "    os.system(f'mmseqs convert2fasta -v 0 {basename}.DB {basename}.fasta')\n",
    "    os.system(f'mmseqs cluster -v 0 --remove-tmp-files 1 --threads {threads} -s 7.5 {basename}.DB {basename}.cluster {root}tmp/{target}')\n",
    "    os.system(f'mmseqs createtsv -v 0 {seq_DB} {seq_DB} {basename}.cluster {basename}.cluster.tsv')\n",
    "    \n",
    "    #clean \n",
    "    os.system(f'mmseqs rmdb {basename}.DB -v 0')\n",
    "    os.system(f'mmseqs rmdb {basename}.cluster -v 0')\n",
    "    os.system(f'rm {basename}')\n",
    "    \n",
    "    return\n",
    "\n",
    "#main script for formatting query and target .fasta and .cluster.tsv files\n",
    "def microcosm_prepare_mmseqs(query, query_DB, target_DB, root):\n",
    "    \n",
    "    #configure paths and flags\n",
    "    thread = multiprocessing.current_process().pid\n",
    "    query_root = root+query+'/'\n",
    "    basename = query_root+query\n",
    "    threadID_string = f'{thread} | {query}:'\n",
    "    os.system(f'mkdir {query_root}/tmp')\n",
    "    \n",
    "    #parse target clusters\n",
    "    target_clusters = read_cluster_tsv(f'{basename}.targets')  \n",
    "\n",
    "    print(f'{threadID_string} Started \\n', end='')\n",
    "    print(f'{threadID_string} Preparing mmseqs data for query\\n', end='')\n",
    "\n",
    "    #createa new DB for the query sequences and cluster it\n",
    "    os.system(f\"mmseqs createsubdb -v 0 --id-mode 1 --subdb-mode 1 {basename}.acc {query_DB} {basename}.DB\")\n",
    "    os.system(f'mmseqs convert2fasta -v 0 {basename}.DB {basename}.fasta')\n",
    "    os.system(f'mmseqs cluster -v 0 --remove-tmp-files 1 --threads 1 -s 7.5 {basename}.DB {basename}.cluster {query_root}/tmp/{query}')\n",
    "    os.system(f'mmseqs createtsv -v 0 {query_DB} {query_DB} {basename}.cluster {basename}.cluster.tsv')\n",
    "    \n",
    "    print(f'{threadID_string} Preparing mmseqs data for merged target hits\\n', end='')\n",
    "\n",
    "    #create a subDB and extract sequences, create a new seqDB in order to recreate the header lookup\n",
    "    #otherwise each query using header info searches the entire original header lookup\n",
    "    with open(f'{basename}.members', 'w') as outfile:\n",
    "        for members in target_clusters.values():\n",
    "            outfile.writelines([member+'\\n' for member in members])\n",
    "    \n",
    "    os.system(f\"mmseqs createsubdb -v 0 --id-mode 1 --subdb-mode 1 {basename}.members {target_DB} {basename}.members.DB\")\n",
    "    os.system(f\"mmseqs convert2fasta -v 0 {basename}.members.DB {basename}.members.fasta\")\n",
    "    os.system(f\"mmseqs createdb -v 0 --createdb-mode 1 {basename}.members.fasta {basename}.members.DB\")\n",
    "    os.system(f'mmseqs cluster -v 0 --remove-tmp-files 1 --threads 1 -s 7.5 {basename}.members.DB {basename}.members.cluster {query_root}/tmp/{query}')\n",
    "    os.system(f'mmseqs createtsv -v 0 {basename}.members.DB {basename}.members.DB {basename}.members.cluster {basename}.members.cluster.tsv')\n",
    "\n",
    "    \n",
    "    #clean\n",
    "    os.system(f'mmseqs rmdb -v 0 {basename}.DB')\n",
    "    os.system(f'mmseqs rmdb -v 0 {basename}.cluster')\n",
    "    \n",
    "    #temporary block for testing\n",
    "    return\n",
    "\n",
    "    #multithreads the individual DB creation and clustering\n",
    "    with multiprocessing.Pool(processes=8) as pool:\n",
    "        \n",
    "        targets = target_clusters.keys()\n",
    "        members = target_clusters.values()\n",
    "        root = query_root\n",
    "        seq_DB = f'{basename}.members.DB'\n",
    "        submit_vars = zip(list(targets), list(members), [root]*len(targets), [seq_DB]*len(targets))\n",
    "        \n",
    "        mmseqs_target_runs = pool.starmap(mmseqs_run_target, submit_vars)\n",
    "    \n",
    "    #final clean\n",
    "    os.system(f'mmseqs rmdb -v 0 {basename}.members.DB')\n",
    "    os.system(f'find {query_root} -maxdepth 1 -type l -exec unlink {{}} \\;')\n",
    "    os.system(f'rm -r *members* {query_root}/tmp')\n",
    "    \n",
    "    #perform hierachical alignment of query and all clusters > 10 \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ba462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrapper function to dictate alignment type based on cluster members\n",
    "def choose_alignment_type(seq_fasta, cluster_file, filter_entropy=False, parallel_n=None):\n",
    "    \n",
    "    hierarchical_threshold = 10\n",
    "    \n",
    "    thread = multiprocessing.current_process().pid\n",
    "    threadID_string = f'{thread} | {seq_fasta.split(\"/\")[-1]} alignment:'\n",
    "\n",
    "    with open(seq_fasta, 'r') as infile:\n",
    "        seq_dict = fasta_to_dict(''.join(infile.readlines()))\n",
    "\n",
    "    seq_members = len(seq_dict.keys())\n",
    "\n",
    "    if seq_members > hierarchical_threshold:\n",
    "        print(f'{threadID_string} Aligning heirarchically with {seq_members} sequences\\n', end='')\n",
    "        hierarchical_realignment(seq_fasta, cluster_file, filter_entropy, parallel_n)\n",
    "\n",
    "    else:\n",
    "        print(f'{threadID_string} Aligning naively with {seq_members} sequences\\n', end='')\n",
    "        os.system(f'clustalo --in {basename}.fasta --out {basename}.fasta.haln --iter 2')\n",
    "        \n",
    "#main realignment script for taking a directory with cluster and target .fastas and .clsuter.tsv files\n",
    "#realigns all files either heirarchically or naively using clustal depending on cluster member number\n",
    "def microcosm_perform_haln(query, entropy_filter, parallel_n, root):\n",
    "    \n",
    "    #configure paths and flags\n",
    "    thread = multiprocessing.current_process().pid\n",
    "    threadID_string = f'{thread} | {query}:'\n",
    "    \n",
    "    query_root = root+query+'/'\n",
    "    basename = query_root+query\n",
    "    \n",
    "    os.system(f'mkdir {query_root}/tmp')\n",
    "    \n",
    "    #parse target clusters\n",
    "    target_clusters = read_cluster_tsv(f'{basename}.targets')  \n",
    "    \n",
    "    \n",
    "    all_clusters = [query]+list(target_clusters.keys())\n",
    "\n",
    "    with multiprocessing.Pool(processes=8) as pool:\n",
    "        \n",
    "        targets = [query_root+cluster+'.fasta' for cluster in all_clusters]\n",
    "        target_clusters = [query_root+cluster+'.cluster.tsv' for cluster in all_clusters]\n",
    "        entropy_filter = 0\n",
    "        parallel_n = None\n",
    "        submit_vars = zip(targets, target_clusters, [entropy_filter]*len(targets), [parallel_n]*len(targets))\n",
    "\n",
    "        mmseqs_target_runs = pool.starmap(choose_alignment_type, submit_vars)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcd2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pool_cluster_alignment(run_seq_dict, filter_entropy=None, write_cluster_align=False, cluster_align_root=''):\n",
    "    \n",
    "    thread = multiprocessing.current_process().pid\n",
    "    ref_seq = list(run_seq_dict.keys())[0]\n",
    "    \n",
    "    threadID_string = f'{thread} | {ref_seq} alignment:'\n",
    "    \n",
    "    print(f'{threadID_string} ref_seq = {ref_seq} started with {len(run_seq_dict)} sequences\\n', end='')\n",
    "    \n",
    "    #filter seqs from \n",
    "    seq_dict = align_with_consensus(run_seq_dict, return_consensus=False)\n",
    "    \n",
    "    if filter_entropy != None:\n",
    "        print(f'{threadID_string} Filtering alignment by bitscore > {filter_entropy}')\n",
    "        seq_dict = filter_by_entropy(seq_dict, filter_entropy)\n",
    "        \n",
    "    if write_cluster_align:\n",
    "        print(f'{threadID_string} Writing alignment to file {ref_seq}.cluster.clu')\n",
    "        with open(f'{cluster_align_root}{ref_seq}.cluster.clu', 'w') as out:\n",
    "            out.write(dict_to_fasta(seq_dict))\n",
    "    \n",
    "    return {ref_seq: seq_dict}\n",
    "\n",
    "\n",
    "#for each cluster in a .tsv realign alll clusters individually by clustalo, filter by bitscore, write to file \n",
    "def cluster_realignment(file_root, seq_fasta, cluster_file, filter_entropy=False, parallel_n=None):\n",
    "    \n",
    "    thread = multiprocessing.current_process().pid\n",
    "    threadID_string = f'{thread} | {seq_fasta.split(\"/\")[-1]} alignment:'\n",
    "    \n",
    "    print(f'{threadID_string} started')\n",
    "\n",
    "    aligner_threads = 1\n",
    "    if parallel_n != None:\n",
    "        aligner_threads = parallel_n\n",
    "        \n",
    "    with open(seq_fasta, 'r') as infile:\n",
    "        seq_dict = fasta_to_dict(''.join(infile.readlines()))\n",
    "\n",
    "    #split larger clusters to avoid large realignments\n",
    "    max_size = 200\n",
    "    singleton_threshold = 10\n",
    "\n",
    "    clusters = read_cluster_tsv(cluster_file, split_large=True, max_size=max_size,\n",
    "                                batch_single=True, single_cutoff=singleton_threshold)\n",
    "\n",
    "    #in some edge cases partitioning and merginf clusters froms a single sequence non-singleton cluster\n",
    "    #add all clusters larger than one to realignment list\n",
    "    seq_dicts = [{key:seq_dict[key] for key in seqs} for seqs in clusters.values() if len(seqs) > 1]\n",
    "\n",
    "    \n",
    "    #add those lonley edge sequences to a separate dict for merger later\n",
    "    singles_dict = {seq[0]:seq_dict[seq[0]] for seq in clusters.values() if len(seq) == 1}\n",
    "    \n",
    "    print(f'{threadID_string} realigning a total of {len(seq_dicts)} clusters')\n",
    "    \n",
    "    if parallel_n != None:\n",
    "        #multiprocessing\n",
    "        print(f'{threadID_string} aligning parallel with {parallel_n} threads')\n",
    "        with multiprocessing.Pool(processes=parallel_n) as pool:\n",
    "            \n",
    "            #do not return consensus sequences for any subalignment \n",
    "            filter_e = [filter_entropy for _ in seq_dicts]\n",
    "            write_cluster_align = [True for _ in seq_dicts]\n",
    "            cluster_align_root = [file_root for root in seq_dicts]\n",
    "            submit_vars = zip(seq_dicts, filter_e, write_cluster_align, cluster_align_root)\n",
    "            \n",
    "            cluster_alignments_stream = pool.starmap(pool_cluster_alignment, submit_vars)\n",
    "    \n",
    "    else:\n",
    "        #serial implementation\n",
    "        cluster_alignments_stream = []\n",
    "        for seq_dict in seq_dicts:\n",
    "            cluster_alignments_stream.append(pool_align_with_consensus(seq_dict))\n",
    "    \n",
    "    print(f'{threadID_string} merging alignments and writing filtered files')\n",
    "    \n",
    "    #merge and order results from stream \n",
    "    cluster_alignments = {key: value for result in cluster_alignments_stream for key, value in result.items()}\n",
    "    \n",
    "    return cluster_alignments\n",
    "    \n",
    "    #delete columns with entropy lower than the filter level\n",
    "    if filter_entropy != None:\n",
    "\n",
    "        for key, alignment in cluster_alignments.items(): \n",
    "            cluster_alignments[key] = filter_by_entropy(alignment, filter_entropy)\n",
    "    \n",
    "    #write individual files for all realignments\n",
    "    for key, alignment in cluster_alignments.items():\n",
    "        with open(f'{seq_fasta}.{key}.clu', 'w') as out:\n",
    "            out.write(dict_to_fasta(alignment))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ccef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75cf52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare one mmseqs\n",
    "\n",
    "microcosm_prepare_mmseqs('OAE21175.1', 'euk72/euk72', 'prok2111/prok2111', 'microcosm2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea03bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare all mmseqs\n",
    "\n",
    "with multiprocessing.Pool(processes=4) as pool:\n",
    "    #format starmap input\n",
    "    queries = euk_queries_test5\n",
    "    query_DB = 'euk72/euk72'\n",
    "    hit_DB = 'prok2111/prok2111'\n",
    "    root = 'microcosm4/'\n",
    "    submit_vars = zip(queries, [query_DB]*len(queries), [hit_DB]*len(queries), [root]*len(queries))\n",
    "\n",
    "    cluster_alignment_stream = pool.starmap(microcosm_prepare_mmseqs, submit_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10103f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one cluster alignment\n",
    "\n",
    "seq_dicts = cluster_realignment(f'microcosm2/OAE21175.1/', \n",
    "                                         'microcosm2/OAE21175.1/OAE21175.1.members.fasta',\n",
    "                                         'microcosm2/OAE21175.1/OAE21175.1.members.cluster.tsv',\n",
    "                                         filter_entropy=0, parallel_n=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30d8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#all cluster alignment\n",
    "root = 'microcosm4/'\n",
    "\n",
    "for query in euk_queries_test5:\n",
    "    file_root = f'{root}{query}/{query}'\n",
    "    cluster_file = file_root + '.members.cluster.tsv'\n",
    "    seq_fasta = file_root + '.members.fasta' \n",
    "    cluster_alignments = cluster_realignment(f'{root}{query}/', seq_fasta, cluster_file, filter_entropy=0, parallel_n=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a64c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one haln alignment\n",
    "seq_dicts = hierarchical_realignment('microcosm3/NP_001002332.1/NP_001002332.1.members.fasta',\n",
    "                                     'microcosm3/NP_001002332.1/NP_001002332.1.members.cluster.tsv',\n",
    "                                     filter_entropy=0, parallel_n=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#all haln alignment\n",
    "root = 'microcosm4/'\n",
    "\n",
    "for query in euk_queries_test5[38:]:\n",
    "    file_root = f'{root}{query}/{query}'\n",
    "    seq_fasta = file_root + '.members.fasta' \n",
    "    cluster_file = file_root + '.members.cluster.tsv'\n",
    "    cluster_alignments = hierarchical_realignment(seq_fasta, cluster_file, filter_entropy=0, parallel_n=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22169ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "euk_queries_test5[38:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea5af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter by columnwise bitscore\n",
    "\n",
    "#files = !find microcosm4/* -name '*'\n",
    "files = ['microcosm4/AGK83073.1/test/AGK83073.1.members.fasta.famsa3']\n",
    "\n",
    "for file in files:\n",
    "    #euk_acc_file = file.split('.cluster.clu.euk')[0]+'.acc'\n",
    "    with open(file, 'r') as infile:#, open(euk_acc_file,'r') as accfile:\n",
    "        #euk_accs = accfile.read().split()\n",
    "        aln = fasta_to_dict(infile.readlines())\n",
    "\n",
    "    aln_filter = filter_by_entropy(aln, 0.5, gaptoken='-')#,filter_accs=euk_accs)\n",
    "    dict_to_fasta(aln_filter, write_file=file+'.fil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b6ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = hierarchical_realignment('microcosm3/XP_005256905.1/XP_005256905.1.members.fasta',\n",
    "                                     'microcosm3/XP_005256905.1/XP_005256905.1.members.cluster.tsv',\n",
    "                                     filter_entropy=0, parallel_n=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54febba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'microcosm4/AGK83073.1/AGK83073.1.members.fasta.haln.euk'\n",
    "\n",
    "with open(file, 'r') as infile:\n",
    "    aln = fasta_to_dict(infile.readlines())\n",
    "\n",
    "euk_accs = open('microcosm4/AGK83073.1/AGK83073.1.acc', 'r').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fbd680",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = !find microcosm/*/merged.fasta.muscle\n",
    "for file in files:\n",
    "    with open(file, 'r') as infile:\n",
    "        aln = fasta_to_dict(infile.readlines())\n",
    "        \n",
    "    aln_filter = filter_by_entropy(aln, 0.5)\n",
    "    dict_to_fasta(aln_filter, write_file=file+'.fil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b0adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_file = 'microcosm3/XP_005256905.1/XP_005256905.1.members.cluster.tsv'\n",
    "#split larger clusters to avoid large realignments\n",
    "max_size = 200\n",
    "singleton_threshold = 10\n",
    "\n",
    "clusters = read_cluster_tsv(cluster_file, \n",
    "                            split_large=False, max_size=max_size, \n",
    "                            batch_single=False, single_cutoff=singleton_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b5be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = !find ./microcosm2/ -name '*members.fasta'\n",
    "roots = ['.'.join(f.split('.')[:-2]) for f in files]\n",
    "for root in roots:\n",
    "    seq_file = root+'.members.fasta'\n",
    "    cluster_tsv = root+'.members.cluster.tsv'\n",
    "    seq_dicts = hierarchical_realignment(seq_file, cluster_tsv, filter_entropy=0, parallel_n=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3145650",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31466ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core_functions.altair_plots import plot_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7145e704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91870724",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python/3.9",
   "language": "python",
   "name": "py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
