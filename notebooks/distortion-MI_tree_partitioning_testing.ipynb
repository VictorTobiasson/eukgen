{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9088e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerics and vis\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import altair as alt\n",
    "import altair_saver\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "import ete3\n",
    "from ete3 import Tree, TreeStyle, TextFace\n",
    "\n",
    "\n",
    "root = \"/data/tobiassonva/data/eukgen/\"\n",
    "sys.path.insert(0, root)\n",
    "%cd {root}\n",
    "\n",
    "from core_functions.microcosm_functions import color_tree, tree_analysis\n",
    "from core_functions.altair_plots import plot_alignment, plot_cumsum_counts\n",
    "from core_functions.tree_functions import get_outlier_nodes_by_lognorm\n",
    "\n",
    "#disable altair max rows\n",
    "alt.data_transformers.disable_max_rows()\n",
    "#get default altair style'\n",
    "%run ~/scripts/altair_style_config_default.py\n",
    "\n",
    "#dont wrap text output from cells\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_area pre {white-space: pre;}</style>\"))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load phylogeny information\n",
    "\n",
    "import pickle \n",
    "#small helper for pkl parsing\n",
    "def load_pkl(pkl_file):\n",
    "    import parseHHsuite as HH\n",
    "    with open(pkl_file, 'rb') as infile:\n",
    "        item = pickle.load(infile)\n",
    "    return item\n",
    "\n",
    "#lighter parsed version\n",
    "prok_tax = load_pkl('analysis/core_data/prok2111_protein_taxonomy_trimmed.pkl')\n",
    "print('Loading taxonomy info')\n",
    "euk_tax = load_pkl('euk72/euk72_protein_taxonomy.pkl')\n",
    "euk_tax.drop(['orgid', 'species'], axis=1, inplace=True)\n",
    "tax_merge = pd.concat([euk_tax, prok_tax])\n",
    "\n",
    "euk_header = load_pkl('analysis/core_data/euk72_header_mapping.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc475558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tools and functions from from TreeClust.py \n",
    "#https://github.com/niemasd/TreeCluster\n",
    "\n",
    "from niemads import DisjointSet\n",
    "from queue import PriorityQueue,Queue\n",
    "from treeswift import read_tree_newick\n",
    "\n",
    "# initialize properties of input tree and return set containing taxa of leaves\n",
    "def prep(tree, support, resolve_polytomies=True, suppress_unifurcations=True):\n",
    "    if resolve_polytomies:\n",
    "        tree.resolve_polytomies()\n",
    "    if suppress_unifurcations:\n",
    "        tree.suppress_unifurcations()\n",
    "    leaves = set()\n",
    "    for node in tree.traverse_postorder():\n",
    "        if node.edge_length is None:\n",
    "            node.edge_length = 0\n",
    "        node.DELETED = False\n",
    "        if node.is_leaf():\n",
    "            leaves.add(str(node))\n",
    "        else:\n",
    "            try:\n",
    "                node.confidence = float(str(node))\n",
    "            except:\n",
    "                node.confidence = 100. # give edges without support values support 100\n",
    "            if node.confidence < support: # don't allow low-support edges\n",
    "                node.edge_length = float('inf')\n",
    "    return leaves\n",
    "\n",
    "# cut out the current node's subtree (by setting all nodes' DELETED to True) and return list of leaves\n",
    "def cut(node):\n",
    "    cluster = list()\n",
    "    descendants = Queue(); descendants.put(node)\n",
    "    while not descendants.empty():\n",
    "        descendant = descendants.get()\n",
    "        if descendant.DELETED:\n",
    "            continue\n",
    "        descendant.DELETED = True\n",
    "        descendant.left_dist = 0; descendant.right_dist = 0; descendant.edge_length = 0\n",
    "        if descendant.is_leaf():\n",
    "            cluster.append(str(descendant))\n",
    "        else:\n",
    "            for c in descendant.children:\n",
    "                descendants.put(c)\n",
    "    return cluster\n",
    "\n",
    "\n",
    "\n",
    "# split leaves into minimum number of clusters such that the maximum leaf pairwise distance is below some threshold\n",
    "def min_clusters_threshold_max(tree,threshold,support):\n",
    "    leaves = prep(tree,support)\n",
    "    clusters = list()\n",
    "    for node in tree.traverse_postorder():\n",
    "        # if I've already been handled, ignore me\n",
    "        if node.DELETED:\n",
    "            continue\n",
    "\n",
    "        # find my undeleted max distances to leaf\n",
    "        if node.is_leaf():\n",
    "            node.left_dist = 0; node.right_dist = 0\n",
    "        else:\n",
    "            children = list(node.children)\n",
    "            if children[0].DELETED and children[1].DELETED:\n",
    "                cut(node); continue\n",
    "            if children[0].DELETED:\n",
    "                node.left_dist = 0\n",
    "            else:\n",
    "                node.left_dist = max(children[0].left_dist,children[0].right_dist) + children[0].edge_length\n",
    "            if children[1].DELETED:\n",
    "                node.right_dist = 0\n",
    "            else:\n",
    "                node.right_dist = max(children[1].left_dist,children[1].right_dist) + children[1].edge_length\n",
    "\n",
    "            # if my kids are screwing things up, cut out the longer one\n",
    "            if node.left_dist + node.right_dist > threshold:\n",
    "                if node.left_dist > node.right_dist:\n",
    "                    cluster = cut(children[0])\n",
    "                    node.left_dist = 0\n",
    "                else:\n",
    "                    cluster = cut(children[1])\n",
    "                    node.right_dist = 0\n",
    "\n",
    "                # add cluster\n",
    "                if len(cluster) != 0:\n",
    "                    clusters.append(cluster)\n",
    "                    for leaf in cluster:\n",
    "                        leaves.remove(leaf)\n",
    "\n",
    "    # add all remaining leaves to a single cluster\n",
    "    if len(leaves) != 0:\n",
    "        clusters.append(list(leaves))\n",
    "    return clusters\n",
    "\n",
    "# min_clusters_threshold_max, but all clusters must define a clade\n",
    "def min_clusters_threshold_max_clade(tree,threshold,support):\n",
    "    leaves = prep(tree, support, resolve_polytomies=False)\n",
    "\n",
    "    # compute leaf distances and max pairwise distances\n",
    "    for node in tree.traverse_postorder():\n",
    "        if node.is_leaf():\n",
    "            node.leaf_dist = 0; node.max_pair_dist = 0\n",
    "        else:\n",
    "            node.leaf_dist = float('-inf'); second_max_leaf_dist = float('-inf')\n",
    "            for c in node.children: # at least 2 children because of suppressing unifurcations\n",
    "                curr_dist = c.leaf_dist + c.edge_length\n",
    "                if curr_dist > node.leaf_dist:\n",
    "                    second_max_leaf_dist = node.leaf_dist; node.leaf_dist = curr_dist\n",
    "                elif curr_dist > second_max_leaf_dist:\n",
    "                    second_max_leaf_dist = curr_dist\n",
    "            node.max_pair_dist = max([c.max_pair_dist for c in node.children] + [node.leaf_dist + second_max_leaf_dist])\n",
    "\n",
    "    # perform clustering\n",
    "    q = Queue(); q.put(tree.root); roots = list()\n",
    "    while not q.empty():\n",
    "        node = q.get()\n",
    "        if node.max_pair_dist <= threshold:\n",
    "            roots.append(node)\n",
    "        else:\n",
    "            for c in node.children:\n",
    "                q.put(c)\n",
    "\n",
    "    return [[str(l) for l in root.traverse_leaves()] for root in roots]\n",
    "\n",
    "\n",
    "# average leaf pairwise distance cannot exceed threshold, and clusters must define clades\n",
    "def min_clusters_threshold_avg_clade(tree,threshold,support):\n",
    "    leaves = prep(tree,support)\n",
    "\n",
    "    # bottom-up traversal to compute average pairwise distances\n",
    "    for node in tree.traverse_postorder():\n",
    "        node.total_pair_dist = 0; node.total_leaf_dist = 0\n",
    "        if node.is_leaf():\n",
    "            node.num_leaves = 1\n",
    "            node.avg_pair_dist = 0\n",
    "        else:\n",
    "            children = list(node.children)\n",
    "            node.num_leaves = sum(c.num_leaves for c in children)\n",
    "            node.total_pair_dist = children[0].total_pair_dist + children[1].total_pair_dist + (children[0].total_leaf_dist*children[1].num_leaves + children[1].total_leaf_dist*children[0].num_leaves)\n",
    "            node.total_leaf_dist = (children[0].total_leaf_dist + children[0].edge_length*children[0].num_leaves) + (children[1].total_leaf_dist + children[1].edge_length*children[1].num_leaves)\n",
    "            node.avg_pair_dist = node.total_pair_dist/((node.num_leaves*(node.num_leaves-1))/2)\n",
    "\n",
    "    # perform clustering\n",
    "    q = Queue(); q.put(tree.root); roots = list()\n",
    "    while not q.empty():\n",
    "        node = q.get()\n",
    "        if node.avg_pair_dist <= threshold:\n",
    "            roots.append(node)\n",
    "        else:\n",
    "            for c in node.children:\n",
    "                q.put(c)\n",
    "\n",
    "    return [[str(l) for l in root.traverse_leaves()] for root in roots]\n",
    "\n",
    "# cut tree at threshold distance from root (clusters will be clades by definition) (ignores support threshold if branch is below cutting point)\n",
    "def root_dist(tree,threshold,support):\n",
    "    leaves = prep(tree,support)\n",
    "    clusters = list()\n",
    "    for node in tree.traverse_preorder():\n",
    "        # if I've already been handled, ignore me\n",
    "        if node.DELETED:\n",
    "            continue\n",
    "        if node.is_root():\n",
    "            node.root_dist = 0\n",
    "        else:\n",
    "            node.root_dist = node.parent.root_dist + node.edge_length\n",
    "        if node.root_dist > threshold:\n",
    "            cluster = cut(node)\n",
    "            if len(cluster) != 0:\n",
    "                clusters.append(cluster)\n",
    "                for leaf in cluster:\n",
    "                    leaves.remove(leaf)\n",
    "\n",
    "    # add all remaining leaves to a single cluster\n",
    "    if len(leaves) != 0:\n",
    "        clusters.append(list(leaves))\n",
    "    return clusters\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f6484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input helper to read tsv from file, merge singletons\n",
    "def read_cluster_tsv(cluster_file, split_large=False, max_size=500, batch_single=False, single_cutoff=1):   \n",
    "\n",
    "    #read TSV and group clusters based on first tsv column\n",
    "    with open(cluster_file, 'r') as infile:\n",
    "        clusters = {}\n",
    "\n",
    "        for l in infile.readlines():\n",
    "            cluster_acc, acc = l.strip().split('\\t')\n",
    "\n",
    "            if cluster_acc not in clusters.keys():\n",
    "                clusters[cluster_acc] = [acc]\n",
    "\n",
    "            else:\n",
    "                clusters[cluster_acc].append(acc)\n",
    "    \n",
    "    #merge all clusters smaller than cutoff into one\n",
    "    if batch_single:\n",
    "        filter_dict = {}\n",
    "        singles = []\n",
    "        for key, accs in clusters.items():  \n",
    "            #gather singletons\n",
    "            if len(accs) <= single_cutoff:\n",
    "                singles.extend(accs)\n",
    "            \n",
    "            #keep larger clusters\n",
    "            else:\n",
    "                filter_dict[key] = accs\n",
    "                \n",
    "        if singles:\n",
    "            filter_dict[singles[0]] = singles\n",
    "        \n",
    "        clusters = filter_dict\n",
    "    \n",
    "    #split clusters larger than x into smaller pieces\n",
    "    if split_large:\n",
    "        filter_dict = {}\n",
    "        for key, accs in clusters.items():  \n",
    "            if len(accs) > max_size:\n",
    "                #partition large clusters into batches of max_size\n",
    "                for split in range(0, len(accs), max_size):\n",
    "                    batch = accs[split:split + max_size]\n",
    "                    filter_dict[batch[0]] = batch\n",
    "\n",
    "            #keep smaller clusters\n",
    "            else:\n",
    "                filter_dict[key] = accs\n",
    "                \n",
    "        clusters = filter_dict         \n",
    "        \n",
    "    return clusters\n",
    "\n",
    "\n",
    "#quick function for adding phylogenetic annotation to tree labels\n",
    "def dirty_phyla_add(tree, tax_mapping):\n",
    "    euk_entries = []\n",
    "    for leaf in tree.get_leaves():\n",
    "        try:\n",
    "            acc = leaf.name\n",
    "            entry = tax_mapping.loc[acc]\n",
    "            leaf.add_feature('tax_superkingdom', entry['superkingdom'])\n",
    "            \n",
    "            leaf.add_feature('tax_class', entry['class'])\n",
    "            \n",
    "            if leaf.tax_superkingdom == 'Eukaryota':\n",
    "                leaf.add_feature('tax_filter', 'Eukaryota')\n",
    "            else:\n",
    "                leaf.add_feature('tax_filter', entry['class'])\n",
    "            \n",
    "        except KeyError:\n",
    "            leaf.add_feature('superkingom', 'ERROR')\n",
    "            leaf.add_feature('class', 'ERROR')\n",
    "\n",
    "#write pdf tree under /data/tobiassonva/data/eukgen/tmp_trees\n",
    "#overwrites previous treee in directory unless specified name\n",
    "#the file path is always relative to the notbook starting directory due to the \"Tornado\" viewer restrictions\n",
    "def view_tree(tree, ts=None, name='test_tree.pdf', backup=None):\n",
    "    from IPython.display import IFrame\n",
    "    tree_img_root = '/data/tobiassonva/data/eukgen/analysis/tmp_trees/'\n",
    "    tree.render(tree_img_root+name, tree_style=ts)\n",
    "    \n",
    "    if backup != None:\n",
    "        import os\n",
    "        os.system(f'cp {tree_img_root+name} {backup}')\n",
    "    return IFrame('tmp_trees/'+name, width=950, height=950*1.5)\n",
    "\n",
    "\n",
    "#return LCA for list of leaf names\n",
    "def get_LCA(tree, leaf_names):\n",
    "    leaves = [tree.get_leaves_by_name(name)[0] for name in leaf_names]\n",
    "    LCA = leaves[0].get_common_ancestor(leaves)\n",
    "    return LCA\n",
    "\n",
    "def cluster_TreeClust(tree, threshold):\n",
    "    fasttree = read_tree_newick(tree.write())\n",
    "    tree_clusters = min_clusters_threshold_max_clade(fasttree, threshold, -999999999)\n",
    "    #tree_clusters = min_clusters_threshold_avg_clade(fasttree, threshold, -999999999)    \n",
    "    return  tree_clusters\n",
    "\n",
    "#extract all against all dsitance matrix for ete3 trees\n",
    "def calculate_pairwise_distances(tree):\n",
    "    leaves = tree.get_leaves()\n",
    "    pairwise_mat = np.zeros((len(leaves),len(leaves)))\n",
    "    for i, m in enumerate(leaves):\n",
    "        for j, k in enumerate(leaves[i+1:]):\n",
    "            pairwise_mat[i,j] = tree.get_distance(m, k)\n",
    "    return pairwise_mat\n",
    "\n",
    "\n",
    "#return closest non self leaf\n",
    "def get_closest_leaf(leaf):\n",
    "    \n",
    "    near_leaves = [near_leaf for near_leaf in leaf.up.get_leaves() if near_leaf!=leaf]\n",
    "    distances = [leaf.get_distance(near_leaf) for near_leaf in near_leaves]\n",
    "    min_dist =min(distances)\n",
    "    closest_leaf = near_leaves[distances.index(min_dist)]\n",
    "    \n",
    "    return closest_leaf, min_dist\n",
    "    \n",
    "#iteratively merge closest leaf pair until less than N leaves\n",
    "def reduce_leaves_to_size(tree, max_size):\n",
    "    current_size = len(tree.get_leaves())\n",
    "    \n",
    "    if max_size >= current_size:\n",
    "        print(f'Tree of length {current_size} smaller than {max_size}')\n",
    "        return tree    \n",
    "    \n",
    "    leaf_partners = {leaf.name:get_closest_leaf(leaf) for leaf in tree.get_leaves()}\n",
    "    leaf_partner_dist = {key:value[1] for key, value in leaf_partners.items()}\n",
    "    leaf_partners = {key:value[0].name for key, value in leaf_partners.items()}\n",
    "    \n",
    "    #print(leaf_partner_dist)\n",
    "    #print(leaf_partners)\n",
    "    \n",
    "    \n",
    "    #serial implementetaion, not ideal as it produced uneven pruning if terminal brach length are very even.\n",
    "    while max_size < current_size:\n",
    "        \n",
    "        min_distance = min(leaf_partner_dist.values())\n",
    "        min_leaf_A = list(leaf_partner_dist.keys())[list(leaf_partner_dist.values()).index(min_distance)]\n",
    "        \n",
    "        min_leaf_B = leaf_partners[min_leaf_A]\n",
    "        \n",
    "        #print(f'{current_size} checking {min_leaf_A}, deleting closest partner is {min_leaf_B} with distance {min_distance}')\n",
    "        \n",
    "        #delete closest leaf\n",
    "        try:\n",
    "            tree.get_leaves_by_name(min_leaf_B)[0].delete()\n",
    "        \n",
    "        #if current leaf A maps to a deleted leaf update closest leaf and delete\n",
    "        except IndexError:\n",
    "            #update the min_leaf with new closest pair\n",
    "            new_leaf = tree.get_leaves_by_name(min_leaf_A)[0]\n",
    "            closest_new_leaf = get_closest_leaf(new_leaf)\n",
    "\n",
    "            leaf_partner_dist[new_leaf.name] = closest_new_leaf[1]\n",
    "            leaf_partners[new_leaf.name] = closest_new_leaf[0].name\n",
    "            min_leaf_B = leaf_partners[min_leaf_A]\n",
    "            \n",
    "            tree.get_leaves_by_name(min_leaf_B)[0].delete()\n",
    "            \n",
    "        \n",
    "        #delete the removed partner from dictionaries\n",
    "        leaf_partner_dist.pop(min_leaf_B, None)\n",
    "        leaf_partners.pop(min_leaf_B, None)\n",
    "        \n",
    "        #update the min_leaf with new closest pair\n",
    "        new_leaf = tree.get_leaves_by_name(min_leaf_A)[0]\n",
    "        closest_new_leaf = get_closest_leaf(new_leaf)\n",
    "        \n",
    "        leaf_partner_dist[new_leaf.name] = closest_new_leaf[1]\n",
    "        leaf_partners[new_leaf.name] = closest_new_leaf[0].name\n",
    "        \n",
    "        current_size -= 1\n",
    "        \n",
    "        #print(leaf_partner_dist)\n",
    "        #print(leaf_partners)\n",
    "\n",
    "    \n",
    "    return tree\n",
    "\n",
    "\n",
    "#fit inverse gamma distribution to all internal node distances and \n",
    "#exclude those beyong threshold probability\n",
    "def get_outlier_nodes_by_invgamma(tree, p_low=0, p_high=0.99, only_leaves=False):\n",
    "    \n",
    "    if only_leaves:\n",
    "        node_dists = [(node, node.dist) for node in tree.get_leaves()]\n",
    "    \n",
    "    else:\n",
    "        node_dists = [(node, node.dist) for node in tree.traverse()]\n",
    "    \n",
    "    dist_series = pd.Series([i[1] for i in node_dists])\n",
    "    \n",
    "    dist_stats = dist_series.describe()\n",
    "    \n",
    "    fit_alpha, fit_loc, fit_beta=stats.invgamma.fit(dist_series.values)\n",
    "\n",
    "    cutoff_high = stats.invgamma.ppf(p_high, a=fit_alpha, loc=fit_loc, scale=fit_beta)\n",
    "    cutoff_low = stats.invgamma.ppf(p_low, a=fit_alpha, loc=fit_loc, scale=fit_beta)\n",
    "    \n",
    "    outlier_nodes = [node[0] for node in node_dists if node[1] < cutoff_low or node[1] > cutoff_high]\n",
    "    print(f'Identified {len(outlier_nodes)} outlier nodes outside interval {cutoff_low} > d > {cutoff_high}')\n",
    "    \n",
    "    return outlier_nodes\n",
    "\n",
    "#starting from one leaf with an attribute traverse upwards untill \n",
    "#all leaves from the ancestor is no loger monophyletic under the given attribute\n",
    "#repeat for all remaining leaves\n",
    "#if any clade would have the global root as ancestor rerooot and retry to avoid \n",
    "#false paraphyly created by tree data struture\n",
    "def get_paraphyletic_groups(tree, attribute='tax_superkingdom', attr_value='Eukaryota', current_root=False):\n",
    "    \n",
    "    #tree.set_outgroup(tree.get_farthest_leaf()[0])\n",
    "    \n",
    "    if current_root:\n",
    "        tree.set_outgroup(current_root)\n",
    "    else:\n",
    "        current_root = tree.get_tree_root()\n",
    "    \n",
    "    #get a list of all leaves with an attribute matching the match value provided\n",
    "    check_leaves = [leaf for leaf in tree.get_leaves() if getattr(leaf, attribute)==attr_value]\n",
    "    clade_nodes = []\n",
    "    \n",
    "    seed_node = check_leaves[0]    \n",
    "\n",
    "    while check_leaves:\n",
    "        #assume monophyly\n",
    "        mono = True\n",
    "        \n",
    "        #check for all parent leaves if attribute matches the value, if not its not monophyletic, break \n",
    "        for leaf in seed_node.up.get_leaves():\n",
    "            if getattr(leaf, attribute)!=attr_value:\n",
    "                mono = False\n",
    "                break\n",
    "        \n",
    "        #if monophyletic try higher node \n",
    "        if mono:\n",
    "            seed_node = seed_node.up\n",
    "        \n",
    "        #else retrun node and exclude all leaves from list of leaves to check\n",
    "        else:\n",
    "            clade_nodes.append(seed_node)\n",
    "            check_leaves = [leaf for leaf in check_leaves if leaf not in seed_node.get_leaves()]\n",
    "            if check_leaves:\n",
    "                seed_node = check_leaves[0] \n",
    "    \n",
    "    #if parent has no parent it is the root\n",
    "    if [node for node in clade_nodes if node.up.up == None]:\n",
    "        #print('A tree clade has rooted parent nodes, rerooting')\n",
    "        \n",
    "        #get the first non-clade daughter from current root \n",
    "        non_clade_daughter = [node for node in current_root.children if node not in clade_nodes][0] \n",
    "        \n",
    "        return get_paraphyletic_groups(tree, attribute=attribute, attr_value=attr_value, current_root=non_clade_daughter)\n",
    "\n",
    "    else:\n",
    "        return clade_nodes\n",
    "    \n",
    "    \n",
    "#calculate the entropy of a list of labels\n",
    "def calculate_label_entropy(l):\n",
    "    H = 0\n",
    "    size = len(l)\n",
    "    for i in set(l):\n",
    "        n = l.count(i)\n",
    "        hl = (n/size)*np.log2(1/(n/size))\n",
    "        H += hl\n",
    "    return H\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#return the set of all leaf pairs\n",
    "def get_sister_leaf_sets(tree):\n",
    "    leaves = set(tree.get_leaves())\n",
    "    sister_sets = []\n",
    "    while leaves:\n",
    "        leaf = leaves.pop()\n",
    "        sister = leaf.get_sisters()[0]\n",
    "        if sister in leaves:\n",
    "            sister_sets.append(set((leaf, sister)))\n",
    "        \n",
    "    return sister_sets\n",
    "\n",
    "#give each node in tree a feature numerical id\n",
    "def add_node_ids(tree, strategy='postorder'):\n",
    "    for i, node in enumerate(tree.traverse(strategy=strategy)):\n",
    "        node.add_feature('post_i', i)\n",
    "    return tree\n",
    "\n",
    "#count all combintions of nodes such that no node is a decendant of any other \n",
    "#infeasible for more than 30 nodes as combinations scale fast!\n",
    "def enumerate_all_clades(tree):\n",
    "    stack = {}\n",
    "    clades = []\n",
    "\n",
    "    #stack a copy of the input tree as first tree\n",
    "    #use ID to prevent duplicates\n",
    "    subtree = tree.copy('deepcopy')\n",
    "    stack[subtree.get_topology_id()] = subtree\n",
    "\n",
    "    while stack:\n",
    "        print(f'Stack contains {len(stack)}')\n",
    "        \n",
    "        #take the last element out of the stack by id\n",
    "        subtree_id = list(stack.keys()).pop()\n",
    "        subtree = stack.pop(subtree_id)\n",
    "        \n",
    "        #save the leaf configuration\n",
    "        clades.append(tuple(leaf.post_i for leaf in subtree.get_leaves()))\n",
    "\n",
    "        sister_sets = get_sister_leaf_sets(subtree)\n",
    "        for pair in sister_sets:\n",
    "\n",
    "            ids = [i.post_i for i in pair]\n",
    "\n",
    "            recursetree = subtree.copy()\n",
    "            \n",
    "            #collapse sister nodes\n",
    "            for node in recursetree.traverse():\n",
    "                if node.post_i in ids:\n",
    "                    node.detach()\n",
    "            \n",
    "            #add cropped tree to stack if the root has children\n",
    "            if recursetree.children:\n",
    "                stack[recursetree.get_topology_id()] = recursetree\n",
    "\n",
    "    return clades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ed9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#each leaf has a weight of its last node depth, \n",
    "#sum weights of daughters postorder then normalize by total weight\n",
    "#---most consistent one so far---\n",
    "def weight_tree_nodes_bottom_up(tree, add_residual=True):\n",
    "    root = tree.get_tree_root()\n",
    "    \n",
    "    #add residual to correct for branch lengths of 0\n",
    "    residual = 0 \n",
    "    if add_residual:\n",
    "        residual = tree.get_farthest_node()[1]/len(tree)\n",
    "        \n",
    "    for node in tree.traverse(strategy='postorder'):\n",
    "        if node.is_leaf():\n",
    "            node.add_feature('weight', node.get_distance(root))\n",
    "            #print(f'{node.name} has weight {node.weight}')\n",
    "        else:\n",
    "            weight = sum([child.weight+residual for child in node.children])\n",
    "            node.add_feature('weight', weight)\n",
    "            #print(f'Internal {node.name} has weight {node.weight}')\n",
    "    \n",
    "    total_weight = sum([child.weight for child in tree.children])\n",
    "    \n",
    "    for node in tree.traverse(strategy='postorder'):\n",
    "        node.weight = node.weight/total_weight\n",
    "    \n",
    "    return total_weight\n",
    "\n",
    "#leaves have weight of distance to root\n",
    "#nodes have summed weight of children minus own distance to root\n",
    "def weight_tree_nodes_bottom_up_corrected(tree, add_residual=True):\n",
    "    root = tree.get_tree_root()\n",
    "    \n",
    "    #add residual to correct for branch lengths of 0\n",
    "    residual = 0 \n",
    "    if add_residual:\n",
    "        residual = tree.get_farthest_node()[1]/len(tree)\n",
    "        \n",
    "    for node in tree.traverse(strategy='postorder'):\n",
    "        if node.is_leaf():\n",
    "            node.add_feature('weight', node.get_distance(root)+residual)\n",
    "            #print(f'{node.name} has weight {node.weight}')\n",
    "        else:\n",
    "            weight = node.get_distance(root) + sum([child.dist for child in node.children])\n",
    "            node.add_feature('weight', weight+residual)\n",
    "            #print(f'Internal {node.name} has weight {node.weight}')\n",
    "    \n",
    "    total_weight = sum([child.weight for child in tree.children])\n",
    "    \n",
    "    for node in tree.traverse(strategy='postorder'):\n",
    "        node.weight = node.weight/total_weight\n",
    "    \n",
    "    return total_weight\n",
    "\n",
    "#each node distributes weight among its decendants dependent on distance \n",
    "def weight_tree_nodes(tree, add_residual=True):\n",
    "    root = tree.get_tree_root()\n",
    "    \n",
    "    #add residual to correct for branch lengths of 0\n",
    "    residual = 0 \n",
    "    if add_residual:\n",
    "        residual = 1/tree.get_farthest_node()[1]/len(tree)\n",
    "    \n",
    "    for node in tree.traverse(strategy='levelorder'):\n",
    "        if node == root:\n",
    "            #print('node is root weight is 1')\n",
    "            tree.add_feature('weight', 1)\n",
    "        \n",
    "        #print(f'node has {len(node.children)} children')\n",
    "        total_dist = sum([child.dist+residual for child in node.children])\n",
    "        \n",
    "        #print(total_dist)\n",
    "        for child in node.children:\n",
    "            weight = node.weight*child.dist/total_dist\n",
    "            child.add_feature('weight', weight)\n",
    "\n",
    "#leaves have weights of distance to root\n",
    "#nodes have weights of sum of children\n",
    "#then all nodes children get probabilities assigned as node.prob*(node.weight-child.weight)/node.weight\n",
    "def weight_tree_nodes_up_down(tree, add_residual=True):\n",
    "    root = tree.get_tree_root()\n",
    "    total_weight = 0\n",
    "    #add residual to correct for branch lengths of 0\n",
    "    \n",
    "    residual = 0 \n",
    "    if add_residual:\n",
    "        residual = tree.get_farthest_node()[1]/len(tree)\n",
    "\n",
    "    for node in tree.traverse(strategy='postorder'):\n",
    "        if node.is_leaf():\n",
    "            node.add_feature('weight', node.get_distance(root))\n",
    "        else:\n",
    "            weight = sum([child.weight+residual for child in node.children])\n",
    "            node.add_feature('weight', weight)\n",
    "            \n",
    "    root.prob = 1\n",
    "    \n",
    "    for node in tree.traverse(strategy='levelorder'):\n",
    "        #print(node.name, node.weight, node.prob)\n",
    "        if len(node.children) == 1:\n",
    "            #for linked internal nodes without branches \n",
    "            child.add_feature('prob', node.prob)\n",
    "        else:\n",
    "            for child in node.children:\n",
    "                prob = node.prob*(node.weight-child.weight)/node.weight\n",
    "                child.add_feature('prob', prob)\n",
    "                #print(child.name, child.prob)\n",
    "\n",
    "    return total_weight\n",
    "\n",
    "\n",
    "#leaves have weight equal to the distance to last node\n",
    "#nodes have weights equal to sum of children\n",
    "#normalised by total weight\n",
    "def weight_tree_nodes_local(tree, add_residual=True):\n",
    "    root = tree.get_tree_root()\n",
    "    \n",
    "    residual = 0 \n",
    "    if add_residual:\n",
    "        residual = tree.get_farthest_node()[1]/len(tree)\n",
    "        \n",
    "    for node in tree.traverse(strategy='postorder'):\n",
    "        if node.is_leaf():\n",
    "            node.add_feature('weight', node.dist+residual)\n",
    "        else:\n",
    "            weight = sum([child.weight+residual for child in node.children])\n",
    "            node.add_feature('weight', weight)\n",
    "        \n",
    "    for node in tree.traverse(strategy='levelorder'):\n",
    "        sum_weight = sum([child.dist for child in node.children])+node.dist\n",
    "        for child in node.children: \n",
    "            prob = child.dist/sum_weight\n",
    "            child.add_feature('prob', prob) \n",
    "            \n",
    "#leaves have weights equal to the root distance normalised of all leaves\n",
    "#nodes have weights equal to the sum of their children\n",
    "def weight_tree_nodes_top_down_prob(tree, add_residual=True):\n",
    "    root = tree.get_tree_root()\n",
    "    \n",
    "    residual = 0 \n",
    "    if add_residual:\n",
    "        residual = tree.get_farthest_node()[1]/len(tree)\n",
    "\n",
    "    root.add_feature('weight', 1)\n",
    "    \n",
    "    for node in tree.traverse(strategy='levelorder'):\n",
    "        sum_dists = sum([child.dist+residual for child in node.children])\n",
    "        \n",
    "        #for nested unbranched nodes in pathological cases\n",
    "        if len(node.children) == 1:\n",
    "            node.children[0].add_feature('weight', node.weight)\n",
    "            \n",
    "        else:    \n",
    "            for child in node.children:\n",
    "                #farther nodes are lesss probable\n",
    "                weight = node.weight*(sum_dists-child.dist+residual)/sum_dists\n",
    "                #farther nodes are more probable\n",
    "                #weight = node.weight*(child.dist+residual)/sum_dists\n",
    "                child.add_feature('weight', weight)\n",
    "\n",
    "                \n",
    "#using precomputed weights\n",
    "def calculate_mututal_I_and_distortion(tree, nodes, uniform=False):\n",
    "    #initialise\n",
    "    leaves = tree.get_leaves()\n",
    "    depth = tree.get_farthest_leaf()[1]\n",
    "    Icx = 0\n",
    "    dis = 0\n",
    "    tiny = np.finfo(np.float64).eps\n",
    "\n",
    "\n",
    "    Px = np.array([leaf.weight for leaf in leaves])\n",
    "\n",
    "    for C in nodes:\n",
    "        \n",
    "        Pc = C.weight\n",
    "        #mututal information is done over all x\n",
    "        Plca2 = np.array([C.get_common_ancestor(x).weight**2 for x in leaves])\n",
    "        Icx += (((Pc**2)/Plca2)*Px*np.log2(Pc/Plca2)).sum()\n",
    "        \n",
    "        #distortion calculations are done over all x\n",
    "        #difference in probabilities\n",
    "        d = np.abs(Px-Pc)\n",
    "        dis += (((Pc**2)/Plca2)*Px*d).sum()\n",
    "\n",
    "    return dis, Icx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ca590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evalutaing automatic clustering scoring usign MI and compression\n",
    "\n",
    "tree_file = 'microcosm2/OAE21175.1/test/WP_172991781_1.cluster.clu.euk.tree'\n",
    "seq_file = 'microcosm2/OAE21175.1/test/WP_172991781_1.cluster.clu'\n",
    "cluster_file = 'microcosm2/KAA6409619.1/KAA6409619.1.members.cluster.tsv'\n",
    "\n",
    "mmseqs_clusters = [value for key, value in read_cluster_tsv(cluster_file).items()]\n",
    "\n",
    "tree = Tree(tree_file)\n",
    "tree_depth = tree.get_farthest_leaf()[1]\n",
    "tree_shallow = tree.get_closest_leaf()[1]\n",
    "#add weights\n",
    "weight_tree_nodes_bottom_up(tree, add_residual=True)\n",
    "print(tree_depth, tree_shallow, tree_file, seq_file)\n",
    "\n",
    "#lower T less clusters\n",
    "#set the distortion cator to average of extremes\n",
    "T=(tree_depth-tree_shallow)/2\n",
    "#T=1\n",
    "\n",
    "cluster_data = pd.DataFrame(columns=['clusters', 'dis', 'Icx', 'dis+Icx'])\n",
    "partitions = {}\n",
    "\n",
    "#\n",
    "#DEAL WITH THE SINGLE CLASS EVENT\n",
    "\n",
    "best_score = 9999999\n",
    "current_clusters = 0\n",
    "break_count = 0\n",
    "break_limit = 5\n",
    "\n",
    "for f in np.linspace(1.5,0,2).round(2):\n",
    "    print(f)\n",
    "    #cluster the tree using a TreeClust method\n",
    "    tree_clusters = cluster_TreeClust(tree, tree_depth*f)\n",
    "    #get LCA ancestors and calculate MI and distortion\n",
    "    tree_cluster_LCA = [get_LCA(tree, cluster) for cluster in tree_clusters]\n",
    "\n",
    "\n",
    "    distortion, Icx  = calculate_mututal_I_and_distortion(tree, tree_cluster_LCA, T)\n",
    "    \n",
    "    #reweight distortion\n",
    "    distortion = distortion*T\n",
    "    \n",
    "    #save each partition for analysis\n",
    "    partitions[f] = (tree_clusters, tree_cluster_LCA)\n",
    "    cluster_data.loc[f] = [len(tree_clusters), distortion, Icx, (distortion+Icx)]\n",
    "    \n",
    "    \n",
    "\n",
    "    #add break counter if clsuters increased and optimal value did not decrease\n",
    "    if (distortion+Icx) < best_score:\n",
    "        best_score = distortion+Icx\n",
    "        current_clusters = len(tree_clusters)\n",
    "        break_count = 0\n",
    "        print('Score improoved, reset break counter')\n",
    "        print(f, len(tree_clusters), distortion, Icx, (distortion+Icx), sep='\\t')\n",
    "    \n",
    "    #break after repeated failures with increasing clusters\n",
    "    elif len(tree_clusters) != current_clusters:\n",
    "        current_clusters = len(tree_clusters)\n",
    "        \n",
    "        print(f'Score did not improove, despite different clusters try: {break_count}')\n",
    "        print(f, len(tree_clusters), distortion, Icx, (distortion+Icx), sep='\\t')\n",
    "        break_count += 1\n",
    "        \n",
    "        if break_count > break_limit:\n",
    "            print('Failed to improve after 5 attempts breaking!')\n",
    "            break\n",
    "        \n",
    "#parse data for plotting\n",
    "cluster_data.reset_index(inplace=True, names='f')\n",
    "cluster_data.drop_duplicates('clusters', inplace=True)\n",
    "#cluster_data.sort_values(by='f', inplace=True)\n",
    "cluster_data_melt = cluster_data.melt(id_vars=['clusters','f'])\n",
    "\n",
    "best_run = cluster_data[cluster_data['dis+Icx'] == cluster_data['dis+Icx'].min()].index.values\n",
    "print(cluster_data.loc[best_run])\n",
    "optimal_f = cluster_data.loc[best_run,'f'].values[0]\n",
    "\n",
    "f_distplot = alt.Chart(cluster_data_melt).mark_line(interpolate='step').encode(\n",
    "    alt.X('f:O'),\n",
    "    alt.Y('value', axis=alt.Axis(labelAlign='left')),\n",
    "    alt.Color('variable')\n",
    ")\n",
    "c_distplot = alt.Chart(cluster_data_melt).mark_bar(color=warmgrays[2],interpolate='step-before',\n",
    "                                            fillOpacity=0.05, line=True).encode(\n",
    "    alt.X('f:O'),\n",
    "    alt.Y('clusters')\n",
    ")\n",
    "#f_distplot.resolve_scale('independent')\n",
    "(c_distplot + f_distplot).resolve_scale(y='independent')\n",
    "\n",
    "tree_clusters, tree_cluster_LCA = partitions[optimal_f]\n",
    "print([len(cluster) for cluster in tree_clusters])\n",
    "\n",
    "#replace with mmseqs clusters\n",
    "#tree_clusters = mmseqs_clusters\n",
    "\n",
    "colors = colorlib['twilight_shifted_r_perm']\n",
    "class_colors = [colors[i%len(colors)] for i, _ in enumerate(tree_clusters)]\n",
    "classes = len(tree_clusters)\n",
    "\n",
    "#define overall stree styling\n",
    "ts = TreeStyle()\n",
    "ts.mode = 'c'\n",
    "ts.show_leaf_name = False\n",
    "ts.show_branch_length = False\n",
    "ts.show_branch_support = False\n",
    "#ts.optimal_scale_level = 'full'\n",
    "ts.allow_face_overlap = True\n",
    "ts.scale = 50\n",
    "\n",
    "default_node_style = ete3.NodeStyle()\n",
    "default_node_style['size'] = 0\n",
    "default_node_style['fgcolor'] = 'Black'\n",
    "\n",
    "#set default colors\n",
    "for _, node in tree.iter_prepostorder():\n",
    "    node.set_style(default_node_style)\n",
    "\n",
    "#color LCA nodes\n",
    "for i, node in enumerate(tree_cluster_LCA):\n",
    "    LCA_node_style = ete3.NodeStyle()\n",
    "    LCA_node_style['fgcolor'] = class_colors[i]\n",
    "    LCA_node_style['size'] = 2\n",
    "    node.set_style(LCA_node_style)\n",
    "    \n",
    "\n",
    "#color leaves\n",
    "for i, cluster in enumerate(tree_clusters):\n",
    "    class_style = ete3.NodeStyle()\n",
    "    class_style['fgcolor'] = class_colors[i]\n",
    "    class_style['size'] = 1\n",
    "    for leaf in cluster:\n",
    "        tree.get_leaves_by_name(leaf)[0].set_style(class_style)\n",
    "        \n",
    "tree.ladderize()\n",
    "\n",
    "view_tree(tree, ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12424f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weighting scheme testing\n",
    "\n",
    "t1 = Tree('((((A,B)D,E)F,G)H,(I,J)K)root;', format=1)\n",
    "print(t1.get_ascii(attributes=['name']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51322541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove isolated leaves by checking all monophyletic nodes for singletons\n",
    "def trim_singleton_leaves(tree, attribute='tax_superkingdom', attr_value='Eukaryota', min_size=1, detach=True):\n",
    "    \n",
    "    LCA_groups = get_paraphyletic_groups(tree, attribute=attribute, attr_value=attr_value)\n",
    "    \n",
    "    pruned_leaves = []\n",
    "    \n",
    "    for node in LCA_groups:\n",
    "        #remove all LCA leaves which are not in the majority among its neighbors\n",
    "        if len(node.get_leaves()) <= min_size:\n",
    "            neigbour_attr = [getattr(leaf, attribute) for leaf in node.up.get_leaves()]\n",
    "            \n",
    "            if neigbour_attr.count(attr_value)/len(neigbour_attr) < 0.5 and detach:\n",
    "                pruned_leaves.append(node.detach())\n",
    "            \n",
    "            else: \n",
    "                pruned_leaves.append(node)\n",
    "                \n",
    "    return pruned_leaves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cffb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the entropy of decendant and non decendant leaf labels \n",
    "def get_entropy_for_partition(tree, node, attribute='tax_filter', attr_value='Eukaryota'):\n",
    "    \n",
    "    all_labels = [getattr(leaf, attribute) for leaf in tree.get_leaves()]\n",
    "    all_label_count = all_labels.count(attr_value)\n",
    "    tree_width = len(all_labels)\n",
    "    \n",
    "#     base_label_Px = (all_label_count/tree_width)\n",
    "#     base_label_H = base_label_Px*np.log2(1/base_label_Px)\n",
    "    \n",
    "    clade_labels = [getattr(leaf, attribute) for leaf in node.get_leaves()]\n",
    "    clade_label_count = clade_labels.count(attr_value)\n",
    "    clade_width = len(clade_labels)\n",
    "    \n",
    "    #calculate label entropy\n",
    "    #print('AAA', clade_labels, clade_label_count, clade_width)\n",
    "    label_Px = (clade_label_count)/(clade_width)\n",
    "    label_H = label_Px*np.log2(1/label_Px)\n",
    "    \n",
    "    #calculate external entropy change\n",
    "    \n",
    "    # if all labels in the clade the external entropy is 0\n",
    "    if all_label_count-clade_label_count == 0:\n",
    "        external_label_H = 0\n",
    "\n",
    "    else:\n",
    "        #calculate external entropy change\n",
    "        external_label_Px = (all_label_count-clade_label_count)/(tree_width-clade_width)\n",
    "        external_label_H = external_label_Px*np.log2(1/external_label_Px)\n",
    "\n",
    "\n",
    "    return label_H, external_label_H\n",
    "    \n",
    "#assign soft LCA node based on minimizing entropy between given label outside and inside clade\n",
    "#more pessimissive than voting ratio, qualitatively underestimates\n",
    "def get_soft_LCA_by_relative_entropy(tree, attribute='tax_superkingdom', attr_value='Eukaryota', save_loss=False):\n",
    "    #count vote ratio for each node for one taxa\n",
    "    tree_width = len(tree.get_leaves())\n",
    "    root = tree.get_tree_root()\n",
    "    \n",
    "    lowest_total_H = float(\"inf\")\n",
    "    best_node = root\n",
    "    vote_label = attr_value\n",
    "    \n",
    "    all_labels = [getattr(leaf, attribute) for leaf in tree.get_leaves()]\n",
    "    all_label_count = all_labels.count(attr_value)\n",
    "    tree_width = len(all_labels)\n",
    "    \n",
    "    all_label_Px = (all_label_count/tree_width)\n",
    "    all_labels_H = all_label_Px*np.log2(1/all_label_Px)\n",
    "    \n",
    "    #for debugging\n",
    "    if save_loss:\n",
    "        for node in tree.traverse():\n",
    "            node.add_feature('vote_loss', 'None')\n",
    "    \n",
    "    #check for monophyly\n",
    "    LCA_groups = get_paraphyletic_groups(tree, attribute=attribute, attr_value=attr_value)\n",
    "    if len(LCA_groups) == 1:\n",
    "        print(f'The attribute {attribute} is monophyletic for {attr_value}. Returning LCA node.')\n",
    "        return (LCA_groups[0], 0)\n",
    "\n",
    "\n",
    "    #the best partition will be on the path from an LCA node to the root\n",
    "    tested_nodes = []\n",
    "    for node in LCA_groups:\n",
    "        \n",
    "        node_label_count = len([leaf for leaf in node.get_leaves() if getattr(leaf, attribute) == attr_value])\n",
    "\n",
    "        #ascend until all labeled leaves are decendants of node \n",
    "        while node != root:\n",
    "           \n",
    "            #skip known nodes\n",
    "            if node not in tested_nodes:\n",
    "                \n",
    "                #calculate internal and external entropy\n",
    "                label_H, external_label_H =  get_entropy_for_partition(tree, node, attribute=attribute, attr_value=attr_value)\n",
    "                total_H = label_H + external_label_H\n",
    "                    \n",
    "                #penalize leaf LCAs to avoid laddered LCAs when having repeated outgroups\n",
    "                #not neccesarily waned as spread singleons get their global LCA as soft_LCA\n",
    "                if node.is_leaf():\n",
    "                    total_H += 0.5\n",
    "                \n",
    "                #update best guess\n",
    "                if total_H < lowest_total_H:\n",
    "                    lowest_total_H = total_H\n",
    "                    best_node = node\n",
    "                \n",
    "                if save_loss:\n",
    "                    node.add_feature('vote_loss', total_H)\n",
    "            \n",
    "            #break after calculations if all nodes are decendants\n",
    "            node_label_count = len([leaf for leaf in node.get_leaves() if getattr(leaf, attribute) == attr_value])\n",
    "            if node_label_count == all_label_count:\n",
    "                break\n",
    "            #print(node_label_count, all_label_count)\n",
    "            \n",
    "            #ascend\n",
    "            tested_nodes.append(node)\n",
    "            node = node.up\n",
    "            \n",
    "            \n",
    "    #print(f'Best node for {attr_value} has a total H of {lowest_total_H}')\n",
    "    return (best_node, lowest_total_H)\n",
    "\n",
    "def get_multiple_soft_LCAs(tree, attribute='tax_filter', attr_value='Eukaryota', min_size=1, min_purity=0, max_entropy=9999):\n",
    "    \n",
    "    print(f'Searching for LCA_nodes by checking where {attribute} is {attr_value}')\n",
    "        \n",
    "    soft_LCA_nodes = []\n",
    "    total_nodes = len([getattr(node, attribute) for node in tree.get_leaves() if getattr(node, attribute) == attr_value])    \n",
    "    \n",
    "    while total_nodes > 0:\n",
    "        \n",
    "        soft_LCA_node, lowest_H_loss = get_soft_LCA_by_relative_entropy(tree, attribute=attribute, attr_value=attr_value)\n",
    "        \n",
    "\n",
    "        soft_LCA_node_leaves = soft_LCA_node.get_leaves()\n",
    "        soft_LCA_node_size = len(soft_LCA_node_leaves)\n",
    "        \n",
    "        #mark labeled included nodes and decrement total nodes left to check\n",
    "        for node in soft_LCA_node_leaves:\n",
    "            if getattr(node, attribute) == attr_value:\n",
    "                setattr(node, attribute, 'SAMPLED')\n",
    "                total_nodes -= 1\n",
    "        \n",
    "            \n",
    "        soft_LCA_nodes.append(soft_LCA_node)\n",
    "        \n",
    "    #reset leaf node attributes\n",
    "    for node in tree.get_leaves():\n",
    "        if getattr(node, attribute) == 'SAMPLED':\n",
    "            setattr(node, attribute, attr_value)        \n",
    "    \n",
    "    #recalculate entropies\n",
    "    print(f'\\tRecalculating sizes, purities and entropies for all LCA nodes')\n",
    "    filtered_soft_LCA_nodes = []\n",
    "    \n",
    "    for i, node in enumerate(soft_LCA_nodes):\n",
    "        label_H, external_label_H = get_entropy_for_partition(tree, node, attribute=attribute, attr_value=attr_value)\n",
    "        \n",
    "        soft_LCA_members = [getattr(leaf, attribute) for leaf in node.get_leaves()]\n",
    "        soft_LCA_size = len(soft_LCA_members)\n",
    "        soft_LCA_purity = soft_LCA_members.count(attr_value) / soft_LCA_size\n",
    "        soft_LCA_entropy = label_H + external_label_H\n",
    "        \n",
    "        if soft_LCA_size >= min_size and soft_LCA_purity >= min_purity and soft_LCA_entropy <= max_entropy:\n",
    "            print(f'\\tFound node of size {soft_LCA_size} with purity of {soft_LCA_purity} and entropy {soft_LCA_entropy} as LCA for {attr_value}')\n",
    "            filtered_soft_LCA_nodes.append([node, soft_LCA_size, soft_LCA_purity, soft_LCA_entropy])\n",
    "        else:\n",
    "            print(f'\\tRejected node of size {soft_LCA_size} with purity of {soft_LCA_purity} and entropy {soft_LCA_entropy} as LCA for {attr_value}')\n",
    "    \n",
    "    if len(filtered_soft_LCA_nodes) == 0:\n",
    "        print(f'WARNING: No valid LCA nodes present for {attribute} = {attr_value} under conditions min_size={min_size}, min_purity={min_purity}, max_entropy={max_entropy}') \n",
    "    \n",
    "    print()\n",
    "    \n",
    "    return filtered_soft_LCA_nodes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af05afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_files = !find testing/IQtree_reverse/ -name '*.contree'\n",
    "\n",
    "tree_files = !find microcosm/ -name '*fil.contree'\n",
    "\n",
    "file = tree_files[1]\n",
    "\n",
    "#file = 'testing/trees/EPZ30938.1.members.fasta.haln.euk.fil.fasttree.2'\n",
    "#for file in tree_files:\n",
    "print(file)\n",
    "tree = Tree(file)\n",
    "tree_name = file.split('/')[1]\n",
    "tree_header = euk_header[euk_header.acc==tree_name].header.values\n",
    "print(tree_header)\n",
    "dirty_phyla_add(tree, tax_merge)\n",
    "\n",
    "#merge leaf pairs until total amount of leaves is smaller than x\n",
    "tree = reduce_leaves_to_size(tree, 300)\n",
    "\n",
    "print(len(tree.get_leaves()))\n",
    "\n",
    "#calculate devaiting branch distances\n",
    "outlier_nodes =  get_outlier_nodes_by_invgamma(tree, p_low=0, p_high=0.999, only_leaves=False)\n",
    "#cut_nodes = [node.detach() for node in outlier_nodes]\n",
    "\n",
    "\n",
    "#calculate soft LCA nodes for prok and euk using partition entropy \n",
    "filter_taxa = set([leaf.tax_filter for leaf in tree.get_leaves()])\n",
    "hard_LCA_dict = {}\n",
    "soft_LCA_dict = {}\n",
    "\n",
    "for tax in filter_taxa:\n",
    "    hard_LCA_dict[tax] = get_paraphyletic_groups(tree, attribute='tax_filter', attr_value=tax)\n",
    "    soft_LCA_dict[tax] = get_multiple_soft_LCAs(tree, attribute='tax_filter', attr_value=tax,\n",
    "                                               min_size=2, min_purity=0.5)\n",
    "    \n",
    "soft_LCA_dict['Eukaryota'] = get_multiple_soft_LCAs(tree, attribute='tax_filter', attr_value='Eukaryota',\n",
    "                                                    min_size=3, min_purity=0.90)\n",
    "\n",
    "#add LCA nodes to list for NODE visualisation\n",
    "hard_LCA_nodes = [node for LCA_nodes in hard_LCA_dict.values() for node in LCA_nodes]\n",
    "soft_LCA_nodes = [node[0][0] for node in soft_LCA_dict.values() if node != []]\n",
    "\n",
    "#add LCA labels for LABEL visualisation\n",
    "for node in tree.traverse():\n",
    "    node.add_feature('soft_LCA', '')\n",
    "    node.add_feature('soft_LCA_H', '')\n",
    "    \n",
    "#add the first valid soft_LCA as LCA for prok and all valid soft_LCAs for euk\n",
    "for tax, nodes in soft_LCA_dict.items():\n",
    "    if nodes != []:\n",
    "        #add all euk nodes\n",
    "        if tax == 'Eukaryota':\n",
    "            for node in nodes:\n",
    "                node[0].soft_LCA = tax\n",
    "                node[0].soft_LCA_H = node[3]                \n",
    "        \n",
    "        \n",
    "        #add first prok node\n",
    "        else:\n",
    "            nodes[0][0].soft_LCA = tax\n",
    "            nodes[0][0].soft_LCA_H = nodes[0][3]\n",
    "    \n",
    "    #skip empty\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "# #track one soft LCA label\n",
    "# tax = 'Flavobacteriia'\n",
    "# soft_LCA_dict[tax] = get_soft_LCA_by_relative_entropy(tree, attribute='tax_class', attr_value=tax, save_loss=True)\n",
    "# for node in tree.traverse():\n",
    "#     node.add_face(TextFace(node.vote_loss, fsize=4), column=0)\n",
    "\n",
    "#define overall tree styling\n",
    "ts = TreeStyle()\n",
    "ts.title.add_face(TextFace(tree_header, fsize=8), column=0)\n",
    "ts.mode = 'r'\n",
    "ts.show_leaf_name = False\n",
    "ts.show_branch_length = False\n",
    "ts.show_branch_support = False\n",
    "#ts.optimal_scale_level = 'full'\n",
    "ts.allow_face_overlap = True\n",
    "ts.scale = 50\n",
    "\n",
    "default_node_style = ete3.NodeStyle()\n",
    "default_node_style['size'] = 0\n",
    "default_node_style['fgcolor'] = 'Black'\n",
    "\n",
    "default_leaf_style = ete3.NodeStyle()\n",
    "default_leaf_style['size'] = 0\n",
    "default_leaf_style['fgcolor'] = 'Black'\n",
    "\n",
    "arc_node_style = ete3.NodeStyle()\n",
    "arc_node_style['size'] = 2\n",
    "arc_node_style['fgcolor'] = 'Cyan'\n",
    "\n",
    "euk_node_style = ete3.NodeStyle()\n",
    "euk_node_style['size'] = 2\n",
    "euk_node_style['fgcolor'] = 'Green'\n",
    "\n",
    "outlier_node_style = ete3.NodeStyle()\n",
    "outlier_node_style['size'] = 3\n",
    "outlier_node_style['fgcolor'] = 'Red'\n",
    "\n",
    "hard_LCA_node_style = ete3.NodeStyle()\n",
    "hard_LCA_node_style['size'] = 2\n",
    "hard_LCA_node_style['fgcolor'] = 'Gray'\n",
    "\n",
    "soft_LCA_node_style = ete3.NodeStyle()\n",
    "soft_LCA_node_style['size'] = 4\n",
    "soft_LCA_node_style['fgcolor'] = 'Black'\n",
    "\n",
    "soft_euk_LCA_node_style = ete3.NodeStyle()\n",
    "soft_euk_LCA_node_style['size'] = 4\n",
    "soft_euk_LCA_node_style['fgcolor'] = 'Gray'\n",
    "\n",
    "#set styling for all leaves and internal nodes\n",
    "for node in tree.traverse():\n",
    "    node.set_style(default_node_style)\n",
    "    node.add_face(TextFace(node.soft_LCA, fsize=4), column=0)\n",
    "    node.add_face(TextFace(node.soft_LCA_H, fsize=4), column=0)\n",
    "\n",
    "    if node.is_leaf():\n",
    "        node.add_face(TextFace(node.name, fsize=4), column=1)\n",
    "        node.add_face(TextFace(' '+node.tax_class,  fsize=4), column=2)\n",
    "        node.set_style(default_leaf_style)\n",
    "\n",
    "        if node.tax_superkingdom == 'Eukaryota':\n",
    "            node.set_style(euk_node_style)\n",
    "            \n",
    "        elif node.tax_superkingdom == 'Archaea':\n",
    "            node.set_style(arc_node_style)\n",
    "\n",
    "    if node in hard_LCA_nodes:\n",
    "        node.set_style(hard_LCA_node_style)\n",
    "    \n",
    "    if node in soft_LCA_nodes:\n",
    "        node.set_style(soft_LCA_node_style)\n",
    "\n",
    "    if node in outlier_nodes or node in euk_singletons:\n",
    "        node.set_style(outlier_node_style)\n",
    "\n",
    "#tree.set_outgroup(tree.get_midpoint_outgroup())\n",
    "tree.ladderize()\n",
    "#tree.render(file+'.pdf', tree_style=ts)\n",
    "#tree.write(features=[\"name\", \"tax_superkingdom\"], outfile=file+'_annot')\n",
    "view_tree(tree, ts, backup=file+'.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a944a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATE TQtree2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843807bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "#read and split data table from IQtree\n",
    "model_data_file = 'testing/IQtree_models/merged_output.txt'\n",
    "with open(model_data_file, 'r') as infile:\n",
    "    #models = [m.split('\\n') for m in infile.read().split(';')]\n",
    "    models = infile.read().split(';')\n",
    "\n",
    "#loop over models adding observations of rank as index to the data\n",
    "#data is already sorted\n",
    "data = pd.DataFrame()\n",
    "for model in models:\n",
    "    new_data = pd.read_csv(io.StringIO(model), delim_whitespace=True)\n",
    "    new_data.reset_index(inplace=True, names='rank')\n",
    "    new_data['rank'] = [max(10-x, 0) for x in new_data['rank']]\n",
    "    data = pd.concat([data,new_data])\n",
    "\n",
    "data_models = pd.Series({model:data[data.Model == model]['rank'].sum()/len(models) for model in data.Model.unique()})\n",
    "data_models.sort_values(ascending=False)[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923de14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.DataFrame(data_models)\n",
    "plot_data.reset_index(inplace=True)\n",
    "plot_data.columns = ['model', 'score']\n",
    "plot_data.sort_values(by='score', ascending=False, inplace=True)\n",
    "plot_data['name']= [l.split('+')[0] for l in plot_data.model]\n",
    "plot_data.reset_index(inplace=True, drop=True)\n",
    "display(plot_data[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5d92c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = alt.Chart(plot_data[0:30]).mark_bar().encode(\n",
    "    x= alt.X('model:O', sort=None, axis=alt.Axis(labelAngle=-45)),\n",
    "    color = alt.Color('name'),\n",
    "    y= alt.Y('score'),\n",
    ").interactive()\n",
    "plot.width = 1200\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a91d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix further discrepancies in eukprot species labelling not reachable from orgid\n",
    "\n",
    "from ete3 import NCBITaxa\n",
    "\n",
    "\n",
    "error_ids = [2985, 191814, 35133, 88547, 438412, 419944, 1104430, 63605, 35686, 299832, 36769, 91373, 186019, 37163, 446134, 5748, 265536, 3028, 195968, 5774, 2949, 81532, 5709, 947084, 12967]\n",
    "\n",
    "error_lineages = NCBITaxa().get_lineage_translator(error_ids)\n",
    "\n",
    "error_lineages\n",
    "\n",
    "#flat_list = [ x for xs in xss for x in xs ]\n",
    "error_dict = [x for xs in error_lineages.values() for x in xs]\n",
    "error_dict = NCBITaxa().get_taxid_translator(error_dict)\n",
    "\n",
    "\n",
    "errorDF = pd.DataFrame()\n",
    "\n",
    "for error_id, lineage in error_lineages.items():\n",
    "\n",
    "    try:\n",
    "        error_class = [error_dict[c] for c in lineage if NCBITaxa().get_rank([c])[c] == 'class']\n",
    "    except ValueError:\n",
    "        error_class = ['none']\n",
    "        \n",
    "    if error_class == []:\n",
    "        error_class = ['none']\n",
    "        \n",
    "    error_name = NCBITaxa().get_taxid_translator([error_id])[error_id]\n",
    "    print(error_id, error_class, error_name)        \n",
    "    classDF = pd.DataFrame({'species': error_name, 'superkingdom':'Eukaryota', 'class':error_class},index=[error_id])\n",
    "    errorDF = pd.concat([errorDF, classDF])\n",
    "\n",
    "errorDF.loc[63605]['class'] = 'Heterolobosea'\n",
    "errorDF.loc[419944]['class'] = 'Picozoa'\n",
    "errorDF.loc[438412]['class'] = 'Amoebozoa'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3587d1a9",
   "metadata": {},
   "source": [
    "---- Temporary ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ceedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check wellnes of fit of inverse gamma distribution for tree subset\n",
    "treefiles = subprocess.run(f\"find processing/microcosm2/ -name '*.treefile'\",shell=True, text=True, capture_output=True)\n",
    "treefiles = [f for f in treefiles.stdout.split('\\n')][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44434612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check wellnes of fit of inverse gamma distribution for tree subset\n",
    "for file in treefiles:\n",
    "\n",
    "    #test invgamma\n",
    "    treefile = file\n",
    "    print(file)\n",
    "    tree = Tree(treefile)\n",
    "    p_high = 0.99\n",
    "    p_low = 0\n",
    "\n",
    "    outlier_nodes, dist_series, fitting_params, cutoffs = get_outlier_nodes_by_invgamma(tree, p_low, p_high)\n",
    "    print(fitting_params)\n",
    "\n",
    "    x_max = cutoffs[0] + 0.2\n",
    "\n",
    "    #tree data\n",
    "    dist_data = pd.DataFrame(dist_series)\n",
    "    dist_data.columns = ['stem_length']\n",
    "\n",
    "    #probability density values from fit\n",
    "    x = np.linspace(stats.invgamma.pdf(0.01, fitting_params[0], fitting_params[1], fitting_params[2]),\n",
    "                    stats.invgamma.pdf(0.99, fitting_params[0], fitting_params[1], fitting_params[2]), 1000)\n",
    "    x = np.linspace(0.01, x_max, 1000)\n",
    "\n",
    "    y = stats.invgamma.pdf(x, fitting_params[0], fitting_params[1], fitting_params[2])\n",
    "\n",
    "    fit_data = pd.DataFrame({'stem_length':x, 'prob_density':y})\n",
    "    fit_data = fit_data[fit_data.stem_length < x_max]\n",
    "\n",
    "    #simulated data\n",
    "    sim_data = stats.invgamma.rvs(fitting_params[0], fitting_params[1], fitting_params[2], size=len(tree))\n",
    "    sim_data = pd.DataFrame(pd.Series(sim_data))\n",
    "    sim_data.columns = ['stem_length']\n",
    "\n",
    "\n",
    "    #plot cumsum hist\n",
    "    plot, data = plot_cumsum_counts(dist_data.stem_length, title='experimental')\n",
    "    plot, fit = plot_cumsum_counts(sim_data.stem_length, title='simulated from fit')\n",
    "\n",
    "    merged = pd.concat([data, fit])\n",
    "\n",
    "    cum_plot, merged_data = plot_cumsum_counts(merged, plot_type='default', formatted_data=True,\n",
    "                                          x_min = 0, x_max=5, x_scale_type='linear', y_scale_type='linear')\n",
    "\n",
    "    ks_test = stats.ks_2samp(dist_data.stem_length, fit_data.stem_length, alternative='two-sided', mode='auto')\n",
    "    cum_plot.title = alt.TitleParams(f'p={ks_test.pvalue}, statistic={ks_test.statistic} ', fontSize=12)\n",
    "\n",
    "\n",
    "    #data_hist with line fit\n",
    "    title = alt.TitleParams(f\"{treefile.split('/')[-1]}, {len(tree)} leaves, 99% CDF {cutoff_high}\", fontSize=12)\n",
    "    data_hist = alt.Chart(dist_data, title = title).mark_bar().encode(\n",
    "        x= alt.X('stem_length', bin=alt.Bin(extent=[0, x_max], step=0.02)),\n",
    "        y= alt.Y('count()')\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    fit_curve = alt.Chart(fit_data).mark_line(color='red').encode(\n",
    "        x= alt.X('stem_length', scale=alt.Scale(domain=[0, x_max])),\n",
    "        y= alt.Y('prob_density')\n",
    "\n",
    "    )\n",
    "\n",
    "    plot = alt.layer(data_hist, fit_curve).resolve_scale(y='independent')\n",
    "    merge_plot = plot | cum_plot\n",
    "    \n",
    "    altair_saver.save(merge_plot, treefile+'.html' )\n",
    "\n",
    "    subprocess.run(f'cp {treefile} /home/tobiassonva/data/eukgen/testing/invgamma_fitting/'.split())\n",
    "    subprocess.run(f'mv {treefile}.html /home/tobiassonva/data/eukgen/testing/invgamma_fitting/'.split())\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check wellnes of fit of lognorm distribution for tree subset\n",
    "from core_functions.tree_functions import get_outlier_nodes_by_lognorm\n",
    "\n",
    "#test lognorm\n",
    "#treefile = treefiles[27]\n",
    "tree = Tree(treefile)\n",
    "p_high = 0.99\n",
    "p_low = 0.0\n",
    "\n",
    "outlier_nodes, dist_series, fitting_params = get_outlier_nodes_by_lognorm(tree, p_low, p_high)\n",
    "print(fitting_params)\n",
    "\n",
    "x_max = fitting_params[4] + 0.2\n",
    "\n",
    "#tree data\n",
    "dist_data = pd.DataFrame(dist_series)\n",
    "dist_data.columns = ['stem_length']\n",
    "\n",
    "#probability density values from fit\n",
    "x = np.linspace(stats.lognorm.pdf(0.01, fitting_params[0], fitting_params[1], fitting_params[2]),\n",
    "                stats.lognorm.pdf(0.99, fitting_params[0], fitting_params[1], fitting_params[2]), 1000)\n",
    "x = np.linspace(0.01, x_max, 1000)\n",
    "\n",
    "y = stats.lognorm.pdf(x, fitting_params[0], fitting_params[1], fitting_params[2])\n",
    "\n",
    "fit_data = pd.DataFrame({'stem_length':x, 'prob_density':y})\n",
    "fit_data = fit_data[fit_data.stem_length < x_max]\n",
    "\n",
    "#simulated data\n",
    "sim_data = stats.lognorm.rvs(fitting_params[0], fitting_params[1], fitting_params[2], size=len(tree))\n",
    "sim_data = pd.DataFrame(pd.Series(sim_data))\n",
    "sim_data.columns = ['stem_length']\n",
    "\n",
    "\n",
    "#plot cumsum hist\n",
    "plot, data = plot_cumsum_counts(dist_data.stem_length, title='experimental')\n",
    "plot, fit = plot_cumsum_counts(sim_data.stem_length, title='simulated from fit')\n",
    "\n",
    "merged = pd.concat([data, fit])\n",
    "\n",
    "cum_plot, merged_data = plot_cumsum_counts(merged, plot_type='default', formatted_data=True,\n",
    "                                      x_min = 0, x_max=5, x_scale_type='linear', y_scale_type='linear')\n",
    "\n",
    "ks_test = stats.ks_2samp(dist_data.stem_length, fit_data.stem_length, alternative='two-sided', mode='auto')\n",
    "cum_plot.title = alt.TitleParams(f'p={ks_test.pvalue}, statistic={ks_test.statistic} ', fontSize=12)\n",
    "\n",
    "\n",
    "#data_hist with line fits\n",
    "title = alt.TitleParams(f\"{treefile.split('/')[-1]}, {len(tree)} leaves, 99% CDF {fitting_params[4]}\", fontSize=12)\n",
    "data_hist = alt.Chart(dist_data, title = title).mark_bar().encode(\n",
    "    x= alt.X('stem_length', bin=alt.Bin(extent=[0, x_max], step=0.02)),\n",
    "    y= alt.Y('count()')\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "fit_curve = alt.Chart(fit_data).mark_line(color='red').encode(\n",
    "    x= alt.X('stem_length', scale=alt.Scale(domain=[0, x_max])),\n",
    "    y= alt.Y('prob_density')\n",
    "\n",
    ")\n",
    "\n",
    "plot = alt.layer(data_hist, fit_curve).resolve_scale(y='independent')\n",
    "merge_plot = plot | cum_plot\n",
    "merge_plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbed182",
   "metadata": {},
   "outputs": [],
   "source": [
    "CDD_annot = pd.read_csv('processing/euk72_ep3/euk72_ep_hhm.CDD_annotation.tsv', sep='\\t', index_col=0)\n",
    "CDD_annot['score'] = CDD_annot.Prob*CDD_annot.Pairwise_cov\n",
    "CDD_annot.sort_values(by='score', inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc91cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# merge all tree_data.csv files and clade_data.csv\n",
    "\n",
    "treefiles = subprocess.run(f\"find processing/microcosm2/ -name '*.tree_data.tsv'\", shell=True, text=True, capture_output=True)\n",
    "treefiles = [file for file in treefiles.stdout.split('\\n')][:-1]\n",
    "\n",
    "print(len(treefiles))\n",
    "new_data = []\n",
    "\n",
    "for file in treefiles:\n",
    "    data = pd.read_csv(file, sep='\\t', index_col=0)\n",
    "    new_data.append(data)\n",
    "\n",
    "all_data = pd.concat(new_data, axis=0)\n",
    "all_data.to_csv('tmp/microcosm_tree_data2.tsv', sep='\\t')\n",
    "\n",
    "tree_data = pd.read_csv('tmp/microcosm_tree_data2.tsv', sep='\\t', index_col=0)\n",
    "tree_data.set_index('tree_name', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a6d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = iter(range(999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4312c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data[(tree_data.prok_taxa == 'Asgard') & (tree_data['c-ELW'] > 0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c014c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data.loc[tree_data.index.unique()[next(i)]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot overall branch length distribution of most likley sister taxa based on c-ELW\n",
    "\n",
    "tree_data = tree_data.sort_values(by='c-ELW', ascending=False)\n",
    "\n",
    "filtered_data = tree_data[(tree_data.stem_length.between(0.00, 2))] #& \n",
    "#                           (tree_data.prok_clade_weight >= 0.6)]# & \n",
    "                          #~(tree_data.euk_clade_rep.duplicated())]\n",
    "\n",
    "a = filtered_data#[~(filtered_data.euk_clade_rep.duplicated())]\n",
    "\n",
    "#a['dist'] = [np.log(stem) for stem in a['dist']]\n",
    "\n",
    "#taxa = a.prok_taxa.value_counts().index[0:20].values\n",
    "taxa = ['Cyanophyceae', 'Asgard', 'Alphaproteobacteria', 'Actinomycetes']\n",
    "#taxa = ['Aquificae']\n",
    "\n",
    "a = a[a.prok_taxa.isin(taxa)]\n",
    "\n",
    "title = f'Eukaryotic branch length per taxa, Sample_size:{a.shape[0]}'\n",
    "#title = f'{a.shape[0]}'\n",
    "#title = f'{len(a)} normalized stem lengths as per Gabaldon 2016'\n",
    "\n",
    "KDE = alt.Chart(a, title=alt.TitleParams(text=title, fontSize=12)).mark_area(line=True, opacity=0.2).transform_density(\n",
    "    'stem_length',\n",
    "    as_=['stem_length', 'density'],\n",
    "    bandwidth=0.1,\n",
    "    groupby = ['prok_taxa']\n",
    "    \n",
    "    ).encode(\n",
    "    x=alt.X('stem_length:Q', scale=alt.Scale(domain=[0,2], clamp=True, )),\n",
    "    y=alt.Y('density:Q', scale=alt.Scale(domain=[0,3], clamp=False)),\n",
    "    color=alt.Color('prok_taxa'),\n",
    "    tooltip = alt.Tooltip(['prok_taxa'])\n",
    "\n",
    ").interactive()\n",
    "\n",
    "bar = alt.Chart(a, title=title).transform_joinaggregate(\n",
    "    total='count(*)',\n",
    "    groupby=['prok_taxa']\n",
    "    ).transform_calculate(\n",
    "    pct='1 / datum.total'\n",
    "    ).mark_bar().encode(\n",
    "    x=alt.X('stem_length:Q', bin=alt.Bin(step=0.05), scale=alt.Scale(domain=[0,4], clamp=True)),\n",
    "    y=alt.Y('sum(pct):Q', scale=alt.Scale(domain=[0,0.4])),\n",
    "    color=alt.Color('prok_taxa'),\n",
    "    tooltip = alt.Tooltip(['prok_taxa'])\n",
    "\n",
    ").interactive()\n",
    "KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a7d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prefilter tree_data\n",
    "filtered_data = tree_data[(tree_data.stem_length.between(0, 2)) & \n",
    "                          (tree_data.euk_clade_weight >= 0.8) & \n",
    "                          (tree_data.prok_clade_weight >= 0.8) & \n",
    "                          ~(tree_data.index.duplicated())]\n",
    "\n",
    "data = {}\n",
    "\n",
    "#prefilter CDD data\n",
    "CDD_annot_filtered = CDD_annot.loc[filtered_data.index]\n",
    "\n",
    "#format all taxa specific closest sisters\n",
    "for taxa in sorted(filtered_data.prok_taxa.unique()):\n",
    "    print(taxa)\n",
    "    tree_data_taxa = filtered_data[filtered_data.prok_taxa == taxa]\n",
    "    \n",
    "    CDD_hits = CDD_annot_filtered.loc[tree_data_taxa.index]\n",
    "    \n",
    "    CDD_good = CDD_hits[CDD_hits['score']>70].sort_values(by='score', ascending=False)\n",
    "    CDD_good = CDD_good[~(CDD_good.index.duplicated())]\n",
    "    CDD_good['closest_sister_taxa'] = [taxa]*CDD_good.shape[0]\n",
    "    \n",
    "    data[taxa] = CDD_good.round(2)\n",
    "\n",
    "#write to file\n",
    "all_data = pd.concat(data.values())\n",
    "\n",
    "all_data.to_csv('tmp/microcosm_data_CDD.tsv', sep='\\t')\n",
    "    \n",
    "#write to multi-sheet excel\n",
    "xlsx_file = 'test2.xlsx'\n",
    "with pd.ExcelWriter(xlsx_file) as writer:\n",
    "    for taxa, df in data.items():\n",
    "        df.to_excel(writer, taxa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7e7729",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot clade analysis confidence distribution for tests \n",
    "\n",
    "from core_functions.altair_plots import plot_cumsum_counts\n",
    "tree_data = tree_data[tree_data['p-AU_accept'] != 'NONE']\n",
    "\n",
    "tests = ['c-ELW','bp-RELL', 'p-KH', 'p-SH', 'p-AU']\n",
    "all_data_dict = {}\n",
    "\n",
    "for test in tests:\n",
    "    \n",
    "    tree_data[test] = tree_data[test].astype(float)\n",
    "    \n",
    "    print(test)\n",
    "    tree_data = tree_data.sort_values(by=[test])\n",
    "\n",
    "    filtered_data = tree_data[(tree_data[test+'_accept'] == '+')]\n",
    "    best_data = filtered_data.groupby('euk_clade_rep').apply(lambda data: data[test].max())\n",
    "\n",
    "    \n",
    "    plot, data1 = plot_cumsum_counts(filtered_data[test], title='accepted '+test)\n",
    "    plot, data2 = plot_cumsum_counts(best_data, title='best '+test)\n",
    "    plot, data3 = plot_cumsum_counts(tree_data[test], title='all '+test)\n",
    "    \n",
    "    all_data = pd.concat([data3,data1,data2])\n",
    "    \n",
    "    plot, data = plot_cumsum_counts(all_data, formatted_data=True, plot_type='default', \n",
    "                                x_scale_type='linear', y_scale_type='linear', \n",
    "                               x_min=0, x_max=1, title='Confidence distributions '+test, x_label='probability')\n",
    "    \n",
    "    display(plot)\n",
    "    \n",
    "    all_data_dict['all_'+test] = data1\n",
    "    all_data_dict['best_'+test] = data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5209179",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2d histogram heatmap of test value vs topology distance\n",
    "# looks a bit garbage\n",
    "\n",
    "tests = ['c-ELW','bp-RELL', 'p-KH', 'p-SH', 'p-AU']\n",
    "\n",
    "dist_metric = 'dist'\n",
    "\n",
    "for test in tests:\n",
    "\n",
    "    plot_data = tree_data.sort_values(by=test + '_accept', ascending=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    hist = np.histogram2d(plot_data[test], plot_data[dist_metric], bins=50)\n",
    "    \n",
    "    data = pd.DataFrame(np.log2(hist[0]))\n",
    "    data.index = np.round(hist[1][1:],2)\n",
    "    data.columns = np.round(hist[2][1:],2)\n",
    "    data2 = data.stack()\n",
    "    data2.index.set_names([test, dist_metric], inplace=True)\n",
    "\n",
    "    data2 = data2.reset_index(name='count')\n",
    "\n",
    "    plot = alt.Chart(data2, ).mark_rect(filled=True, size=100).encode(\n",
    "        x = alt.X(dist_metric+':O',  axis=alt.Axis(grid=False, labelAngle=90)),\n",
    "        y = alt.Y(test+':O',  axis=alt.Axis(grid=False), scale=alt.Scale(reverse=True)),\n",
    "        color = alt.Color('count')\n",
    "    ).interactive().properties(width=700, height=700)\n",
    "\n",
    "    display(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed91cdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scatter of test values against distance metric\n",
    "\n",
    "tests = ['c-ELW','bp-RELL', 'p-KH', 'p-SH', 'p-AU']\n",
    "dist_metric = 'top_dist'\n",
    "\n",
    "for test in tests:\n",
    "    plot_data = tree_data.sort_values(by=test + '_accept', ascending=True)\n",
    "    \n",
    "    plot = alt.Chart(plot_data).mark_point(filled=True, size=100, opacity=0.05).encode(\n",
    "        x = alt.X(dist_metric),\n",
    "        y = alt.Y(test),\n",
    "        color = alt.Color(test + '_accept')\n",
    "    ).interactive()\n",
    "    \n",
    "    display(plot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b1b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa040bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data[(tree_data.prok_taxa == 'Cyanophyceae') & (tree_data['c-ELW'] > 0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c6057",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#system = tree_data[tree_data.stem_length < 0.1].index.unique()[28]\n",
    "#system = small_systems[12]\n",
    "system = 'EP00669P010048'\n",
    "#print(system)\n",
    "\n",
    "data = tree_data.loc[[system]].sort_values(by='c-ELW', ascending=False)\n",
    "    \n",
    "clades = data.euk_clade_rep.unique()\n",
    "print(f'System {system} has Eukaryotic clades {clades}')\n",
    "\n",
    "clade = clades[0]\n",
    "\n",
    "plot_data = data[data.euk_clade_rep == clade]\n",
    "\n",
    "dist_bar = alt.Chart(data, title=f'{system}: {clade}').mark_bar().encode(\n",
    "    x = alt.X('prok_taxa', sort=None, axis=alt.Axis(labelAngle=-45)),\n",
    "    y = alt.Y('top_dist'),\n",
    "    color = alt.Color('prok_taxa:O'),\n",
    "    tooltip = alt.Tooltip(['top_dist', 'prok_clade_weight', 'prok_clade_size', 'prok_clade_rep', 'prok_leaf_clade'])\n",
    ")\n",
    "\n",
    "tree = Tree(f'processing/microcosm2/{system}/{system}.merged.fasta.muscle.treefile.annot')\n",
    "annot_tree, tree_img = color_tree(tree, view_in_notebook=True)\n",
    "\n",
    "\n",
    "from core_functions.altair_plots import plot_alignment\n",
    "aln_file = f'processing/microcosm2/{system}/{system}.merged.fasta.muscle'\n",
    "\n",
    "#leaf_names = [leaf.name for leaf in trees[system].get_leaves()]\n",
    "leaf_names = [leaf.name for leaf in tree.get_leaves()]\n",
    "\n",
    "plot, aln_data = plot_alignment(aln_file, seqlimit=100, plot_range=(0,300), label_order=leaf_names)\n",
    "\n",
    "display(dist_bar)\n",
    "display(data.iloc[:,[0,5,10,11,12,16,17,18,19,20,21,22,23,24,25,26,27]])\n",
    "display(pd.DataFrame(CDD_annot.loc[system]))\n",
    "display(plot)\n",
    "display(tree_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf98f865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree = Tree(f'processing/microcosm2_old/EP00411P016539/EP00411P016539.merged.fasta.muscle.treefile')\n",
    "annot_tree, tree_img = color_tree(tree, view_in_notebook=True)\n",
    "display(tree_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965a31dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data.euk_clade_rep.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6d71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([tree_data.prok_taxa.value_counts()[0:15], tree_data.prok_taxa.value_counts(normalize=True)[0:15]]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c3d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = []\n",
    "for tree, data in tree_data[tree_data['c-ELW_accept'].isin(['+', 'NONE'])].sort_values(by='c-ELW', ascending=False).groupby('euk_clade_rep'):\n",
    "    best.extend(data['prok_taxa'].values)\n",
    "    \n",
    "pd.DataFrame([pd.Series(best).value_counts()[0:15],pd.Series(best).value_counts(normalize=True)[0:15]]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87adfe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = []\n",
    "for tree, data in tree_data[tree_data['c-ELW_accept'].isin(['+', 'NONE'])].sort_values(by='c-ELW', ascending=False).groupby('euk_clade_rep'):\n",
    "    best.append(data.iloc[0]['prok_taxa'])\n",
    "    \n",
    "pd.DataFrame([pd.Series(best).value_counts()[0:15],pd.Series(best).value_counts(normalize=True)[0:15]]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08688ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = []\n",
    "for tree, data in tree_data[(tree_data['c-ELW_accept'] == '+') &\n",
    "                            (tree_data['c-ELW'] > 0.8)].sort_values(by='c-ELW', ascending=False).groupby('euk_clade_rep'):\n",
    "    best.append(data.iloc[0]['prok_taxa'])\n",
    "    \n",
    "pd.DataFrame([pd.Series(best).value_counts()[0:15],pd.Series(best).value_counts(normalize=True)[0:15]]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7232eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data['rank'] = [None]*tree_data.shape[0]\n",
    "for clade, data in tree_data.groupby('euk_clade_rep'):\n",
    "    ranks = data['c-ELW'].rank(method='dense', pct=True)\n",
    "    tree_data.loc[tree_data.euk_clade_rep == clade, 'rank'] = ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f853a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data[tree_data.euk_clade_rep == 'EP00437P002328'].iloc[:,6:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da614f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tree_data.loc['EP00746P122355'].sort_values(by='c-ELW', ascending=False).iloc[:,4:25][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c3eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data[(tree_data['c-ELW_accept'] == '+') & \n",
    "          (tree_data.prok_taxa == 'Alphaproteobacteria') &\n",
    "          (tree_data['c-ELW'] > 0.5)].sort_values(by='c-ELW', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4947f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data.sort_values(by='c-ELW', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246784cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data[tree_data.euk_clade_rep == 'EP00741P020146'].sort_values(by='c-ELW', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad48969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core_functions.altair_plots import plot_alignment\n",
    "\n",
    "clusters = pd.read_csv('processing/prok2111_as/prok2111_as.repseq.cascaded_cluster.tsv', sep='\\t', names =['cluster_acc', 'acc'], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab3979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1cfd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = iter(clusters.loc[clusters.index.value_counts().between(50,100)].index.unique().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aln_file = f'processing/prok2111_as/cluster_fastas/{next(a)}'\n",
    "\n",
    "#leaf_names = [leaf.name for leaf in trees[system].get_leaves()]\n",
    "#leaf_names = [leaf.name for leaf in tree.get_leaves()]\n",
    "\n",
    "plot, aln_data = plot_alignment(aln_file, seqlimit=100, plot_range=(0,300))\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d54676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd962c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abed306",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELW = tree_data[tree_data['c-ELW_accept'] == '+'].groupby('prok_taxa').apply(lambda x: x['c-ELW'].describe())\n",
    "\n",
    "ELW['weight'] = ELW['count']*ELW['mean']\n",
    "\n",
    "ELW = ELW[['count', 'weight', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']]\n",
    "\n",
    "ELW[ELW['count'] > 50].sort_values(by='mean', ascending=False)[0:40]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab81df15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot overall branch length distribution of most likley sister taxa based on c-ELW\n",
    "\n",
    "tree_data = tree_data.sort_values(by='c-ELW', ascending=False)\n",
    "\n",
    "filtered_data = tree_data[tree_data['c-ELW_accept'] == '+']\n",
    "#[(tree_data.stem_length.between(0.00, 2))] #& \n",
    "#                           (tree_data.prok_clade_weight >= 0.6)]# & \n",
    "                          #~(tree_data.euk_clade_rep.duplicated())]\n",
    "\n",
    "a = filtered_data#[~(filtered_data.euk_clade_rep.duplicated())]\n",
    "\n",
    "#a['dist'] = [np.log(stem) for stem in a['dist']]\n",
    "\n",
    "#taxa = a.prok_taxa.value_counts().index[0:20].values\n",
    "taxa = ['Cyanophyceae', 'Asgard', 'Alphaproteobacteria', 'Actinomycetes', 'Gammaproteobacteria']\n",
    "#taxa = ['Aquificae']\n",
    "\n",
    "a = a[a.prok_taxa.isin(taxa)]\n",
    "\n",
    "title = f'c-ELW distribution per taxa, Sample_size:{a.shape[0]}'\n",
    "#title = f'{a.shape[0]}'\n",
    "#title = f'{len(a)} normalized stem lengths as per Gabaldon 2016'\n",
    "\n",
    "KDE = alt.Chart(a, title=alt.TitleParams(text=title, fontSize=12)).mark_area(line=True, opacity=0.2).transform_density(\n",
    "    'c-ELW',\n",
    "    as_=['c-ELW', 'density'],\n",
    "    bandwidth=0.05,\n",
    "    groupby = ['prok_taxa']\n",
    "    \n",
    "    ).encode(\n",
    "    x=alt.X('c-ELW:Q', scale=alt.Scale(domain=[0,1], clamp=True, )),\n",
    "    y=alt.Y('density:Q', scale=alt.Scale(domain=[0,3.5], clamp=False)),\n",
    "    color=alt.Color('prok_taxa'),\n",
    "    tooltip = alt.Tooltip(['prok_taxa'])\n",
    "\n",
    ").interactive()\n",
    "\n",
    "bar = alt.Chart(a, title=title).transform_joinaggregate(\n",
    "    total='count(*)',\n",
    "    groupby=['prok_taxa']\n",
    "    ).transform_calculate(\n",
    "    pct='1 / datum.total'\n",
    "    ).mark_bar().encode(\n",
    "    x=alt.X('stem_length:Q', bin=alt.Bin(step=0.05), scale=alt.Scale(domain=[0,4], clamp=True)),\n",
    "    y=alt.Y('sum(pct):Q', scale=alt.Scale(domain=[0,0.4])),\n",
    "    color=alt.Color('prok_taxa'),\n",
    "    tooltip = alt.Tooltip(['prok_taxa'])\n",
    "\n",
    ").interactive()\n",
    "KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea78823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python/3.9",
   "language": "python",
   "name": "py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
