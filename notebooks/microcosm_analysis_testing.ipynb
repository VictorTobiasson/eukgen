{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4842f96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vf/users/tobiassonva/data/eukgen\n"
     ]
    }
   ],
   "source": [
    "#multiprocessing and io\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "#numerics and vis\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#trees\n",
    "import ete3\n",
    "from ete3 import Tree, TreeStyle, TextFace\n",
    "\n",
    "\n",
    "root = '/data/tobiassonva/data/eukgen/'\n",
    "%cd {root}\n",
    "\n",
    "import sys\n",
    "sys.path.append(root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c89b9a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEQUENCE ALIGNMENTS OPERATIONS\n",
    "\n",
    "\n",
    "#helper IO functions for basic fasta reading into {id:seq} dict\n",
    "#id taken as string up until first space character\n",
    "def fasta_to_dict(fastastring=None, file=None):\n",
    "    \n",
    "    if file != None:\n",
    "        with open(file, 'r') as fastafile:\n",
    "            fastalines = fastafile.readlines()\n",
    "    \n",
    "    #trim everything after first space in line. Avoids pathologic cases of headers such as \"NR_XXXX (abc-->cd)\"\n",
    "    for n, line in enumerate(fastalines):\n",
    "        fastalines[n] = ''.join(line.split(' ')[0])\n",
    "    \n",
    "    fastastring = '\\n'.join(fastalines)\n",
    "    \n",
    "    entries = [entry.strip() for entry in ''.join(fastastring).split('>') if entry != '']\n",
    "    fastas = {entry.split('\\n')[0]: ''.join(entry.split('\\n')[1:]) for entry in entries}\n",
    "    \n",
    "    #replace unknown characters with A\n",
    "    blacklist = set('BJUXZ')\n",
    "    for key, seq in fastas.items():\n",
    "        \n",
    "        for char in set(seq):\n",
    "            if char in blacklist:\n",
    "                seq = seq.replace(char, 'A')\n",
    "                print(f'WARNING: {key} Replaced illegal {char} with A')\n",
    "                \n",
    "        fastas[key] = seq\n",
    "    return fastas\n",
    "\n",
    "#returns simple single line fasta from {id:seq} dict\n",
    "def dict_to_fasta(seq_dict, write_file=False):\n",
    "    \n",
    "    fasta_str =  '\\n'.join(f'>{key}\\n{value}' for key, value in seq_dict.items())\n",
    "    \n",
    "    if write_file != False:\n",
    "        with open(write_file, 'w') as outfile:\n",
    "            outfile.write(fasta_str)\n",
    "        print(f'Wrote {write_file}')\n",
    "            \n",
    "    return fasta_str\n",
    "\n",
    "#small helper for pkl parsing\n",
    "def load_pkl(pkl_file):\n",
    "    with open(pkl_file, 'rb') as infile:\n",
    "        item = pickle.load(infile)\n",
    "    return item\n",
    "\n",
    "def dump_pkl(item, pkl_file):\n",
    "    with open(pkl_file, 'wb') as outfile:\n",
    "        pickle.dump(item, outfile)\n",
    "    print(f'Pickled item as {pkl_file}')\n",
    "\n",
    "#define the entropy for a string given amino acids, protein=True or, DNA protein=False\n",
    "def column_entropy(string, protein=True, gaptoken='-'):\n",
    "    \n",
    "    size = len(string)\n",
    "    counts = [string.count(i) for i in set(string).difference({gaptoken})]\n",
    "    entropy = -sum([i/size*math.log2(i/size) for i in counts])\n",
    "    \n",
    "    if protein:\n",
    "        entropy_uniform = math.log2(20)\n",
    "    else:\n",
    "        entropy_uniform = 2\n",
    "        \n",
    "    gap_entropy = entropy_uniform*(string.count(gaptoken)/size)\n",
    "    information = entropy_uniform - entropy - gap_entropy\n",
    "    \n",
    "    return max(information,0)\n",
    "\n",
    "\n",
    "#columnwise cut based on criteria\n",
    "def filter_by_entropy(seq_dict, entropy_min, filter_accs=[], gaptoken='-'):\n",
    "        \n",
    "    #transpose seqs into columns\n",
    "    cols = [''.join(seq) for seq in list(zip(*seq_dict.values()))]\n",
    "    \n",
    "    #filter only by columns present in filter_accs keys\n",
    "    if filter_accs:\n",
    "        filter_dict = {key:value for key, value in seq_dict.items() if key in filter_accs}\n",
    "        filter_cols = [''.join(seq) for seq in list(zip(*filter_dict.values()))]\n",
    "    \n",
    "    else:\n",
    "        filter_cols = cols\n",
    "    \n",
    "    #include based on entropy threshold\n",
    "    filter_cols = [col for i, col in enumerate(cols) if column_entropy(filter_cols[i], gaptoken=gaptoken) > entropy_min]\n",
    "\n",
    "    #transpose back to alignment\n",
    "    filter_aln = [''.join(col) for col in list(zip(*filter_cols))]\n",
    "    \n",
    "    #return with original keys\n",
    "    return {key: value for key, value in zip(seq_dict.keys(), filter_aln)}\n",
    "\n",
    "#input helper to read tsv from file, merge singletons\n",
    "def read_cluster_tsv(cluster_file, split_large=False, max_size=500, batch_single=False, single_cutoff=1):   \n",
    "\n",
    "    #read TSV and group clusters based on first tsv column\n",
    "    with open(cluster_file, 'r') as infile:\n",
    "        clusters = {}\n",
    "\n",
    "        for l in infile.readlines():\n",
    "            cluster_acc, acc = l.strip().split('\\t')\n",
    "\n",
    "            if cluster_acc not in clusters.keys():\n",
    "                clusters[cluster_acc] = [acc]\n",
    "\n",
    "            else:\n",
    "                clusters[cluster_acc].append(acc)\n",
    "    \n",
    "    #merge all clusters smaller than cutoff into one\n",
    "    if batch_single:\n",
    "        filter_dict = {}\n",
    "        singles = []\n",
    "        for key, accs in clusters.items():  \n",
    "            #gather singletons\n",
    "            if len(accs) <= single_cutoff:\n",
    "                singles.extend(accs)\n",
    "            \n",
    "            #keep larger clusters\n",
    "            else:\n",
    "                filter_dict[key] = accs\n",
    "                \n",
    "        if singles:\n",
    "            filter_dict[singles[0]] = singles\n",
    "        \n",
    "        clusters = filter_dict\n",
    "    \n",
    "    #split clusters larger than x into smaller pieces\n",
    "    if split_large:\n",
    "        filter_dict = {}\n",
    "        for key, accs in clusters.items():  \n",
    "            if len(accs) > max_size:\n",
    "                #partition large clusters into batches of max_size\n",
    "                for split in range(0, len(accs), max_size):\n",
    "                    batch = accs[split:split + max_size]\n",
    "                    filter_dict[batch[0]] = batch\n",
    "\n",
    "            #keep smaller clusters\n",
    "            else:\n",
    "                filter_dict[key] = accs\n",
    "                \n",
    "        clusters = filter_dict         \n",
    "        \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27fbacb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TREE OPERATIONS\n",
    "\n",
    "#quick function for adding phylogenetic annotation to tree labels\n",
    "def dirty_phyla_add(tree, tax_mapping):\n",
    "    euk_entries = []\n",
    "    for leaf in tree.get_leaves():\n",
    "        try:\n",
    "            acc = leaf.name\n",
    "            entry = tax_mapping.loc[acc]\n",
    "            leaf.add_feature('tax_superkingdom', entry['superkingdom'])\n",
    "            \n",
    "            leaf.add_feature('tax_class', entry['class'])\n",
    "            \n",
    "            if leaf.tax_superkingdom == 'Eukaryota':\n",
    "                leaf.add_feature('tax_filter', 'Eukaryota')\n",
    "            else:\n",
    "                leaf.add_feature('tax_filter', entry['class'])\n",
    "            \n",
    "        except KeyError:\n",
    "            leaf.add_feature('superkingom', 'ERROR')\n",
    "            leaf.add_feature('class', 'ERROR')\n",
    "\n",
    "\n",
    "#return closest non self leaf\n",
    "def get_closest_leaf(leaf):\n",
    "    \n",
    "    near_leaves = [near_leaf for near_leaf in leaf.up.get_leaves() if near_leaf!=leaf]\n",
    "    distances = [leaf.get_distance(near_leaf) for near_leaf in near_leaves]\n",
    "    min_dist =min(distances)\n",
    "    closest_leaf = near_leaves[distances.index(min_dist)]\n",
    "    \n",
    "    return closest_leaf, min_dist\n",
    "    \n",
    "#iteratively merge closest leaf pair until less than N leaves\n",
    "def crop_leaves_to_size(tree, max_size):\n",
    "    current_size = len(tree.get_leaves())\n",
    "    \n",
    "    if max_size >= current_size:\n",
    "        print(f'Tree of length {current_size} smaller than {max_size}')\n",
    "        return tree    \n",
    "    \n",
    "    leaf_partners = {leaf.name:get_closest_leaf(leaf) for leaf in tree.get_leaves()}\n",
    "    leaf_partner_dist = {key:value[1] for key, value in leaf_partners.items()}\n",
    "    leaf_partners = {key:value[0].name for key, value in leaf_partners.items()}\n",
    "    \n",
    "    #print(leaf_partner_dist)\n",
    "    #print(leaf_partners)\n",
    "    \n",
    "    \n",
    "    #serial implementetaion, not ideal as it produced uneven pruning if terminal brach length are very even.\n",
    "    while max_size < current_size:\n",
    "        \n",
    "        min_distance = min(leaf_partner_dist.values())\n",
    "        min_leaf_A = list(leaf_partner_dist.keys())[list(leaf_partner_dist.values()).index(min_distance)]\n",
    "        \n",
    "        min_leaf_B = leaf_partners[min_leaf_A]\n",
    "        \n",
    "        #print(f'{current_size} checking {min_leaf_A}, deleting closest partner is {min_leaf_B} with distance {min_distance}')\n",
    "        \n",
    "        #delete closest leaf\n",
    "        try:\n",
    "            tree.get_leaves_by_name(min_leaf_B)[0].delete()\n",
    "        \n",
    "        #if current leaf A maps to a deleted leaf update closest leaf and delete\n",
    "        except IndexError:\n",
    "            #update the min_leaf with new closest pair\n",
    "            new_leaf = tree.get_leaves_by_name(min_leaf_A)[0]\n",
    "            closest_new_leaf = get_closest_leaf(new_leaf)\n",
    "\n",
    "            leaf_partner_dist[new_leaf.name] = closest_new_leaf[1]\n",
    "            leaf_partners[new_leaf.name] = closest_new_leaf[0].name\n",
    "            min_leaf_B = leaf_partners[min_leaf_A]\n",
    "            \n",
    "            tree.get_leaves_by_name(min_leaf_B)[0].delete()\n",
    "            \n",
    "        \n",
    "        #delete the removed partner from dictionaries\n",
    "        leaf_partner_dist.pop(min_leaf_B, None)\n",
    "        leaf_partners.pop(min_leaf_B, None)\n",
    "        \n",
    "        #update the min_leaf with new closest pair\n",
    "        new_leaf = tree.get_leaves_by_name(min_leaf_A)[0]\n",
    "        closest_new_leaf = get_closest_leaf(new_leaf)\n",
    "        \n",
    "        leaf_partner_dist[new_leaf.name] = closest_new_leaf[1]\n",
    "        leaf_partners[new_leaf.name] = closest_new_leaf[0].name\n",
    "        \n",
    "        current_size -= 1\n",
    "        \n",
    "        #print(leaf_partner_dist)\n",
    "        #print(leaf_partners)\n",
    "\n",
    "    \n",
    "    return tree\n",
    "\n",
    "\n",
    "#fit inverse gamma distribution to all internal node distances and \n",
    "#exclude those beyong threshold probability\n",
    "def get_outlier_nodes_by_invgamma(tree, p_low=0, p_high=0.99, only_leaves=False):\n",
    "    \n",
    "    if only_leaves:\n",
    "        node_dists = [(node, node.dist) for node in tree.get_leaves()]\n",
    "    \n",
    "    else:\n",
    "        node_dists = [(node, node.dist) for node in tree.traverse()]\n",
    "    \n",
    "    dist_series = pd.Series([i[1] for i in node_dists])\n",
    "    \n",
    "    dist_stats = dist_series.describe()\n",
    "    \n",
    "    fit_alpha, fit_loc, fit_beta=stats.invgamma.fit(dist_series.values)\n",
    "\n",
    "    cutoff_high = stats.invgamma.ppf(p_high, a=fit_alpha, loc=fit_loc, scale=fit_beta)\n",
    "    cutoff_low = stats.invgamma.ppf(p_low, a=fit_alpha, loc=fit_loc, scale=fit_beta)\n",
    "    \n",
    "    outlier_nodes = [node[0] for node in node_dists if node[1] < cutoff_low or node[1] > cutoff_high]\n",
    "    print(f'Identified {len(outlier_nodes)} outlier nodes outside interval {cutoff_low} > d > {cutoff_high}')\n",
    "    \n",
    "    return outlier_nodes\n",
    "\n",
    "#starting from one leaf with an attribute traverse upwards untill \n",
    "#all leaves from the ancestor is no loger monophyletic under the given attribute\n",
    "#repeat for all remaining leaves\n",
    "#if any clade would have the global root as ancestor rerooot and retry to avoid \n",
    "#false paraphyly created by tree data struture\n",
    "def get_paraphyletic_groups(tree, attribute='tax_superkingdom', attr_value='Eukaryota', current_root=False):\n",
    "    \n",
    "    #tree.set_outgroup(tree.get_farthest_leaf()[0])\n",
    "    \n",
    "    if current_root:\n",
    "        tree.set_outgroup(current_root)\n",
    "    else:\n",
    "        current_root = tree.get_tree_root()\n",
    "    \n",
    "    #get a list of all leaves with an attribute matching the match value provided\n",
    "    check_leaves = [leaf for leaf in tree.get_leaves() if getattr(leaf, attribute)==attr_value]\n",
    "    clade_nodes = []\n",
    "    \n",
    "    seed_node = check_leaves[0]    \n",
    "\n",
    "    while check_leaves:\n",
    "        #assume monophyly\n",
    "        mono = True\n",
    "        \n",
    "        #check for all parent leaves if attribute matches the value, if not its not monophyletic, break \n",
    "        for leaf in seed_node.up.get_leaves():\n",
    "            if getattr(leaf, attribute)!=attr_value:\n",
    "                mono = False\n",
    "                break\n",
    "        \n",
    "        #if monophyletic try higher node \n",
    "        if mono:\n",
    "            seed_node = seed_node.up\n",
    "        \n",
    "        #else retrun node and exclude all leaves from list of leaves to check\n",
    "        else:\n",
    "            clade_nodes.append(seed_node)\n",
    "            check_leaves = [leaf for leaf in check_leaves if leaf not in seed_node.get_leaves()]\n",
    "            if check_leaves:\n",
    "                seed_node = check_leaves[0] \n",
    "    \n",
    "    #if parent has no parent it is the root\n",
    "    if [node for node in clade_nodes if node.up.up == None]:\n",
    "        #print('A tree clade has rooted parent nodes, rerooting')\n",
    "        \n",
    "        #get the first non-clade daughter from current root \n",
    "        non_clade_daughter = [node for node in current_root.children if node not in clade_nodes][0] \n",
    "        \n",
    "        return get_paraphyletic_groups(tree, attribute=attribute, attr_value=attr_value, current_root=non_clade_daughter)\n",
    "\n",
    "    else:\n",
    "        return clade_nodes\n",
    "\n",
    "\n",
    "#return the entropy of decendant and non decendant leaf labels \n",
    "def get_entropy_for_partition(tree, node, attribute='tax_filter', attr_value='Eukaryota'):\n",
    "    \n",
    "    all_labels = [getattr(leaf, attribute) for leaf in tree.get_leaves()]\n",
    "    all_label_count = all_labels.count(attr_value)\n",
    "    tree_width = len(all_labels)\n",
    "    \n",
    "#     base_label_Px = (all_label_count/tree_width)\n",
    "#     base_label_H = base_label_Px*np.log2(1/base_label_Px)\n",
    "    \n",
    "    clade_labels = [getattr(leaf, attribute) for leaf in node.get_leaves()]\n",
    "    clade_label_count = clade_labels.count(attr_value)\n",
    "    clade_width = len(clade_labels)\n",
    "    \n",
    "    #calculate label entropy\n",
    "    #print('AAA', clade_labels, clade_label_count, clade_width)\n",
    "    label_Px = (clade_label_count)/(clade_width)\n",
    "    label_H = label_Px*np.log2(1/label_Px)\n",
    "    \n",
    "    #calculate external entropy change\n",
    "    \n",
    "    # if all labels in the clade the external entropy is 0\n",
    "    if all_label_count-clade_label_count == 0:\n",
    "        external_label_H = 0\n",
    "\n",
    "    else:\n",
    "        #calculate external entropy change\n",
    "        external_label_Px = (all_label_count-clade_label_count)/(tree_width-clade_width)\n",
    "        external_label_H = external_label_Px*np.log2(1/external_label_Px)\n",
    "\n",
    "\n",
    "    return label_H, external_label_H\n",
    "    \n",
    "#assign soft LCA node based on minimizing entropy between given label outside and inside clade\n",
    "#more pessimissive than voting ratio, qualitatively underestimates\n",
    "def get_soft_LCA_by_relative_entropy(tree, attribute='tax_superkingdom', attr_value='Eukaryota', save_loss=False):\n",
    "    #count vote ratio for each node for one taxa\n",
    "    tree_width = len(tree.get_leaves())\n",
    "    root = tree.get_tree_root()\n",
    "    \n",
    "    lowest_total_H = float(\"inf\")\n",
    "    best_node = root\n",
    "    vote_label = attr_value\n",
    "    \n",
    "    all_labels = [getattr(leaf, attribute) for leaf in tree.get_leaves()]\n",
    "    all_label_count = all_labels.count(attr_value)\n",
    "    tree_width = len(all_labels)\n",
    "    \n",
    "    all_label_Px = (all_label_count/tree_width)\n",
    "    all_labels_H = all_label_Px*np.log2(1/all_label_Px)\n",
    "    \n",
    "    #for debugging\n",
    "    if save_loss:\n",
    "        for node in tree.traverse():\n",
    "            node.add_feature('vote_loss', 'None')\n",
    "    \n",
    "    #check for monophyly\n",
    "    LCA_groups = get_paraphyletic_groups(tree, attribute=attribute, attr_value=attr_value)\n",
    "    if len(LCA_groups) == 1:\n",
    "        print(f'The attribute {attribute} is monophyletic for {attr_value}. Returning LCA node.')\n",
    "        return (LCA_groups[0], 0)\n",
    "\n",
    "\n",
    "    #the best partition will be on the path from an LCA node to the root\n",
    "    tested_nodes = []\n",
    "    for node in LCA_groups:\n",
    "        \n",
    "        node_label_count = len([leaf for leaf in node.get_leaves() if getattr(leaf, attribute) == attr_value])\n",
    "\n",
    "        #ascend until all labeled leaves are decendants of node \n",
    "        while node != root:\n",
    "           \n",
    "            #skip known nodes\n",
    "            if node not in tested_nodes:\n",
    "                \n",
    "                #calculate internal and external entropy\n",
    "                label_H, external_label_H =  get_entropy_for_partition(tree, node, attribute=attribute, attr_value=attr_value)\n",
    "                total_H = label_H + external_label_H\n",
    "                    \n",
    "                #penalize leaf LCAs to avoid laddered LCAs when having repeated outgroups\n",
    "                #not neccesarily waned as spread singleons get their global LCA as soft_LCA\n",
    "                if node.is_leaf():\n",
    "                    total_H += 0.5\n",
    "                \n",
    "                #update best guess\n",
    "                if total_H < lowest_total_H:\n",
    "                    lowest_total_H = total_H\n",
    "                    best_node = node\n",
    "                \n",
    "                if save_loss:\n",
    "                    node.add_feature('vote_loss', total_H)\n",
    "            \n",
    "            #break after calculations if all nodes are decendants\n",
    "            node_label_count = len([leaf for leaf in node.get_leaves() if getattr(leaf, attribute) == attr_value])\n",
    "            if node_label_count == all_label_count:\n",
    "                break\n",
    "            #print(node_label_count, all_label_count)\n",
    "            \n",
    "            #ascend\n",
    "            tested_nodes.append(node)\n",
    "            node = node.up\n",
    "            \n",
    "            \n",
    "    #print(f'Best node for {attr_value} has a total H of {lowest_total_H}')\n",
    "    return (best_node, lowest_total_H)\n",
    "\n",
    "def get_multiple_soft_LCAs(tree, attribute='tax_filter', attr_value='Eukaryota', min_size=1, min_purity=0, max_entropy=9999):\n",
    "    \n",
    "    print(f'Searching for LCA_nodes by checking where {attribute} is {attr_value}')\n",
    "        \n",
    "    soft_LCA_nodes = []\n",
    "    total_nodes = len([getattr(node, attribute) for node in tree.get_leaves() if getattr(node, attribute) == attr_value])    \n",
    "    \n",
    "    while total_nodes > 0:\n",
    "        \n",
    "        soft_LCA_node, lowest_H_loss = get_soft_LCA_by_relative_entropy(tree, attribute=attribute, attr_value=attr_value)\n",
    "        \n",
    "\n",
    "        soft_LCA_node_leaves = soft_LCA_node.get_leaves()\n",
    "        soft_LCA_node_size = len(soft_LCA_node_leaves)\n",
    "        \n",
    "        #mark labeled included nodes and decrement total nodes left to check\n",
    "        for node in soft_LCA_node_leaves:\n",
    "            if getattr(node, attribute) == attr_value:\n",
    "                setattr(node, attribute, 'SAMPLED')\n",
    "                total_nodes -= 1\n",
    "        \n",
    "            \n",
    "        soft_LCA_nodes.append(soft_LCA_node)\n",
    "        \n",
    "    #reset leaf node attributes\n",
    "    for node in tree.get_leaves():\n",
    "        if getattr(node, attribute) == 'SAMPLED':\n",
    "            setattr(node, attribute, attr_value)        \n",
    "    \n",
    "    #recalculate entropies\n",
    "    print(f'\\tRecalculating sizes, purities and entropies for all LCA nodes')\n",
    "    filtered_soft_LCA_nodes = []\n",
    "    \n",
    "    for i, node in enumerate(soft_LCA_nodes):\n",
    "        label_H, external_label_H = get_entropy_for_partition(tree, node, attribute=attribute, attr_value=attr_value)\n",
    "        \n",
    "        soft_LCA_members = [getattr(leaf, attribute) for leaf in node.get_leaves()]\n",
    "        soft_LCA_size = len(soft_LCA_members)\n",
    "        soft_LCA_purity = soft_LCA_members.count(attr_value) / soft_LCA_size\n",
    "        soft_LCA_entropy = label_H + external_label_H\n",
    "        \n",
    "        if soft_LCA_size >= min_size and soft_LCA_purity >= min_purity and soft_LCA_entropy <= max_entropy:\n",
    "            print(f'\\tFound node of size {soft_LCA_size} with purity of {soft_LCA_purity} and entropy {soft_LCA_entropy} as LCA for {attr_value}')\n",
    "            filtered_soft_LCA_nodes.append([node, soft_LCA_size, soft_LCA_purity, soft_LCA_entropy])\n",
    "        else:\n",
    "            print(f'\\tRejected node of size {soft_LCA_size} with purity of {soft_LCA_purity} and entropy {soft_LCA_entropy} as LCA for {attr_value}')\n",
    "    \n",
    "    if len(filtered_soft_LCA_nodes) == 0:\n",
    "        print(f'WARNING: No valid LCA nodes present for {attribute} = {attr_value} under conditions min_size={min_size}, min_purity={min_purity}, max_entropy={max_entropy}') \n",
    "    \n",
    "    print()\n",
    "    \n",
    "    return filtered_soft_LCA_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "208ac758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrapper function for target reclustering which required a temporary accession file to be written\n",
    "def mmseqs_run_target(target, members, root, seq_DB, threads=1):\n",
    "    \n",
    "    #boilerplate for informative print()\n",
    "    thread = multiprocessing.current_process().pid\n",
    "    threadID_string = f'{thread} | {target}:'\n",
    "    print(f'{threadID_string} Preparing mmseqs data for target {target}\\n', end='')\n",
    "    \n",
    "    basename = f'{root+target}'\n",
    "    \n",
    "    #write all accessstions to temporary file\n",
    "    with open(basename, 'w') as outfile:\n",
    "        outfile.writelines([acc+'\\n' for acc in members])\n",
    "    \n",
    "    #create a temporary seqDB, extract fastas, cluster and calculate cluster.tsv file\n",
    "    os.system(f'mmseqs createsubdb -v 0 --id-mode 1 {basename} {seq_DB} {basename}.DB')\n",
    "    os.system(f'mmseqs convert2fasta -v 0 {basename}.DB {basename}.fasta')\n",
    "    os.system(f'mmseqs cluster -v 0 --remove-tmp-files 1 --threads {threads} -s 7.5 {basename}.DB {basename}.cluster {root}tmp/{target}')\n",
    "    os.system(f'mmseqs createtsv -v 0 {seq_DB} {seq_DB} {basename}.cluster {basename}.cluster.tsv')\n",
    "    \n",
    "    #clean \n",
    "    os.system(f'mmseqs rmdb {basename}.DB -v 0')\n",
    "    os.system(f'mmseqs rmdb {basename}.cluster -v 0')\n",
    "    os.system(f'rm {basename}')\n",
    "    \n",
    "    return\n",
    "\n",
    "#main script for formatting query and target .fasta and .cluster.tsv files\n",
    "#should be updated to subprocess.run() as well as matching style of latter processing\"\n",
    "def microcosm_prepare_mmseqs(query, query_DB, target_DB, root):\n",
    "    \n",
    "    #configure paths and flags\n",
    "    thread = multiprocessing.current_process().pid\n",
    "    query_root = root+query+'/'\n",
    "    basename = query_root+query\n",
    "    threadID_string = f'{thread} | {query}:'\n",
    "    os.system(f'mkdir {query_root}/tmp')\n",
    "    \n",
    "    #parse target clusters\n",
    "    target_clusters = read_cluster_tsv(f'{basename}.targets')  \n",
    "\n",
    "    print(f'{threadID_string} Started \\n', end='')\n",
    "    print(f'{threadID_string} Preparing mmseqs data for query\\n', end='')\n",
    "\n",
    "    #createa new DB for the query sequences and cluster it\n",
    "    os.system(f\"mmseqs createsubdb -v 0 --id-mode 1 --subdb-mode 1 {basename}.acc {query_DB} {basename}.DB\")\n",
    "    os.system(f'mmseqs convert2fasta -v 0 {basename}.DB {basename}.fasta')\n",
    "    \n",
    "    print(f'{threadID_string} Preparing mmseqs data for merged target hits\\n', end='')\n",
    "\n",
    "    #create a subDB and extract sequences, create a new seqDB in order to recreate the header lookup\n",
    "    #otherwise each query using header info searches the entire original header lookup\n",
    "    with open(f'{basename}.members', 'w') as outfile:\n",
    "        for members in target_clusters.values():\n",
    "            outfile.writelines([member+'\\n' for member in members])\n",
    "    \n",
    "    os.system(f\"mmseqs createsubdb -v 0 --id-mode 1 --subdb-mode 1 {basename}.members {target_DB} {basename}.members.DB\")\n",
    "    os.system(f\"mmseqs convert2fasta -v 0 {basename}.members.DB {basename}.members.fasta\")\n",
    "    \n",
    "    #clean\n",
    "    os.system(f'mmseqs rmdb -v 0 {basename}.DB')\n",
    "    os.system(f'mmseqs rmdb -v 0 {basename}.cluster')\n",
    "    os.system(f'mmseqs rmdb -v 0 {basename}.members.DB')\n",
    "    os.system(f'find {query_root} -maxdepth 1 -type l -exec unlink {{}} \\;')\n",
    "    os.system(f'rm -r *members* {query_root}/tmp')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ee6137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run command for generating a diversified ensemble with muscle and then extracting the maxcc aln\n",
    "def microcosm_muscle_ensamble(base_fasta, threads, muscle_reps, muscle5_exe, super5=False):\n",
    "    thread = multiprocessing.current_process().pid\n",
    "    threadID_string = f'{thread} | {base_fasta}:'\n",
    "    \n",
    "    os.environ['OMP_NUM_THREADS'] = str(threads)\n",
    "    \n",
    "    logfile = open(f'{base_fasta}.muscle.log', 'a')\n",
    "    \n",
    "    if super5:\n",
    "        for rep in range(muscle_reps):\n",
    "            print(f'aligning {rep}')\n",
    "            muscle5_align_ete_params = f\" -super5 {base_fasta} -output {base_fasta}.@.super5-tmp -perm all -perturb {rep} -threads {threads}\"\n",
    "            muscle5_ete_command = muscle5_exe+muscle5_align_ete_params\n",
    "            subprocess.run(muscle5_ete_command.split(), stdout=logfile, stderr=logfile)\n",
    "\n",
    "        root = '/'.join(base_fasta.split('/')[:-1])+'/'\n",
    "\n",
    "        efa_files = [file for file in os.listdir(root) if file.endswith('super5-tmp')]\n",
    "\n",
    "        with open(base_fasta+'.muscle-efa', 'w') as efa_merge:\n",
    "            for file in efa_files:\n",
    "                with open(root+file, 'r') as efa_in:\n",
    "                    efa_merge.write(f'<{file}\\n')\n",
    "                    efa_merge.write(efa_in.read())\n",
    "                subprocess.run(f'rm {root+file}'.split())\n",
    "\n",
    "\n",
    "        #extract maximum CC alignment\n",
    "        muscle5_maxcc_params = f' -maxcc {base_fasta}.muscle-efa -output {base_fasta}.muscle'\n",
    "        muscle5_command = muscle5_exe+muscle5_maxcc_params\n",
    "        subprocess.run(muscle5_command.split(), stdout=logfile, stderr=logfile)\n",
    "\n",
    "        logfile.close()\n",
    "        \n",
    "        return f'{base_fasta}.muscle'\n",
    "\n",
    "    else:\n",
    "        #run diversified ensemble\n",
    "        muscle5_align_ete_params = f\" -threads {threads} -diversified -replicates {muscle_reps} -align {base_fasta} -output {base_fasta}.muscle-efa\"\n",
    "        muscle5_ete_command = muscle5_exe+muscle5_align_ete_params\n",
    "        subprocess.run(muscle5_ete_command.split(), stdout=logfile, stderr=logfile)\n",
    "\n",
    "        #extract maximum CC alignment\n",
    "        muscle5_maxcc_params = f' -maxcc {base_fasta}.muscle-efa -output {base_fasta}.muscle'\n",
    "        muscle5_command = muscle5_exe+muscle5_maxcc_params\n",
    "        subprocess.run(muscle5_command.split(), stdout=logfile, stderr=logfile)\n",
    "        \n",
    "        logfile.close()\n",
    "        \n",
    "        return f'{base_fasta}.muscle'\n",
    "\n",
    "\n",
    "\n",
    "#read fasta, align with FAMSA, filter and construct FastTree,\n",
    "#crop eaves to size and write new fasta with only cropeed tree leaf sequences\n",
    "def microcosm_reduce_size(base_fasta, threads, max_leaf_size, filter_entropy, famsa_exe, fasttree_exe):\n",
    "    thread = multiprocessing.current_process().pid\n",
    "    threadID_string = f'{thread} | {base_fasta}:'\n",
    "\n",
    "    #threads for FastTree\n",
    "    os.environ['OMP_NUM_THREADS'] = str(threads)\n",
    "    \n",
    "    famsa_logfile = open(f'{base_fasta}.famsa.log', 'a')\n",
    "    fasttree_logfile = open(f'{base_fasta}.famsa.log', 'a')\n",
    "    \n",
    "    #align seqs\n",
    "    print(threadID_string+' Aligning FAMSA')\n",
    "\n",
    "    famsa_command = famsa_exe+f' -t {threads} {base_fasta} {base_fasta}.famsa'\n",
    "    subprocess.run(famsa_command.split(), stdout=famsa_logfile, stderr=famsa_logfile)\n",
    "\n",
    "    #backup original alignment\n",
    "    subprocess.run(f'cp {base_fasta}.famsa {base_fasta}.famsa.b'.split())\n",
    "\n",
    "    #filter euk by entropy\n",
    "    print(threadID_string+' Filtering by entropy')\n",
    "\n",
    "    aln = fasta_to_dict(file=f'{base_fasta}.famsa')\n",
    "    aln_filter = filter_by_entropy(aln, filter_entropy)\n",
    "    dict_to_fasta(aln_filter, write_file=f'{base_fasta}.famsa')\n",
    "\n",
    "\n",
    "    #construct a FastTree\n",
    "    print(threadID_string+' Constructing FastTree')\n",
    "\n",
    "    fasttree_command = fasttree_exe+f\" -gamma -out {base_fasta}.fasttree {base_fasta}.famsa\"\n",
    "    subprocess.run(fasttree_command.split(), stdout=fasttree_logfile , stderr=fasttree_logfile )\n",
    "\n",
    "\n",
    "    #reduce leaves to size\n",
    "    print(threadID_string+' Cropping Leaves')\n",
    "\n",
    "    tree = Tree(f'{base_fasta}.fasttree')\n",
    "    reduce_leaves_to_size(tree, max_leaf_size)\n",
    "\n",
    "    #write cropped fasta\n",
    "    cropped_leaves = tree.get_leaf_names()\n",
    "    cropped_aln = {key:value.replace('-', '') for key, value in aln.items() if key in cropped_leaves}\n",
    "\n",
    "    #replace original fasta with cropped version and save backup\n",
    "\n",
    "    subprocess.run(f'cp {base_fasta} {base_fasta}.uncropped'.split())\n",
    "    dict_to_fasta(cropped_aln, write_file=base_fasta)\n",
    "\n",
    "    famsa_logfile.close()\n",
    "    fasttree_logfile.close()\n",
    "    \n",
    "    return base_fasta\n",
    "\n",
    "\n",
    "def microcosm_realign_and_filter(query, root, threads=1, max_euk_leaf_size=200, max_prok_leaf_size=1300,\n",
    "                                filter_entropy=0.5, muscle_reps_euk=25, muscle_reps_prok=5):\n",
    "    \n",
    "    #configure paths and flags\n",
    "    thread = multiprocessing.current_process().pid\n",
    "    threadID_string = f'{thread} | {query}:'\n",
    "    \n",
    "    query_root = root+query+'/'\n",
    "    euk_fasta = query_root+query+'.fasta'\n",
    "    prok_fasta = query_root+query+'.members.fasta'\n",
    "    \n",
    "    famsa_exe = '/data/tobiassonva/data/software/FAMSA-2.0.1/famsa'\n",
    "    fasttree_exe = 'FastTree'\n",
    "    muscle5_exe = 'muscle'\n",
    "\n",
    "\n",
    "    \n",
    "    #for euk\n",
    "    euk_seqs = fasta_to_dict(file=euk_fasta)\n",
    "    euk_size = len(euk_seqs.keys())\n",
    "    \n",
    "    #if there are too many eukaryotic sequences, crop to size\n",
    "    if euk_size > max_euk_leaf_size:\n",
    "        print(threadID_string+f' There are more than {max_euk_leaf_size} sequences in {euk_fasta} ({euk_size}), will crop to size')    \n",
    "        euk_fasta = microcosm_reduce_size(euk_fasta, threads, max_euk_leaf_size, filter_entropy, famsa_exe, fasttree_exe)\n",
    "\n",
    "    else: \n",
    "        print(threadID_string+f' There are less than {max_euk_leaf_size} sequences in {euk_fasta} ({euk_size}), no cropping needed')    \n",
    "        \n",
    "    #align using muscle\n",
    "    print(threadID_string+f' Aligning with muscle5 as ensemble with {muscle_reps_euk} replicates')\n",
    "    euk_muscle = microcosm_muscle_ensamble(euk_fasta, threads, muscle_reps_euk, muscle5_exe, super5=False)\n",
    "    \n",
    "    \n",
    "    #for prok\n",
    "    prok_seqs = fasta_to_dict(file=prok_fasta)\n",
    "    prok_size = len(prok_seqs.keys())\n",
    "    \n",
    "    #if there are too many prokaryotic sequences, crop to size\n",
    "    if prok_size > max_prok_leaf_size:\n",
    "        print(threadID_string+f' There are more than {max_prok_leaf_size} sequences in {prok_fasta}({prok_size}), will crop to size')    \n",
    "        prok_fasta = microcosm_reduce_size(prok_fasta, threads, max_prok_leaf_size, filter_entropy, famsa_exe, fasttree_exe)\n",
    "\n",
    "    else: \n",
    "        print(threadID_string+f' There are less than {max_prok_leaf_size} sequences in {prok_fasta}({prok_size}), no cropping needed')    \n",
    "    \n",
    "    #align using muscle\n",
    "    print(threadID_string+f' Aligning with muscle5 as ensemble with {muscle_reps_prok} replicates')\n",
    "    prok_muscle = microcosm_muscle_ensamble(prok_fasta, threads, muscle_reps_prok, muscle5_exe, super5=False)\n",
    "    \n",
    "    \n",
    "    #filter both cropped alignments by columnwise bitscore\n",
    "    print(threadID_string+f' Filtering alignments to columnwise bitscore > {filter_entropy}')\n",
    "\n",
    "    #backup original alignments\n",
    "    subprocess.run(f'cp {euk_fasta}.muscle {euk_fasta}.muscle.b'.split())\n",
    "    subprocess.run(f'cp {prok_fasta}.muscle {prok_fasta}.muscle.b'.split())    \n",
    "    \n",
    "    aln = fasta_to_dict(file=f'{euk_fasta}.muscle')\n",
    "    aln_filter = filter_by_entropy(aln, filter_entropy)\n",
    "    dict_to_fasta(aln_filter, write_file=f'{euk_fasta}.muscle')\n",
    "    \n",
    "    aln = fasta_to_dict(file=f'{prok_fasta}.muscle')\n",
    "    aln_filter = filter_by_entropy(aln, filter_entropy)\n",
    "    dict_to_fasta(aln_filter, write_file=f'{prok_fasta}.muscle')\n",
    "    \n",
    "    return f'{euk_fasta}.muscle'\n",
    "\n",
    "\n",
    "def microcosm_merge_align_tree(query, root, threads=1, filter_entropy=0.5, muscle_reps=25):\n",
    "    \n",
    "    #configure paths and flags\n",
    "    thread = multiprocessing.current_process().pid\n",
    "    threadID_string = f'{thread} | {query}:'\n",
    "    \n",
    "    query_root = root+query+'/'\n",
    "    euk_muscle = query_root+query+'.fasta.muscle'\n",
    "    prok_muscle = query_root+query+'.members.fasta.muscle'\n",
    "    \n",
    "    merged_fasta = query_root+query+'.merged.fasta'\n",
    "    \n",
    "    muscle5_exe = 'muscle'   \n",
    "    iqtree_exe = 'iqtree2'\n",
    "    \n",
    "    evo_model_params = '-m MFP -mset LG,Q.pfam --cmin 4 --cmax 12'\n",
    "    \n",
    "    iqtree_logfile = open(f'{query}.iqtree.log', 'a')\n",
    "\n",
    "    #filter both cropped alignments by columnwise bitscore\n",
    "    print(threadID_string+f' Merging alignments {euk_muscle} and {prok_muscle} to .merged.fasta') \n",
    "    \n",
    "    #merge aligned and filtered pro and euk\n",
    "    with open(merged_fasta, 'w') as merged:\n",
    "        subprocess.run(f'cat {euk_muscle} {prok_muscle}'.split(), stdout=merged)\n",
    "    \n",
    "    #realign merged using muscle\n",
    "    print(threadID_string+f'  Realigning with muscle5 as ensemble with {muscle_reps} replicates')\n",
    "    merged_muscle = microcosm_muscle_ensamble(merged_fasta, threads, muscle_reps, muscle5_exe, super5=False)\n",
    "\n",
    "    \n",
    "    #filter alignments by columnwise bitscore\n",
    "    print(threadID_string+f' Filtering alignment to columnwise bitscore > {filter_entropy}')\n",
    "\n",
    "    #backup original alignments\n",
    "    subprocess.run(f'cp {merged_muscle} {merged_muscle}.b'.split())\n",
    "    \n",
    "    aln = fasta_to_dict(file=f'{merged_muscle}')\n",
    "    aln_filter = filter_by_entropy(aln, filter_entropy)\n",
    "    dict_to_fasta(aln_filter, write_file=f'{merged_muscle}')\n",
    "    \n",
    "    \n",
    "    #construct IQtree\n",
    "    print(threadID_string+f' Constructing IQTree for {merged_muscle}')\n",
    "    iqtree_command = f'{iqtree_exe} -s {merged_muscle} {evo_model_params} --threads {threads} -B 1000'\n",
    "    subprocess.run(iqtree_command.split(), stdout=iqtree_logfile, stderr=iqtree_logfile)\n",
    "    \n",
    "    iqtree_logfile.close()\n",
    "    \n",
    "    return\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "478bc362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def microcosm_tree_analysis(query, root, max_tree_leaves=1500, \n",
    "                            outlier_inv_gamma_low = 0,\n",
    "                            outlier_inv_gamma_high = 0.999,\n",
    "                            prok_min_size = 2, prok_min_purity = 0.5, euk_min_size = 3, euk_min_purity = 0.76):\n",
    "    \n",
    "    #configure paths and flags\n",
    "    thread = multiprocessing.current_process().pid\n",
    "    threadID_string = f'{thread} | {query}:'\n",
    "    \n",
    "    query_root = root+query+'/'\n",
    "    merged_tree = query_root+query+'.merged.fasta.muscle.contree'\n",
    "    \n",
    "    \n",
    "    #filter alignments by columnwise bitscore\n",
    "    print(threadID_string+f' Reading consensus tree from {merged_tree}')\n",
    "    \n",
    "    #load parsed taxonomy data\n",
    "    #standardize and reformat tax data\n",
    "    prok_tax = load_pkl('analysis/core_data/prok2111_protein_taxonomy_trimmed.pkl')\n",
    "    euk_tax = load_pkl('analysis/core_data/euk72_protein_taxonomy.pkl')\n",
    "    euk_tax.drop(['orgid', 'species'], axis=1, inplace=True)\n",
    "    tax_merge = pd.concat([euk_tax, prok_tax])\n",
    "    euk_header = load_pkl('analysis/core_data/euk72_header_mapping.pkl')\n",
    "    \n",
    "    #initialize tree\n",
    "    tree = Tree(merged_tree)\n",
    "    tree_name = merged_tree.split('/')[1]\n",
    "    tree_header = euk_header[euk_header.acc==tree_name].header.values\n",
    "    \n",
    "    dirty_phyla_add(tree, tax_merge)\n",
    "    \n",
    "    #merge leaf pairs until total amount of leaves is smaller than x\n",
    "    tree = reduce_leaves_to_size(tree, max_tree_leaves)\n",
    "    \n",
    "    #calculate devaiting branch distances\n",
    "    print(threadID_string+f' Identifying outlier nodes by branch inverse gamma distribution')\n",
    "    outlier_nodes =  get_outlier_nodes_by_invgamma(tree, p_low=outlier_inv_gamma_low, p_high=outlier_inv_gamma_high, only_leaves=False)\n",
    "    #cut_nodes = [node.detach() for node in outlier_nodes]\n",
    "    \n",
    "    \n",
    "    #calculate soft LCA nodes for prok and euk using partition entropy \n",
    "    print(threadID_string+f' Evaluationg soft LCAs from {merged_tree}')\n",
    "    \n",
    "    filter_taxa = set([leaf.tax_filter for leaf in tree.get_leaves()])\n",
    "    hard_LCA_dict = {}\n",
    "    soft_LCA_dict = {}\n",
    "\n",
    "    for tax in filter_taxa:\n",
    "        hard_LCA_dict[tax] = get_paraphyletic_groups(tree, attribute='tax_filter', attr_value=tax)\n",
    "        soft_LCA_dict[tax] = get_multiple_soft_LCAs(tree, attribute='tax_filter', attr_value=tax,\n",
    "                                                   min_size=prok_min_size, min_purity=prok_min_purity)\n",
    "    \n",
    "    valid_prok_LCAs = [key for key, value in soft_LCA_dict.items() if value != []]\n",
    "\n",
    "    soft_LCA_dict['Eukaryota'] = get_multiple_soft_LCAs(tree, attribute='tax_filter', attr_value='Eukaryota',\n",
    "                                                        min_size=euk_min_size, min_purity=euk_min_purity)\n",
    "\n",
    "    print(threadID_string+f' Found valid soft LCAs for a total of {len(valid_prok_LCAs)} out of a possible {len(filter_taxa)-1} taxons')\n",
    "    print(threadID_string+f' Found {len(soft_LCA_dict[\"Eukaryota\"])} valid LCAs for Eukaryota')\n",
    "    \n",
    "    #add LCA nodes to list for NODE visualisation\n",
    "    hard_LCA_nodes = [node for LCA_nodes in hard_LCA_dict.values() for node in LCA_nodes]\n",
    "    #all first prok LCAs\n",
    "    soft_LCA_nodes = [node[0][0] for node in soft_LCA_dict.values() if node != []]\n",
    "    #add all euk LCAs\n",
    "    soft_LCA_nodes = [node[0] for node in soft_LCA_dict['Eukaryota'] if node != []]\n",
    "\n",
    "    #add LCA labels for LABEL visualisation\n",
    "    for node in tree.traverse():\n",
    "        node.add_feature('soft_LCA', '')\n",
    "        node.add_feature('soft_LCA_H', '')\n",
    "\n",
    "    #add the first valid soft_LCA as LCA for prok and all valid soft_LCAs for euk\n",
    "    for tax, nodes in soft_LCA_dict.items():\n",
    "        if nodes != []:\n",
    "            #add all euk nodes\n",
    "            if tax == 'Eukaryota':\n",
    "                for node in nodes:\n",
    "                    node[0].soft_LCA = tax\n",
    "                    node[0].soft_LCA_H = node[3]                \n",
    "\n",
    "\n",
    "            #add first prok node\n",
    "            else:\n",
    "                nodes[0][0].soft_LCA = tax\n",
    "                nodes[0][0].soft_LCA_H = nodes[0][3]\n",
    "\n",
    "        #skip empty\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    #--- TREE PRINTING ---\n",
    "    \n",
    "    #define overall tree styling\n",
    "    ts = TreeStyle()\n",
    "    ts.title.add_face(TextFace(tree_header, fsize=8), column=0)\n",
    "    ts.mode = 'r'\n",
    "    ts.show_leaf_name = False\n",
    "    ts.show_branch_length = False\n",
    "    ts.show_branch_support = False\n",
    "    #ts.optimal_scale_level = 'full'\n",
    "    ts.allow_face_overlap = True\n",
    "    ts.scale = 50\n",
    "\n",
    "    default_node_style = ete3.NodeStyle()\n",
    "    default_node_style['size'] = 0\n",
    "    default_node_style['fgcolor'] = 'Black'\n",
    "\n",
    "    default_leaf_style = ete3.NodeStyle()\n",
    "    default_leaf_style['size'] = 0\n",
    "    default_leaf_style['fgcolor'] = 'Black'\n",
    "\n",
    "    arc_node_style = ete3.NodeStyle()\n",
    "    arc_node_style['size'] = 2\n",
    "    arc_node_style['fgcolor'] = 'Cyan'\n",
    "\n",
    "    euk_node_style = ete3.NodeStyle()\n",
    "    euk_node_style['size'] = 2\n",
    "    euk_node_style['fgcolor'] = 'Green'\n",
    "\n",
    "    outlier_node_style = ete3.NodeStyle()\n",
    "    outlier_node_style['size'] = 3\n",
    "    outlier_node_style['fgcolor'] = 'Red'\n",
    "\n",
    "    hard_LCA_node_style = ete3.NodeStyle()\n",
    "    hard_LCA_node_style['size'] = 2\n",
    "    hard_LCA_node_style['fgcolor'] = 'Gray'\n",
    "\n",
    "    soft_LCA_node_style = ete3.NodeStyle()\n",
    "    soft_LCA_node_style['size'] = 4\n",
    "    soft_LCA_node_style['fgcolor'] = 'Black'\n",
    "\n",
    "    soft_euk_LCA_node_style = ete3.NodeStyle()\n",
    "    soft_euk_LCA_node_style['size'] = 4\n",
    "    soft_euk_LCA_node_style['fgcolor'] = 'Gray'\n",
    "\n",
    "    #set styling for all leaves and internal nodes\n",
    "    for node in tree.traverse():\n",
    "        node.set_style(default_node_style)\n",
    "        node.add_face(TextFace(node.soft_LCA, fsize=4), column=0)\n",
    "        node.add_face(TextFace(node.soft_LCA_H, fsize=4), column=0)\n",
    "\n",
    "        if node.is_leaf():\n",
    "            node.add_face(TextFace(node.name, fsize=4), column=1)\n",
    "            node.add_face(TextFace(' '+node.tax_class,  fsize=4), column=2)\n",
    "            node.set_style(default_leaf_style)\n",
    "\n",
    "            if node.tax_superkingdom == 'Eukaryota':\n",
    "                node.set_style(euk_node_style)\n",
    "\n",
    "            elif node.tax_superkingdom == 'Archaea':\n",
    "                node.set_style(arc_node_style)\n",
    "\n",
    "        if node in hard_LCA_nodes:\n",
    "            node.set_style(hard_LCA_node_style)\n",
    "\n",
    "        if node in soft_LCA_nodes:\n",
    "            node.set_style(soft_LCA_node_style)\n",
    "\n",
    "        if node in outlier_nodes:\n",
    "            node.set_style(outlier_node_style)\n",
    "            \n",
    "    #for more consistent visualisation\n",
    "    tree.ladderize()\n",
    "    tree.write(features=[\"name\", 'soft_LCA', \"tax_filter\", \"tax_superkingdom\"], outfile=merged_tree+'.annot')\n",
    "    #tree.render(merged_tree+'.annot.pdf', tree_style=ts)\n",
    "    #view_tree(tree, ts, backup=file+'.pdf')\n",
    "    \n",
    "    return merged_tree+'.annot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37b16270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare all mmseqs\n",
    "def microcosm_run(query, root, threads):\n",
    "\n",
    "    microcosm_prepare_mmseqs(query, \n",
    "                             query_DB='euk72/euk72',\n",
    "                             target_DB='prok2111/prok2111',\n",
    "                             root=root)\n",
    "\n",
    "    microcosm_realign_and_filter(query, \n",
    "                                 root, \n",
    "                                 threads=threads,\n",
    "                                 max_euk_leaf_size=50,\n",
    "                                 max_prok_leaf_size=250,\n",
    "                                 filter_entropy=0.5,\n",
    "                                 muscle_reps_euk=25, \n",
    "                                 muscle_reps_prok=10)\n",
    "    \n",
    "    microcosm_merge_align_tree(query, \n",
    "                               root,\n",
    "                               threads=threads, \n",
    "                               filter_entropy=0.5, \n",
    "                               muscle_reps=10)\n",
    "\n",
    "    microcosm_tree_analysis(query, \n",
    "                            root, \n",
    "                            max_tree_leaves=1500, \n",
    "                            prok_min_size = 2, \n",
    "                            prok_min_purity = 0.5,\n",
    "                            euk_min_size = 3,\n",
    "                            euk_min_purity = 0.76)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d974380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#argparse define\n",
    "parser = argparse.ArgumentParser(description='Evalute microcosm')\n",
    "parser.add_argument('--query', type=str, required=True, help='query root name, maps to folder containing containing .acc and .targets accession files within microcosm')\n",
    "parser.add_argument('--root', type=str, required=True, help='path to microcosm root')\n",
    "parser.add_argument('--threads', type=int, required=True, help='threads to run')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "#run main\n",
    "if __name__ == '__main__':\n",
    "    microcosm_run(args.query, args.root, args.threads)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62959e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15047"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93cb7530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1acd8bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d1d490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63105893",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python/3.9",
   "language": "python",
   "name": "py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
